\documentclass[12pt]{article} %***
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
\usepackage[]{hyperref}  %<----modified by Ivan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{sectsty, secdot}
%\sectionfont{\fontsize{12}{15}\selectfont}
\sectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
\renewcommand{\theequation}{\thesection\arabic{equation}}
\subsectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth=31.9pc
\textheight=46.5pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
%\headheight=.2cm
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage[title]{appendix}

\usepackage{bm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{IEEEtrantools}
\usepackage{enumerate}
\usepackage[FIGTOPCAP]{subfigure}
\usepackage{xr}


%\usepackage{refcheck}


\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bL}{\mathbf{L}}

\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\ud}{\mathbf{d}}

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}
\def\blambda {\bfsym {\lambda}} 
\def\bLambda {\bfsym {\Lambda}} 
\def\bSigma {\bfsym {\Sigma}} 
\def\bTheta {\bfsym {\Theta}} 
\def\bPsi {\bfsym {\Psi}} 





\newtheorem{assumption}{Assumption}

\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{}
%\headrulewidth=0pt  %<-modified by Ivan

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{2}

\markright{ \hbox{\footnotesize\rm Statistica Sinica
%{\footnotesize\bf 24} (201?), 000-000
}\hfill\\[-13pt]
\hbox{\footnotesize\rm
%\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
}\hfill }

\markboth{\hfill{\footnotesize\rm Rui Wang AND Xingzhong Xu} \hfill}
{\hfill {\footnotesize\rm Least Favorable Direction Test} \hfill}

\renewcommand{\thefootnote}{}
$\ $\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont \vspace{0.8pc}
\centerline{\large\bf Least Favorable Direction Test for }
\vspace{2pt} \centerline{\large\bf Multivariate Analysis of Variance}
\vspace{2pt} \centerline{\large\bf in High Dimension}
\vspace{.4cm} \centerline{
    Rui Wang, Xingzhong Xu
} \vspace{.4cm} \centerline{\it
Beijing Institute of Technology
}
 \vspace{.55cm} \fontsize{9}{11.5pt plus.8pt minus
.6pt}\selectfont

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{quotation}
\noindent {\it Abstract:}
This study considers multivariate analysis of variance for normal samples in a high-dimensional medium sample size setting.
    When the sample dimension is larger than the sample size, the classical likelihood ratio test is not defined, because the likelihood function is unbounded.
    Based on this unboundedness, we propose a new test called the least favorable direction test.
    The asymptotic distributions of the test statistic are derived under both nonspiked and spiked covariances.
    The local asymptotic power function of the test is also given.
    %{\color{red}
    %These asymptotic results also hold under spiked covariance matrix which can characterize the strong correlations between variables.
%}
    The results for the asymptotic power function and simulations show that the proposed test is particularly powerful under the spiked covariance.

\vspace{9pt}
    \noindent {\it Key words and phrases:}
    High-dimensional data, least favorable direction test, multivariate analysis of variance, principal component analysis, spiked covariance.
%Balanced incomplete block design, Hadamard matrix, nearly balanced incomplete block design, orthogonal array.
\par
\end{quotation}\par



\def\thefigure{\arabic{figure}}
\def\thetable{\arabic{table}}

\renewcommand{\theequation}{\thesection.\arabic{equation}}


\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont

\setcounter{section}{1} %***
\setcounter{equation}{0} %-1
\noindent {\bf 1. Introduction}

Suppose there are $k$ ($k\geq 2$) independent samples of $p$-dimensional data.
Within the $i$th sample ($1\leq i\leq k$), the observations $\{X_{ij}\}_{j=1}^{n_i}$ are independent and identically distributed (i.i.d.) as $\mathcal{N}_p(\theta_i,\bSigma)$, which is a $p$-dimensional normal distribution with mean vector $\theta_i$ and common variance matrix $\bSigma$.
%Suppose there are $k$ normal populations with possibly different means $\theta_1,\ldots,\theta_k$, but all with the same variance $\bSigma$.
%Suppose we observe $k$ independent random samples, each from the distribution $\mathcal{N}_p(\theta_i,\bSigma)$, where $1\leq i\leq k$, $k\geq 2$ is a fixed constant, $\theta_i$ and $\bSigma$ are unknown parameters.
%Denote by $X_{ij}\in \mathbb{R}^p$ the $j$th observation in group $i$, $j=1,\ldots,n_i$, $i=1,\ldots, k$,  where $n_i$ is the samples size of group $i$, $1\leq i \leq k$.
We test the following hypotheses:
\begin{equation}\label{hypothesis}
    H_0: \theta_1=\theta_2=\cdots=\theta_k\quad \text{vs.}\quadã€€H_1: \text{$\theta_i\neq \theta_j$, for some $i\neq j$}.
\end{equation}
This testing problem is known as the one-way multivariate analysis of variance (MANOVA), and has been well studied when $p$ is small relative to $N$, where $N=\sum_{i=1}^k n_i$ is the total sample size.

Let $\bH=\sum_{i=1}^k n_i (\bar{\bX}_i-\bar{\bX})(\bar{\bX}_i-\bar{\bX})^\top$ be the sum-of-squares between groups, and let $\bG=\sum_{i=1}^k \sum_{j=1}^{n_i}(X_{ij}-\bar{\bX}_i)(X_{ij}-\bar{\bX}_i)^\top$ be the sum-of-squares within groups, where $\bar{\bX}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ is the sample mean of group $i$, and $\bar{\bX}=N^{-1}\sum_{i=1}^k\sum_{j=1}^{n_i}X_{ij}$ is the pooled sample mean.
   There are four classical test statistics for hypotheses~\eqref{hypothesis}, all of which are based on the eigenvalues of $\bH\bG^{-1}$. 


       \begin{center}
       \begin{tabular}{|cc|}
           \hline
       {Wilks' Lambda:} & $|\bG+\bH|/|\bG|$\\
       {Pillai trace:} & $\mytr[\bH(\bG+\bH)^{-1}]$\\
       {Hotelling--Lawley trace:} & $\mytr[\bH \bG^{-1}]$\\
       {Roy's maximum root:} & $\lambda_{1}(\bH \bG^{-1})$\\
           \hline
           \end{tabular}
       \end{center}

%There are four classical tests for hypothesis~\eqref{hypothesis}: Wilks' Lambda (which is also the LRT), Hotelling-Lawley trace, Pillai Trace and Roy's maximum root.


In some modern scientific applications, researchers would like to test hypotheses~\eqref{hypothesis} in the high-dimensional setting, that is, where $p$ is greater than $N$;
see, for example,~\citet{Verstynen1209} and~\citet{Tsai2009}.
%However, when $p>n-k$, the LRT for hypothesis~\eqref{hypothesis} is not well defined.
However, none of the four classical test statistics are defined when $p\geq N$.
As a result, extensive research has been done on the testing problem~\eqref{hypothesis} in high-dimensional settings.
 Thus far, numerous tests have been proposed for the case $k=2$;
 see, for example,~\citet{Bai1996Efiect},~\cite{Srivastava2007Multivariate},~\citet{Chen2010A},~\citet{Tony2013}, and \citet{Feng2014Two}.
  Tests have also been proposed for the general case of $k\geq 2$.
 \cite{Schott2007Some} modified the Hotelling--Lawley trace and proposed the following test statistic:
  %$$
  %T_{SC}=\frac{1}{\sqrt{n-1}}\big(
  %\frac{1}{k-1}\mytr\big(\sum_{i=1}^k n_i\bar{X}_i\bar{X}_i^\top-n\bar{X}\bar{X}^\top\big)-\frac{1}{n-k}\mytr\big(\sum_{i=1}^k \sum_{j=1}^{n_i}X_{ij}X_{ij}^\top-\sum_{i=1}^k n_i\bar{X}_i\bar{X}_i^\top\big)
  %\big),
  %$$
  %where $\bar{\bX}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ and $\bar{\bX}=n^{-1}\sum_{i=1}^k\sum_{j=1}^{n_i}X_{ij}$.
  $$
  T_{Sc}=\frac{1}{\sqrt{N-1}}\Big(
  \frac{1}{k-1}\mytr\big(\bH\big)-\frac{1}{N-k}\mytr\big(\bG\big)
  \Big).
  $$
  Here, $T_{Sc}$ is a member of the so-called sum-of-squares statistics, because it is based on an estimation of the squared Euclidean norm $\sum_{i=1}^k n_i\|\theta_i-\bar{\theta}\|^2$, where $\bar{\theta}=N^{-1}\sum_{i=1}^k n_i \theta_i$.
  See~\cite{Srivastava2013},~\cite{Yamada2015},~\cite{Hu2017},~\cite{ZHANG2017200},~\cite{Chang2017}, and~\cite{2017arXiv171007878C} for other sum-of-squares test statistics for $k\geq 2$.
Sum-of-squares tests are known to be particularly powerful in the case of dense alternatives.
In another work,~\cite{Cai2014High} proposed the test statistic 
  $$
  T_{CX}=\max_{1\leq i\leq p} \sum_{1\leq j<l\leq k}\frac{n_j n_l}{n_j+n_l}\frac{(\Omega(\bar{\bX}_j-\bar{\bX}_l))_i^2}{\omega_{ii}},
  $$
  where $\Omega=(\omega)_{ij}=\bSigma^{-1}$ is the precision matrix. When $\Omega$ is unknown, it is substituted by an estimator.
  Unlike $T_{Sc}$, $T_{CX}$ is an extreme-value test statistic, and is powerful in the case of sparse alternatives.
  %Statistics $T_{CX}$ and $T_{SC}$ are the representatives of two popular methodologies for high dimensional tests.
  
  %Suppose $\{X_{i1},\ldots, X_{in_i}\}$ are iid\ distributed as $\mathcal{N}(\theta_i,\bSigma)$ for $1\leq i\leq K$.
%The $k$ samples are independent.
%%$\theta_i$, $i=1\ldots, k$ and $\bSigma>0$ are unknown. An interesting problem in multivariate analysis is to test the hypotheses
%\begin{equation}
    %H: \theta_1=\theta_2=\cdots=\theta_k\quad v.s.\quad K: \textrm{$\theta_i\neq \theta_j$ for some $i\neq j$}.
%\end{equation}
%The likelihood ratio test (LRT) has a dominated position in classical multivariate analysis.

Most existing sum-of-squares test procedures require the condition  $\mytr(\bSigma^4)/\mytr^2(\bSigma^2)\to 0$, which is equivalent to
\begin{equation}\label{nonSpikedC}
    \frac{\blambda_1}{\sqrt{\mytr(\bSigma^2)}}\to 0,
\end{equation}
where $\blambda_i$ is the $i$th largest eigenvalue of $\bSigma$, for $i=1,\ldots, p$.
In fact, the equivalence of these two conditions can be seen from the following inequalities:
\begin{equation*}
    \frac{\blambda_1^4}{\mytr^2(\bSigma^2)} 
    \leq
    \frac{\mytr(\bSigma^4)}{\mytr^2(\bSigma^2)}
    \leq
    \frac{\blambda_1^2 \mytr(\bSigma^2)}{\mytr^2(\bSigma^2)}
    =\frac{\blambda_1^2}{\mytr(\bSigma^2)}.
\end{equation*}
Condition \eqref{nonSpikedC} is reasonable if $\bSigma$ is nonspiked, in the sense that it does not have significantly large eigenvalues.
%, which ensures the asymptotic normality of sum-of-squares type statistics.
However, in practice, variables may be heavily correlated with common factors, in which case, the covariance matrix $\bSigma$ is spiked, in the sense that a few eigenvalues of $\bSigma$ are significantly larger than the others~\citep{Fan2013Large,Cai2015Optimal,wang2017As}.
In such cases, condition \eqref{nonSpikedC} can be violated and, consequently, existing sum-of-squares tests may not have the correct level.
Adjusted sum-of-squares test procedures have been proposed to solve this problem;
see, for example,
\cite{Katayama2013Asymptotic}, \cite{Ma2015A}, \cite{ZHANG2017200}, and \cite{WANG2018}.
However, the power behavior of these corrected tests may not be satisfactory.

Recently, \cite{Aoshima2018} and \cite{WANG2018225} considered a two-sample mean testing problem under the spiked covariance model.
These tests have better power behavior than that of sum-of-squares tests.
However, both studies imposed strong conditions on the magnitude of $p$.
For example, under the approximate factor model in \cite{Fan2013Large}, the test in \cite{Aoshima2018} requires $p/N \to 0$, whereas the test in \cite{WANG2018225} requires that $p/N^2\to 0$ and that the small eigenvalues of $\bSigma$ are all equal.

   The likelihood ratio test (LRT) method has been very successful in leading to satisfactory procedures in many specific problems.
    However, the LRT statistic for hypotheses~\eqref{hypothesis}, that is, Wilks' Lambda statistic, is not defined for $p>N-k$.
   In the high-dimensional setting, neither the sum-of-squares nor the extreme-value statistics are based on the likelihood function.
    This motivates us to construct a likelihood-based test in the high-dimensional setting.
    %Note that Roy's maximum root, one of the four classic test statistics, is derived by Roy's union intersection principle.
    In a recent work,~\cite{Zhao2016A} proposed a generalized likelihood ratio test in the context of the one-sample mean vector test.
    They used a least favorable argument to construct a generalized likelihood ratio test statistic.
    %They wrote the null hypothesis as the intersection of a class of component hypotheses.
    %For each component hypotheses, the likelihood ratio test is constructed.
    Their simulation results showed that their test exhibits good power performance, especially when the variables are correlated.
    However, they do not provide a theoretical proof.


    We propose a generalized likelihood ratio test statistic for hypotheses~\eqref{hypothesis}, called the least favorable direction (LFD) test statistic, which is a generalization of the test in \cite{Zhao2016A}.
    We give the asymptotic distributions of the test statistic under both nonspiked and spiked covariances.
    An adaptive LFD test procedure is constructed by consistently detecting the unknown covariance structure and estimating the unknown parameters.
    The asymptotic local power function of the LFD test is also given.
    Our theoretical results show that the LFD test is particularly powerful under the spiked covariance.
    This explains the simulation results of~\cite{Zhao2016A}.
    Extending the work of \cite{Zhao2016A}, our main contribution is that we provide a thorough theoretical analysis of the LFD test.
    This analysis falls within the high-dimensional medium sample size setting, where both $N,p\to \infty$, but $p/N\to \infty$ (see \cite{aoshima2018A}, Section 5).
    To prove our main results, we carefully study the high-order asymptotic behavior of the eigenvalues and eigenspaces of the sample covariance matrix.
    These results are also of independent interest.
    We further compare the proposed test procedure with existing tests using simulations.
    Here, we show that the LFD test exhibits behavior comparable with that of existing sum-of-squares tests under the nonspiked covariance, while significantly outperforming competing tests under the spiked covariance.

    The rest of the paper is organized  as follows.
    In Section~\ref{methodology}, we propose the LFD test statistic and derive its explicit forms.
    The asymptotic distributions of the LFD test statistic under nonspiked and spiked covariances are given in Section \ref{THEO}.
    Based on these theoretical results, an adaptive LFD test procedure is proposed.
     Section~\ref{numerical} complements our study with numerical simulations.
     Section~\ref{concluding} concludes the paper.
     Finally, the proofs are gathered in the Supplementary Material.




%\setcounter{section}{2} %***
%\setcounter{equation}{0} %-1
%\noindent {\bf 2. The Second Section}

 
\section{Least favorable direction test}\label{methodology}
\setcounter{equation}{0} %-1
We first introduce some necessary notation.
 Define the $p\times N$ pooled sample matrix $\bX$ as
 $$\bX=(X_{11},X_{12},\ldots,X_{1n_1},X_{21},X_{22},\ldots,X_{2n_2},\ldots,X_{k1},X_{k2},\ldots,X_{kn_k}).$$
 The sum-of-squares within groups $\bG$ can be written as $\bG=\bX(\bI_N-\bJ\bJ^\top)\bX^\top$, where
 $$
 \bJ=\begin{pmatrix}
     \frac{1}{\sqrt{n_1}}\mathbf{1}_{n_1}&\mathbf{0} & \mathbf{0}\\
     \mathbf{0}&\frac{1}{\sqrt{n_2}} \mathbf{1}_{n_2}& \mathbf{0}\\
     \vdots &\vdots &\vdots \\
     \mathbf{0}&\mathbf{0}&\frac{1}{\sqrt{n_k}}\mathbf{1}_{n_k}
 \end{pmatrix}
 $$
 is an $N\times k$ matrix,
 and $\mathbf{1}_{n_i}$ is an $n_i$-dimensional vector with all elements equal to one, for $i=1,\ldots, k$.
 Let $n=N-k$ be the degrees of freedom of $\bG$.
 Construct an $N\times n$ matrix $\tilde{\bJ}$ as 
 $$
 \tilde{\bJ}=\begin{pmatrix}
     \tilde{\bJ}_1&\mathbf{0} & \mathbf{0}\\
     \mathbf{0}&\tilde{\bJ}_2& \mathbf{0}\\
     \vdots &\vdots &\vdots \\
     \mathbf{0}&\mathbf{0}&\tilde{\bJ}_k
 \end{pmatrix},
 $$
 where $\tilde{\bJ}_i$ is an $n_i\times (n_{i}-1)$ matrix defined as
 $$
\tilde{\bJ}_i=\begin{pmatrix}
    \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{6}}&\cdots&\frac{1}{\sqrt{(n_i-2)(n_i-1)}}&\frac{1}{\sqrt{(n_i-1)n_i}}\\
    -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{6}}&\cdots&\frac{1}{\sqrt{(n_i-2)(n_i-1)}}&\frac{1}{\sqrt{(n_i-1)n_i}}\\
    0&-\frac{2}{\sqrt{6}}&\cdots&\vdots&\vdots\\
    \vdots&\vdots&\cdots&-\frac{n_i-2}{\sqrt{(n_i-2)(n_i-1)}}&\frac{1}{\sqrt{(n_i-1)n_i}}\\
    0&0&\cdots&0&-\frac{n_i-1}{\sqrt{(n_i-1)n_i}}\\
\end{pmatrix}.
 $$
The matrix $\tilde{\bJ}$ is a column orthogonal matrix  satisfying $\tilde{\bJ}^\top\tilde{\bJ}=\bI_{n}$ and $\tilde{\bJ}\tilde{\bJ}^\top =\bI_N-\bJ\bJ^\top$.
Define $\bY=\bX\tilde{\bJ}$.
Then, $\bG$ can be written as
$$\bG=
\bY \bY^\top.
$$

The sum-of-squares between groups $\bH$ can be written as
$$
        \bH=\bX(\bJ\bJ^\top-\frac{1}{N}\mathbf{1}_N\mathbf{1}_N^\top)\bX^\top
=\bX \bJ(\bI_k-\frac{1}{N}\bJ^\top\mathbf{1}_N \mathbf{1}_N^\top \bJ)\bJ^\top \bX^\top.
$$
By some matrix algebra, we have $\bI_k-N^{-1}\bJ^\top\mathbf{1}_N \mathbf{1}_N^\top \bJ=\bC\bC^\top$,
where $\bC$ is a $k\times (k-1)$ matrix defined as $\bC=\bC_1\bC_2$, and
 $$
\bC_1=\begin{pmatrix}
    \sqrt{n_1}&\sqrt{n_1}&\cdots&\sqrt{n_1}&\sqrt{n_1}\\
    -\frac{n_1}{\sqrt{n_2}}&\sqrt{n_2}&\cdots&\sqrt{n_2}&\sqrt{n_2}\\
    0&-\frac{n_1+n_2}{\sqrt{n_3}}&\cdots&\vdots&\vdots\\
    \vdots&\vdots&\cdots&-\frac{\sum_{i=1}^{k-2}n_i}{\sqrt{n_{k-1}}}&\sqrt{n_{k-1}}\\
    0&0&\cdots&0&-\frac{\sum_{i=1}^{k-1}n_i}{\sqrt{n_k}}\\
\end{pmatrix},
 $$
 $$
\bC_2=\begin{pmatrix}
    \frac{n_1(n_1+n_2)}{n_2}&0&\cdots&0\\
    0&\frac{(\sum_{i=1}^2 n_i)(\sum_{i=1}^3 n_i)}{n_3}&\cdots&0\\
    \vdots&\vdots&\cdots&\vdots\\
    0&0&\cdots&\frac{(\sum_{i=1}^{k-1} n_i)(\sum_{i=1}^k n_i)}{n_{k}}\\
\end{pmatrix}^{-\frac{1}{2}}.
$$
Then, $\bH$ can be written as
\begin{equation*}
    \begin{aligned}
        \bH=\bX \bJ\bC \bC^\top \bJ^\top \bX^\top.
    \end{aligned}
\end{equation*}
Define $\bTheta=(\sqrt{n_1}\theta_1,\ldots,\sqrt{n_k}\theta_k)$.
Then, the null hypothesis $H_0$ is equivalent to $\bTheta \bC=\bO_{p\times (k-1)}$, where $\bO_{p\times (k-1)}$ is a $p\times (k-1)$ matrix with all entries zero.
 Thus, the hypotheses~\eqref{hypothesis} are equivalent to
 $$
 H_0:\bTheta \bC=\bO_{p\times (k-1)}\quad \text{vs.}\quad H_1: \bTheta \bC\neq \bO_{p\times (k-1)}.
 $$

The testing problem~\eqref{hypothesis} is well studied for low-dimensional settings.
A classical test statistic is Roy's maximum root, constructed by~\cite{Roy1953} using his well-known union intersection principle.
%The difficulty occurs when $p\geq n$, where the four classical test statistics can not be defined.
%To construct test statistic in high dimensional setting, a simple idea is to reduce the problem~\eqref{hypothesis} to a class of univariate problems.
        The key idea is to decompose $\bX$ into a set of univariate data $\{\bX_{a}=a^\top \bX:\, a\in \mathbb{R}^p, a^\top a=1\}$.
        This induces the following decompositions of the null and alternative hypotheses:
        $$
        H_0=\bigcap_{a\in\mathbb{R}^p, a^\top a=1} H_{0a} \quad \text{vs.} \quad 
        H_1=\bigcup_{a\in\mathbb{R}^p, a^\top a=1} H_{1a},
        $$
        where 
 $H_{0a}: a^\top \bTheta \bC = \bO_{1\times (k-1)}$ and  $H_{1a} : a^\top \bTheta \bC \neq \bO_{1\times (k-1)}$.
Let $L_0(a)$ and $L_1(a)$ be the maximum likelihood of $\bX_a$ under $H_{0a}$ and $H_{1a}$, respectively.
For each $a$ satisfying $a^\top a=1$, the component LRT statistic
$$ \frac{L_1(a)}{L_0(a)}=\Big(\frac{a^\top(\bG+\bH) a}{a^\top \bG a}\Big)^{N/2}$$
can be used to test $H_{0a}$ versus $H_{1a}$. 
Using the union intersection principle, Roy proposed the test statistic
$
\max_{a^\top a=1}  {L_1(a)}/{L_0(a)}=(1+\lambda_{1}(\bH\bG^{-1}))^{N/2},
$
where $\lambda_{i}(\cdot)$ denotes the $i$th largest eigenvalue.
This statistic is an increasing function of Roy's maximum root.
%It turns out that many tests in the literature can be derived by the above strategy.
%While the LRT may be the best choice of univariate problems in step 2,
% there are more choices in step 1 and step 3.
%In step 3, Roy's union intersection principle suggests using $\max_{\gamma\in\Gamma}T_{\gamma}$ as a global test statistic~\citep{Roy1953},
%while another choice is to integrate $T_\gamma$ according to some measure $\mu(\gamma)$ and use $\int_\gamma T_{\gamma}\,\mu(d\gamma)$ as global test statistic.
%For step 1, we consider two different constructions of data projection.
%\begin{enumerate}[(i)]
%    \item
%        Consider the class of univariate data $\{\bX_{i}=e_i^\top \bX:i=1,\ldots,p\}$, where $e_i$ is the $i$th standard basis in $\mathbb{R}^p$.
%        Hence $H_0=\bigcap_{i=1}^p H_{0i}$ and $H_1=\bigcup_{i=1}^p H_{1i}$, where
% $$
% H_{0i}: e_i^\top \bTheta \bC = \bO_{1\times (k-1)}\quad \text{and}\quad H_{1a} : e_i^\top \bTheta \bC \neq \bO_{1\times (k-1)}.
% $$
%\item
%    Consider the class of univariate data $\{\bX_{a}=a^\top \bX:i=1,a\in\mathbb{R}^p, a^\top a=1\}$.
%        Hence $H_0=\bigcap_{a\in \mathbb{R}^p, a^\top a=1}H_{0a}$ and $H_1=\bigcup_{a\in \mathbb{R}^p,a^\top a=1}H_{1a}$, where
% $$
% H_{0a}: a^\top \bTheta \bC = \bO_{1\times (k-1)}\quad \text{and}\quad H_{1a} : a^\top \bTheta \bC \neq \bO_{1\times (k-1)}.
% $$
%\end{enumerate}
%
%First, we consider the construction (i) in step 1.
%Suppose component test statistics 
%$$T_i={(k-1)^{-1}} e_i^\top \bH e_i-(n-k)^{-1}e_i^\top \bG e_i\quad i=1,\ldots, p$$
%are used in step 2, and in step 3 we integrate $T_i$ according to the uniform measure on $1,\ldots,p$. Then the resulting statistic is $p^{-1}\sum_{i=1}^p T_i$ which is equivalent to $T_{SC}$.
%If instead the likelihood ratio test statistic $e_i^\top \bH e_i/e_i^\top \bG e_i$ is used in step 2, one obtains a scalar invariant test statistic which is a direct generalization of~\citet{Srivastava2009A}'s test.
%On the other hand, by using data $\Omega^{-1}\bX$ and component test statistics
%$$
%T_i^*=\sum_{1\leq j<l \leq k} \frac{n_j n_l}{n_j+n_l}\frac{(\Omega(\bar{\bX}_j-\bar{\bX}_l))^2_i}{\omega_{ii}},
%$$
%we have $T_{CX}=\max_{1\leq i\leq p}T_i^*$.
%Here the component test statistic $T_i^*$ is similar to likelihood ratio tests.
%
%While many existing tests can be derived by the construction (i) in step 1, this construction has limitation in that it relies on the choice of an orthogonal basis of $\mathbb{R}^p$.
%In fact, test statistics resulting from this construction mostly requires certain prior information about the covariance matrix.
%For example,~\citet{Schott2007Some} requires that $\mytr(\bSigma^{2j})/p\to \tau_j\in(0,\infty)$, $j=1,2$, and~\citet{Cai2014High} requires a consistent estimator of $\Omega$.
%
%Next, we consider using construction (ii) in step 1, which does not rely on the basis of $\mathbb{R}^p$.
%Suppose the likelihood ratio test statistic $T_a=a^\top \bH a/a^\top \bG a$ is used in step 2.
%If we use the integrating strategy in step 3 and choose $\mu$ equal to the uniform distribution on the sphere, then the test statistic becomes
%$$
%\int_{a^\top a=1} \frac{a^\top \bH a}{a^\top \bG a}\, \mu(da).
%$$
%Although it is hard to give an explicit form of the integration, we can approximate it by random projection.
%More specifically, one can randomly generate unit vectors $a_1,\ldots,a_M$ and the statistics can be approximated by $M^{-1}\sum_{i=1}^M a_i^\top \bH a_i/a_i^\top \bG a_i$.
%This statistic is well defined in high dimensional setting.
%A similar method is proposed by~\cite{Lopes2015A} for $k=2$ from a different point of view.
%Their analysis and simulations show that such random projection method has relatively good performance especially when variables are correlated.
%On the other hand, if $n-k\geq p$, Roy's union intersection principle can be used in step 3, the resulting  statistic is the well-known Roy's maximum root:
%$$
%\max_{a^\top a=1}T_a=\lambda_{\max}(\bH \bG^{-1}).
%$$
%In fact, this statistic is first derived in~\cite{Roy1953} as an example of his union intersection principle.
%
%However, it can only be defined when $n-k\geq p$.
%In fact, if $p>n-k$, $G$ is not invertible and $T_a$ is not defined for some $a$. 
%We will follow~\citet{Zhao2016A}'s idea and propose a new test statistic for $p>n-k$.

From a likelihood point of view, the log likelihood ratio is an estimator of the Kullback--Leibler divergence between the true distribution and the null distribution.
Hence, the component LRT statistic $L_1(a)/L_0(a)$ characterizes the discrepancy between  the true and the null distribution along the direction $a$.
%By maximizing $ (L_1(a)/L_0 (a))$, 
%one obtains component hypothesis $H_{0a^*}$, where $a^*=\argmax_{a^\top a=1}(L_1(a)/L_0(a))$. 
This motivates us to consider the direction 
\begin{equation}\label{LFDdef1}
    a^*=\argmax_{a^\top a=1}\frac{L_1(a)}{L_0(a)},
\end{equation}
which hopefully yields the largest discrepancy between the true and the null distribution.
Thus, $H_{0a^*}$ is the component null hypothesis least likely to be true.
We call $a^*$ the least favorable direction. %and $H_{0a^*}$ the least favorable hypothesis
Note that Roy's maximum root is the component LRT statistic along the least favorable direction.




%Note that $\max_{a^\top a=1}\text{LRT}_a=\text{LRT}_{a^*}$, where 
%$a^*=\argmax_{a^\top a=1}\text{LRT}_a$
%is the direction achieving the maximum discrepancy between the null hypothesis and alternative hypothesis.
%We shall call $a^*$ the rejection direction.
%In the derivation of Roy's maximum root, we have
%$$
%\text{LRT}_a=\Big(1+\frac{a^\top H a}{a^\top G a}\Big)^{n/2}.
%$$
%In another viewpoint, union intersection principle finds an direction $a$ along which the evidence against null hypothesis is maximized.
%Such an $a$ is data dependent.
%In the classical setting, the evidence of direction $a$ is $\text{LRT}_a$.
%In the current context, there are a class of $a$ such that $\text{LRT}_a$ achieve the infinity, the largest evidence in classical sense.
%We need to further choose a single $a$ from $\{a\,|\,\text{LRT}_a=+\infty\text{ and }a^\top a =1\}$.
 %From the expression of $\text{LRT}_a$, we would like to make the largest discrepancy between $a^\top H a$ and $a^\top G a$.
%Note that if $\text{LRT}_{\alpha}=+\infty$, then $a^\top G a=0$.
%Hence it's natural to choose $a$ as
%$$
        %a^{*}=
        %\argmax_{a^\top a=1, a^\top G a=0} 
        %a^\top H a.
%$$
%Since $a^{*T}G a^*=0$, we propose the following test statistic for $H$:


Unfortunately, Roy's maximum root can only be defined when $n\geq p$, and hence cannot be used in the high-dimensional setting.
%While it is hard to generalize Roy's maximum root to high dimensional setting, the least favorable hypothesis $a^*$ can be formally generalized to high dimensional setting.
In what follows, we assume $p>n$.
%The essence of Roy's union intersection principle is to find a direction $a$ along which $\text{LRT}_a$ is maximized.
%Based on this idea, we have the following heuristic argument to propose a new test statistic.
In this case,
the set
$$\mathcal{A}\overset{def}{=}\{a:L_1(a)=+\infty, \, a^\top a=1\}=\{a:a^\top \bG a=0, \, a^\top a=1\}$$
is not empty because $\bG$ is singular. 
Consequently, the right-hand side of~\eqref{LFDdef1} is not well defined because the ratio involves infinity.
Hence, we need a new definition for the LFD in the high-dimensional setting.
Define
$$\mathcal{B}=\{a:L_0(a)=+\infty, \, a^\top a=1\}=\{a:a^\top (\bG+\bH)a= 0, \, a^\top a=1\}.$$
Note that $\mathcal{B}\subset \mathcal{A}$.
Moreover, by the independence of $\bG$ and $\bH$, with probability one, we have $\mathcal{A}\cap \mathcal{B}^c\neq \emptyset$.
Then, for any direction $a$, there are three possible scenarios: $L_1(a)<+\infty$ and $L_0(a)<+\infty$; $L_1(a)=+\infty$ and $L_0(a)<+\infty$; and $L_1(a)=+\infty$ and $L_0(a)=+\infty$.
To maximize the discrepancy between $L_1(a)$ and $L_0(a)$, one may consider the direction $a$ such that $L_1(a)=+\infty$ and $L_0(a)<+\infty$.
This suggests that the least favorable direction $a^*$, which hopefully maximizes the discrepancy between $L_1(a)$ and $L_0(a)$, should be defined as $a^* = \argmin_{a\in\mathcal{A}\cap\mathcal{B}^c} L_0 (a)$.
%and take $H_{a^*}$ as the least favorable hypothesis.
Equivalently,
$$
\begin{aligned}
    a^*&=\argmin_{a\in \mathcal{A}\cap\mathcal{B}^c} L_0(a) = \argmax_{a^\top a=1,a^\top Ga=0} {a^\top \bH a}.
\end{aligned}
$$
Based on $a^*$ and the likelihood $L_0(a)$, we propose a new test statistic,
\begin{equation*}
    \begin{aligned}
        T(\bX)=a^{*T} \bH a^*
        =
        \max_{a^\top a=1, a^\top \bG a=0} 
        a^\top \bH a.
    \end{aligned}
\end{equation*}
The null hypothesis is rejected when $T(\bX)$ is sufficiently large.
We call $T(\bX)$ the LFD test statistic.
Because the least favorable direction $a^*$ is obtained from the component likelihood function, the statistic $T(\bX)$ is also a generalized likelihood ratio test statistic.
%The above method is first proposed by~\cite{Zhao2016A} in the context of testing one sample mean vector.
%The idea of generalized likelihood ratio test is first proposed by~\citet{Zhao2016A}.

Now, we derive the explicit forms of the LFD test statistic. 
Let $\bY=\bU_{\bY}\bD_{\bY}\bV_{\bY}^\top$ be the singular value decomposition of $\bY$, where $\bU_{\bY}$ and $\bV_{\bY}$ are $p\times \min(n,p)$ and $n \times \min(n,p)$ column orthogonal matrices, respectively, and $\bD_{\bY}$ is a $\min(n,p)\times \min(n,p)$ diagonal matrix, with diagonal elements comprising the non-increasingly ordered singular values of $\bY$.
If $p>n$, let $\bP_{\bY}=\bU_{\bY}\bU_{\bY}^\top$ be the projection matrix onto the column space of $\bY$.
Then, Lemma $1$ in the Supplementary Material implies that, for $p>n$,
\begin{equation}\label{statisticForm1}
\begin{aligned}
    T(\bX)
    %&=\lambda_{\max}\big(\bX \bJ\bC\bC^\top\bJ^\top\bX^\top (\bI_p-
    %\bP_{\bY})
    %\big)
    =\lambda_{1}\big(\bC^\top\bJ^\top\bX^\top (\bI_p-
    \bP_{\bY}
    )\bX\bJ\bC\big).
\end{aligned}
\end{equation}

Although~\eqref{statisticForm1} is convenient for the theoretical analysis, it is not convenient for computation.
When $p>N$, another simple form of $T(\bX)$ can be used for computation.
%where $\bP_\bA=Z\tilde{J}{(\tilde{J}^\top Z^\top Z\tilde{J})}^{-1}\tilde{J}^\top Z^\top$.
If $p>N$, then $\bX^\top \bX$ is invertible.
By the relationship
\begin{equation*}
    \begin{aligned}
        \begin{pmatrix}
            \bJ^\top \bX^\top \bX\bJ & \bJ^\top \bX^\top \bX\tilde{\bJ}\\
            \tilde{\bJ}^\top \bX^\top \bX \bJ & \tilde{\bJ}^\top \bX^\top \bX \tilde{\bJ}
        \end{pmatrix}^{-1}
        =&
        \Big(
        \begin{pmatrix}
            \bJ^\top\\
            \tilde{\bJ}^\top
        \end{pmatrix}
        \bX^\top \bX
        \begin{pmatrix}
            \bJ &\tilde{\bJ}
        \end{pmatrix}
        \Big)^{-1}
        \\
        =&
        \begin{pmatrix}
            \bJ^\top {(\bX^\top \bX)}^{-1}\bJ & \bJ^\top {(\bX^\top \bX)}^{-1}\tilde{\bJ}\\
            \tilde{\bJ}^\top {(\bX^\top \bX)}^{-1}\bJ & \tilde{\bJ}^\top {(\bX^\top \bX)}^{-1} \tilde{\bJ}
        \end{pmatrix}
    \end{aligned}
\end{equation*}
and the matrix inverse formula, we have that
\begin{equation*}
    \begin{aligned}
        \big( \bJ^\top {(\bX^\top \bX)}^{-1}\bJ \big)^{-1}
        =&\bJ^\top \bX^\top \bX \bJ - \bJ^\top \bX^\top \bX\tilde{\bJ}{(\tilde{\bJ}^\top \bX^\top \bX \tilde{\bJ})}^{-1}
            \tilde{\bJ}^\top \bX^\top \bX\bJ 
            \\
        =& \bJ^\top \bX^\top( \bI_p- \bP_{\bY}) \bX \bJ.
    \end{aligned}
\end{equation*}
Thus, 
\begin{equation}\label{statisticForm2}
    \begin{aligned}
        T(\bX)=
        \lambda_1 \Big(\bC^\top\big( \bJ^\top (\bX^\top \bX)^{-1}\bJ \big)^{-1}\bC\Big).
    \end{aligned}
\end{equation}
Compared with~\eqref{statisticForm1}, the expression in~\eqref{statisticForm2} does not involve $\bP_{\bY}$ and, thus, is more convenient for computation.

In the case of $k=2$, it can be seen that the least favorable direction is proportional to
$
(\bI_p-\bP_{\bY}) (\bar{\bX}_1-\bar{\bX}_2)
$, and the LFD test statistic has expression
$$
T(\bX)=\frac{n_1 n_2}{n_1+n_2}\| (\bI_p-\bP_{\bY}) (\bar{\bX}_1-\bar{\bX}_2)\|^2.
$$
In this case, 
the least favorable direction coincides with the maximal data piling direction proposed by~\cite{Ahn2010}.

\section{Theoretical analysis}\label{THEO}
\setcounter{equation}{0} %-1
We now analyze the asymptotic distributions of the LFD test statistic.
The normality of the observations is an important assumption for our results, and is assumed throughout this section.
We present theoretical results under both nonspiked and spiked covariances.
Based on these results, we construct an adaptive test with an asymptotically correct level.
In addition, these results allow us to derive the local asymptotic power function of the LFD test.









\subsection{Nonspiked covariance}

In this subsection, we establish the asymptotic distribution of $T(\bX)$ under the nonspiked covariance.
Let $\bW_{k-1}$ be a $(k-1)\times(k-1)$ symmetric random matrix in which the entries above the main diagonal are i.i.d. $\mathcal{N}(0,1)$ random variables, and the entries on the diagonal are i.i.d. $\mathcal{N}(0,2)$ random variables.
The following theorem establishes the asymptotic distribution of the LFD test statistic.
\begin{theorem}
    \label{fenTheorem1}
    Suppose as $n,p\to \infty$, condition \eqref{nonSpikedC} holds.
    Furthermore, suppose $n\blambda_1/\mytr(\bSigma)\to 0$ and $\blambda_1-\blambda_p=O(n^{-1}\sqrt{\mytr(\bSigma^2)})$.
    Then, under the local alternative hypothesis $\|\bC^\top \bTheta^\top \bTheta \bC\|=O(\sqrt{\mytr(\bSigma^2)})$,
    \begin{equation*}
        \frac{T(\bX)-\left(\mytr(\bSigma)-n\mytr(\bSigma^2)/\mytr(\bSigma)\right)}{\sqrt{\mytr(\bSigma^2)}}
        \sim
        \lambda_1\left(\bW_{k-1}+\frac{\bC^\top \bTheta^\top \bTheta \bC}{\sqrt{\mytr(\bSigma^2)} }\right)
        +o_P(1),
    \end{equation*}
    where $\sim$ means having the same distribution.
\end{theorem}
\begin{remark}
    The condition $n\blambda_{1}/\mytr(\bSigma)\to 0$ implies $p/n \to \infty$.
    Hence, $T(\bX)$ is well defined for large $n$.
    The condition $\blambda_1-\blambda_p=O(n^{-1}\sqrt{\mytr(\bSigma^2)})$ requires that the range of the eigenvalues of $\bSigma$ not be too large.
\end{remark}
To centralize $T(\bX)$ % formulate a test procedure with asymptotically correct level 
under the conditions of Theorem \ref{fenTheorem1}, we need to estimate the parameters $\mytr(\bSigma)$ and $\mytr(\bSigma^2)$.
Let $\hat{\bSigma}=n^{-1}\bG=n^{-1}\bY\bY^\top$ be the sample covariance matrix.
We use the following simple estimators:
\begin{equation*}
    \widehat{\mytr(\bSigma)}=\mytr(\hat{\bSigma}),
    \quad
    \widehat{\mytr(\bSigma^2)}=\mytr ( \hat{\bSigma}^2 )-n^{-1}\mytr^2(\hat \bSigma).
\end{equation*}
Define
$$
Q_1=
\frac{T(\bX)-\left(\widehat{\mytr(\bSigma)}-n\widehat{\mytr(\bSigma^2)}/\widehat{\mytr(\bSigma)}\right)}{\sqrt{\widehat{\mytr(\bSigma^2)}}}.
$$
Let $F_1(x)$ be the cumulative distribution function of $\lambda_{1}(\bW_{k-1})$.
Then, we reject the null hypothesis if $Q_1> F_1^{-1}(1-\alpha)$.
The following corollary gives the asymptotic local power function of the proposed test under the nonspiked covariance.
\begin{corollary}\label{kuCor1}
    Under the conditions of Theorem \ref{fenTheorem1}, 
    \begin{equation*}
        \begin{split}
        &\Pr\left(
            Q_1>F_1^{-1}(1-\alpha)
        \right) 
        \\
        =&
        \Pr\left(
        \lambda_1\left(\bW_{k-1}+\frac{\bC^\top \bTheta^\top \bTheta \bC}{\sqrt{\mytr(\bSigma^2)} }\right)
        >F_1^{-1}(1-\alpha)
    \right)+o(1).
        \end{split}
    \end{equation*}
\end{corollary}
Corollary \ref{kuCor1} shows that under the nonspiked covariance, the LFD test exhibits power behavior similar to that of existing sum-of-squares tests.
In fact, if $k=2$, the asymptotic local power function given by Corollary \ref{kuCor1} is equal to the asymptotic local power function of the tests in~\citet{Bai1996Efiect} and~\citet{Chen2010A}.




\subsection{Spiked covariance}


Now, we derive the asymptotic results under the spiked covariance, which is more involved than the nonspiked case.
%Let $\blambda_1\geq\cdots \geq \blambda_p$ denote the non-increasingly ordered eigenvalues of $\bSigma$.
Let $\bSigma= \bU\bLambda \bU^\top$ denote the eigenvalue decomposition of $\bSigma$, where $\bLambda =\mydiag (\blambda_1,\ldots,\blambda_p)$ and $\bU$ is an orthogonal matrix.
Suppose that $\bSigma$ has $r$ spiked eigenvalues, where $1\leq r\leq p$ can also vary as $n,p\to \infty$.
    We first assume the spiked number $r$ is known.
    We latter consider the adaptation to unknown $r$.
Denote $\bLambda_1=\mydiag(\blambda_1,\ldots,\blambda_r)$ and $\bLambda_2=\mydiag(\blambda_{r+1},\ldots,\blambda_p)$.
Correspondingly, we denote $\bU=(\bU_1,\bU_2)$, where $\bU_1$ and $\bU_2$ are the first $r$ columns and the last $p-r$ columns, respectively, of $\bU$.
Then, $\bSigma=\bU_1\bLambda_1 \bU_1^\top+\bU_2\bLambda_2 \bU_2^\top$.

First, we derive the asymptotic properties of the eigenvalues and eigenspaces of the sample covariance matrix $\hat{\bSigma}$, because these play a key role in our later analysis.
    The following proposition gives the asymptotic behavior of $\lambda_1(\hat{\bSigma}),\ldots, \lambda_r(\hat{\bSigma})$ and $\sum_{i=r+1}^n\lambda_i(\hat{\bSigma})$.
\begin{proposition}
    \label{eigenvalueProp}
    Suppose $r\leq n$.
    Then, uniformly for $i=1,\ldots, r$, 
\begin{equation*}
        %\lambda_i\left(n^{-1}\bZ^\top \bLambda \bZ\right)
    \lambda_i(\hat{\bSigma})
        =
        \blambda_i
        +
        n^{-1}\mytr(\bLambda_2)
        +O_P\left(\blambda_i \sqrt{\frac{r}{n}}+\sqrt{\frac{\mytr(\bLambda_2^2)}{ n}}+\blambda_{r+1}\right)
    \end{equation*}
        and
\begin{equation*}
     \sum_{i=r+1}^n\lambda_i(\hat{\bSigma})
    =
    \left(1-\frac{r}{n}\right)\mytr(\bLambda_2)
    +O_P\left(r\sqrt{\frac{\mytr(\bLambda_2^2)}{ n}}+r\blambda_{r+1}\right)
    .
\end{equation*}
\end{proposition}

\begin{remark}
Recent works have examined the asymptotic behavior of the spiked eigenvalues of the sample covariance matrix; see, for example, \cite{Yata2013PCA}, \cite{Shen2016A}, \cite{wang2017As}, and \cite{Cai2017Limiting}.
An important improvement of Proposition~\ref{eigenvalueProp} over existing results is that Proposition~\ref{eigenvalueProp} does not impose any conditions on the structure of $\bSigma$, but still gives the correct convergence rate.
\end{remark}

Based on Proposition~\ref{eigenvalueProp}, we propose the following estimators of  $\mytr(\bLambda_2)$ and $\blambda_1,\ldots,\blambda_r$:
\begin{equation*}
    \widehat{\mytr(\bLambda_2)}=\left(1-\frac{r}{n}\right)^{-1}\sum_{i=r+1}^n \lambda_i (\hat{\bSigma})
    ,\quad
    \hat{\blambda}_i=\lambda_i(\hat{\bSigma})-n^{-1}\widehat{\mytr(\bLambda_2)},\quad i=1,\ldots,r.
\end{equation*}
Moreover, we propose the following estimator of $\mytr(\bLambda_2^2)$, which we use in our later analysis:
\begin{equation*}
    \widehat{\mytr(\bLambda_2^2)}=\sum_{i=r+1}^n \left(\lambda_i(\hat{\bSigma})-n^{-1}\widehat{\mytr(\bLambda_2)}\right)^2.
\end{equation*}
The following proposition gives the convergence rate of these estimators.
\begin{proposition}
    \label{eigenvalueProp:R3}
    Suppose $r=o(n)$.
    Then, uniformly for $i=1,\ldots, r$, 
\begin{equation*}
        %\lambda_i\left(n^{-1}\bZ^\top \bLambda \bZ\right)
    \hat{\blambda}_i
        =
        \blambda_i
        +O_P\left(\blambda_i \sqrt{\frac{r}{n}}+\sqrt{\frac{\mytr(\bLambda_2^2)}{ n}}+\blambda_{r+1}\right)
\end{equation*}
and
\begin{align*}
    &\widehat{\mytr(\bLambda_2)}=\mytr(\bLambda_2) + O_P\left(r\sqrt{\frac{\mytr(\bLambda_2^2)}{n}}+r\blambda_{r+1}\right),
        \\
&\widehat{\mytr(\bLambda_2^2)}
        =
         \mytr(\bLambda_2^2)
        +
        O_P\left(\frac{r \mytr(\bLambda_2^2)}{n} + r  \blambda_{r+1}^2\right).
\end{align*}
\end{proposition}
\begin{remark}
    Our estimators of $\blambda_1,\ldots, \blambda_r$ and $\mytr(\bLambda_2)$ are similar to some existing estimators, including the noise-reduction estimators of~\cite{YATA2012193} and the estimators of~\cite{wang2017As}.
    However, their theoretical results require that $r$ is fixed, $p$ is not large, and $\bSigma$ satisfies certain spiked covariance models.
\end{remark}

\begin{remark}
    The estimation of $\mytr(\bLambda_2^2)$ is relatively unexplored.
    Recently, \cite{Aoshima2018} proposed an estimator of $\mytr(\bLambda_2^2)$ based on the cross-data-matrix methodology.
    They also proved the consistency of their estimator.
    However, their method relies on an arbitrary split of the data into two samples of equal size.
    %In comparison, our estimator does not suffer from this problem.
    %Moreover, we give the consistency rate of our estimator.
\end{remark}

Next, we consider the asymptotic behavior of the eigenspaces of $\hat{\bSigma}$.
Let $\bU_{\bY,1}$ denote the first $r$ columns of $\bU_{\bY}$.
Then, the columns of $\bU_{\bY,1}$ are the principal eigenvectors of $\hat{\bSigma}$, and $\bP_{\bY,1}=\bU_{\bY,1}\bU_{\bY,1}^\top$ is the projection matrix onto the rank $r$ principal subspace of $\hat{\bSigma}$.
The properties of $\bP_{\bY,1}$ and the individual principal eigenvectors have been studied extensively.
See \cite{Cai2015Optimal}, \cite{Shen2016A}, and \cite{wang2017As}, and the references therein.
Existing results include the consistency of the principal subspace and the high-order asymptotic behavior of the individual principal eigenvectors.
However, these results are not sufficient for our analysis.
The following proposition gives the high-order asymptotic behavior of $\bP_{\bY,1}$.
To the best of our knowledge, this is a novel result in the literature.



Write $\bY=\bU\bLambda^{1/2}\bZ$, where $\bZ$ is a $p\times n$ random matrix with i.i.d. $\mathcal{N}(0,1)$ entries.
Then, $\bY=\bU_1 \bLambda_1^{1/2} \bZ_1 +\bU_2 \bLambda_2^{1/2} \bZ_2$, where $\bZ_1$ and $\bZ_2$ are the first $r$ rows and the last $p-r$ rows, respectively, of $\bZ$.



\begin{proposition}\label{newEigenvectorPropCor}
    Suppose $r=o(n)$, $\mytr(\bLambda_2)/(n\blambda_r)\to 0$, and $r \blambda_{r+1}/\mytr(\bLambda_2)\to 0$.
    Then,
    \begin{equation*}
        \left\|\bP_{\bY,1} - 
        \bP_{\bY,1}^{\dagger}\right\|
        =O_P\left(\frac{\mytr(\bLambda_2)}{n\blambda_r}+\frac{\blambda_{r+1}}{\blambda_r}\right),
    \end{equation*}
where
$\|\cdot\|$ is the spectral norm,
$
\bP_{\bY,1}^{\dagger}
=\bU_1 \bU_1^\top + \bU_1 \bQ^\top \bU_2^\top
            +\bU_2 \bQ \bU_1^\top
            $, and 
$\bQ
       =
       \bLambda_2^{1/2} \bZ_2 \bZ_1^\top (\bZ_1 \bZ_1^\top)^{-1} \bLambda_1^{-1/2}
       $.
\end{proposition}
\begin{remark}
    The condition $\mytr(\bLambda_2)/(n\blambda_r)\to 0$ is commonly adopted in studies on principal subspaces.
    In fact, when this condition is violated, the principal subspace loses its relation to the rank-$r$ eigenspace of $\bSigma$; see, for example, \cite{Nadler2009Finite}.
\end{remark}
\begin{remark}
    Several high-order Davis--Kahan theorems have been established, for example, Lemma 2 in \cite{koltchinskii2016} and Lemma 2 in \cite{fan2017Dist}.
    These general results explicitly characterize the linear term and the high-order error on the rank-$r$ eigenspace due to matrix perturbation.
    Applying these results to $\hat{\bSigma}$ and $\bSigma$, we can obtain similar results to that given in Proposition \ref{newEigenvectorPropCor};
    however, the above results are slightly weaker and require stronger conditions. 
    %Lemma 2 in \cite{fan2017Dist} uses Frobenius norm and Lemma 2 in \cite{koltchinskii2016} contains an extra factor.
\end{remark}

If $p>n$, let $\bU_{\bY,2}$ be the $r+1$ to $n$th columns of $\bU_{\bY}$.
Then, $\bP_{\bY,2}=\bU_{\bY,2}\bU_{\bY,2}^\top$ is the projection matrix onto the eigenspace spanned by the $r+1$ to $n$th eigenvectors of $\hat{\bSigma}$.
Our later analysis also requires the asymptotic properties of $\bP_{\bY,2}$, which have not been considered in the literature.
    Let $\bV_{\bZ_1}=\bZ_1^\top (\bZ_1 \bZ_1^\top)^{-1/2}$.
    Then, $\bV_{\bZ_1}\bV_{\bZ_1}^\top=\bZ_1^\top (\bZ_1 \bZ_1^\top)^{-1}\bZ_1$ is the projection matrix onto the row space of $\bZ_1$.
    Let $\tilde{\bV}_{\bZ_1}$ be an $n\times (n-r)$ column orthogonal matrix that satisfies $\tilde{\bV}_{\bZ_1}\tilde{\bV}_{\bZ_1}^\top= \bI_{n}-\bV_{\bZ_1}\bV_{\bZ_1}^\top$.
    The following proposition gives the asymptotic behavior of $\bP_{\bY,2}$.

\begin{proposition}
    \label{eigenvectorprop3}
    Suppose $r=o(n)$, $\mytr(\bLambda_2)\blambda_1/(n\blambda_r^2)\to 0$, and $n\blambda_{r+1} /\mytr(\bLambda_2)\to 0$. Then,
    \begin{equation*}
            \left\|\bP_{\bY,2}-
            \bP_{\bY,2}^{\dagger}
            \right\|
    = 
    O_P\left(
        \sqrt{\frac{\mytr(\bLambda_2) \blambda_1}{n\blambda_r^2}}
    +
    \sqrt{\frac{n\blambda_{r+1}}{\mytr(\bLambda_2)}}\right),
    \end{equation*}
    where $
            \bP_{\bY,2}^{\dagger}=
            \left(\mytr(\bLambda_2)\right)^{-1}
            \bU_2 \bLambda_2^{1/2}\bZ_{2} \tilde{\bV}_{\bZ_1}
            \tilde{\bV}_{\bZ_1}^\top \bZ_2^\top \bLambda_2^{1/2} \bU_2^\top
            $.
\end{proposition}
\begin{remark}
    The condition $\mytr(\bLambda_2)\blambda_1/(n\blambda_r^2)\to 0$ is stronger than the condition $\mytr(\bLambda_2)/(n\blambda_r)\to 0$ in Proposition \ref{newEigenvectorPropCor}.
    These two conditions are equivalent if $\blambda_1$ and $\blambda_r$ are of the same order.
\end{remark}


Now, we are ready to derive the asymptotic properties of $T(\bX)$ under the spiked covariance.
Let $\bW^*_{k-1}$ be a $(k-1)\times (k-1)$ symmetric random matrix, distributed as $\textrm{Wishart}(r,\bI_{k-1})$ and independent of $\bW_{k-1}$, where $\textrm{Wishart}(m,\bPsi)$ is the Wishart distribution with parameter $\bPsi$ and $m$ degrees of freedom.
The following theorem gives the asymptotic distribution of $T(\bX)$ under the null and local alternative hypotheses.
\begin{theorem}\label{thm1}
    Suppose $r=o(\sqrt{n})$, $r\mytr(\bLambda_2)\blambda_1/(n\blambda_r^2)\to 0$, $rn\blambda_{r+1} /\mytr(\bLambda_2)\to 0$,
    $r\blambda_{r+1}/\sqrt{\mytr(\bLambda_2^2)}\to 0$, and $\blambda_{r+1}-\blambda_p=O(n^{-1}\sqrt{\mytr(\bLambda_2^2)})$.
    Then,
    \begin{enumerate}[(i)]
        \item 
            under the null hypothesis $\bTheta \bC=\bO_{p\times (k-1)}$,
\begin{equation*}
    \begin{split}
&
\frac{
    T(\bX)
    -
    \left((1+r/n)\mytr(\bLambda_2)-n\mytr(\bLambda_2^2)/\mytr(\bLambda_2)\right)
}{
    \sqrt{
        rn^{-2}\mytr^2(\bLambda_2)+ 
        \mytr(\bLambda_2^2)
    }
}
\\
\sim &
\lambda_1
\left(
\frac{
    n^{-1} \mytr(\bLambda_2)
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
(\bW_{k-1}^* - r\bI_{k-1})
\right.
\\
&\quad\quad+
\left.
\frac{
    \sqrt{\mytr(\bLambda_2^2)}
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
\bW_{k-1}
\right)
+o_P(1);
    \end{split}
\end{equation*}
        \item
            if $r\to \infty$ or $\mytr(\bLambda_2)/(n \sqrt{\mytr(\bLambda_2^2)})\to 0$, then under the local alternative hypothesis $\|\bC^\top \bTheta^\top \bTheta \bC\|=O(\sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
            })$,
\begin{equation*}
    \begin{split}
&
\frac{
    T(\bX)
    -
    \left((1+r/n)\mytr(\bLambda_2)-n\mytr(\bLambda_2^2)/\mytr(\bLambda_2)\right)
}{
    \sqrt{
        rn^{-2}\mytr^2(\bLambda_2)+ 
        \mytr(\bLambda_2^2)
    }
}
\\
\sim &
\lambda_1
\left(
\frac{
    n^{-1} \mytr(\bLambda_2)
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
(\bW_{k-1}^* - r\bI_{k-1})
\right.
\\
&
\quad\quad
+
\frac{
    \sqrt{\mytr(\bLambda_2^2)}
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
\bW_{k-1}
\\
&
\quad\quad
+\left.
\frac{
    \bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
\right)
+o_P(1).
    \end{split}
\end{equation*}
    \end{enumerate}
\end{theorem}
\begin{remark}
    Suppose the approximate factor model in \cite{Fan2013Large} holds.
    That is, $r$ is fixed, $\blambda_1,\ldots, \blambda_r$ diverge at rate $O(p)$, and $\blambda_{r+1},\ldots, \blambda_p$ are bounded.
    Then, the conditions of Theorem \ref{thm1} become $p/n\to \infty$ and $\blambda_{r+1}-\blambda_p=O(\sqrt p /n)$.
    Hence, Theorem \ref{thm1} holds for ultrahigh-dimensional data.
    In contrast, recent tests under the spiked covariance model can only be used for lower-dimensional data.
    In fact, under the approximate factor model in \cite{Fan2013Large}, \cite{Aoshima2018} requires $p/n \to 0$, and \cite{WANG2018225} requires $p/n^2\to 0$ and $\blambda_{r+1}=\cdots=\blambda_p$.   
    Note that if $k=2$ and $p/n^2 \to 0$, then the coefficient of $\bW_{k-1}^* - r\bI_{k-1}$ is negligible, and, as a result, $T(\bX)$ is asymptotically normally distributed.
    Thus, Theorem \ref{thm1} gives the high-order behavior of $T(\bX)$.

\end{remark}

Now, we formulate a test procedure with an asymptotically correct level.
Define the standardized statistic as
\begin{equation*}
    Q_2=
\frac{
    T(\bX)
    -
    \left((1+r/n)\widehat{\mytr(\bLambda_2)}-n\widehat{\mytr(\bLambda_2^2)}/\widehat{\mytr(\bLambda_2)}\right)
}{
    \sqrt{
        rn^{-2}(\widehat{\mytr(\bLambda_2)})^2+ 
        \widehat{\mytr(\bLambda_2^2)}
    }
}.
\end{equation*}
Let $
F_2(x;\mytr(\bLambda_2),\mytr(\bLambda_2^2))
$ be the cumulative distribution function of
\begin{equation*}
    \begin{split}
&\lambda_1
\left(
\frac{
    n^{-1} \mytr(\bLambda_2)
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
(\bW_{k-1}^* - r\bI_{k-1})
\right.
\\
&
\quad \quad
+
\left.
\frac{
    \sqrt{\mytr(\bLambda_2^2)}
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
\bW_{k-1}
\right).
    \end{split}
\end{equation*}
Then, we reject the null hypothesis if
\begin{equation*}
    Q_2
    >
    F_2^{-1}\left(1-\alpha;\widehat{\mytr(\bLambda_2)},\widehat{\mytr(\bLambda_2^2)}\right).
\end{equation*}
The following corollary shows that this test procedure has an asymptotically correct level, as well as giving the asymptotic local power function.
\begin{corollary}
    Suppose the conditions of Theorem \ref{thm1} hold.
    Then,
    \begin{enumerate}[(i)]
        \item 
            under the null hypothesis $\bTheta \bC=\bO_{p\times (k-1)}$,
\begin{equation*}
    \Pr
    \left(
        Q_2
    >
    F_2^{-1}\left(1-\alpha;\widehat{\mytr(\bLambda_2)},\widehat{\mytr(\bLambda_2^2)}\right)
\right)=\alpha +o(1)
        ;
\end{equation*}
        \item
            if $r\to \infty$ or $\mytr(\bLambda_2)/(n \sqrt{\mytr(\bLambda_2^2)})\to 0$, then under the local alternative hypothesis $\|\bC^\top \bTheta^\top \bTheta \bC\|=O(\sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
            })$,
\begin{equation*}
\begin{split}
    &\Pr
    \left(
        Q_2
    >
    F_2^{-1}\left(1-\alpha;\widehat{\mytr(\bLambda_2)},\widehat{\mytr(\bLambda_2^2)}\right)
\right)
\\
=
    &\Pr
    \Bigg(
\lambda_1
\left(
\frac{
    n^{-1} \mytr(\bLambda_2)
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
(\bW_{k-1}^* - r\bI_{k-1})
\right.
\\
&\quad\quad\quad\quad+
\frac{
    \sqrt{\mytr(\bLambda_2^2)}
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
\bW_{k-1}
\\
&\left.
\quad\quad\quad\quad
+
\frac{
    \bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
\right)
\\
    &\quad\quad>
    F_2^{-1}\left(1-\alpha;\mytr(\bLambda_2),\mytr(\bLambda_2^2)\right)
\Bigg)
+o(1)
.
\end{split}
\end{equation*}
    \end{enumerate}
    \label{kuCor2}
\end{corollary}


To gain some insight into the asymptotic behavior of $T(\bX)$, we consider $k=2$ and compare the power of the LFD test with that of~\cite{Bai1996Efiect} and~\cite{Chen2010A}.
Corollary \ref{kuCor2} implies that if
\begin{equation*}
    \liminf_{n\to \infty}\frac{
    \bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
>0,
\end{equation*}
then the LFD test has nontrivial power, asymptotically.
In contrast, if
\begin{equation*}
    \limsup_{n\to \infty}\frac{
    \bC^\top \bTheta^\top \bTheta \bC
}{
    \sqrt{
        \mytr(\bSigma^2)
    }
}
=0,
\end{equation*}
then the tests in~\cite{Bai1996Efiect} and~\cite{Chen2010A} exhibit trivial power, asymptotically.
To compare 
$
    \bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC
$
and
$
    \bC^\top \bTheta^\top \bTheta \bC
$,
we temporarily place a prior on $\bTheta$.
Suppose $\sqrt{n_i} \theta_i$ has prior distribution $\mathcal{N}_p(\mathbf{0}_p,\psi \bI_p)$, for $i=1,2$.
Then,
$
\psi^{-1}\bC^\top \bTheta^\top \bTheta \bC$
follows a $\chi^2$ distribution with $p$ degrees of freedom.
%$ \text{Wishart}_{k-1}(p,\bI_{k-1})$, the $(k-1)$-dimensional Wishart distribution with degree of freedom $p$ and parameter $\bI_{k-1}$.
On the other hand,
$\psi^{-1}\bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC$ follows a 
%$\text{Wishart}_{k-1}(p-n+k,\bI_{k-1})$.
$\chi^2$ distribution with $p-r$ degrees of freedom.
Thus, we have
$$
\frac{\bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC}{\bC^\top \bTheta^\top \bTheta \bC}\xrightarrow{P}1.
$$
Therefore, on average, the signal contained in $\bC^\top \bTheta^\top \bU_2 \bU_2^\top \bTheta \bC$ is roughly the same as that in $\bC^\top \bTheta^\top \bTheta \bC$.
Now, we compare the asymptotic variance.
It is not hard to see that under the conditions of Theorem \ref{thm1}, we have
    ${rn^{-2} \mytr^2 (\bLambda_2)}/{
        \mytr(\bSigma^2)
    }\to 0$.
    Also, if $\blambda_1, \dots, \blambda_r$ are sufficiently large, then $\mytr(\bLambda_2^2)/\mytr(\bSigma^2) \to 0$.
    Hence, it can be expected that
\begin{equation*}
    \frac{rn^{-2} \mytr^2 (\bLambda_2)  + \mytr(\bLambda_2^2)}{
        \mytr(\bSigma^2)
    }\to 0.
\end{equation*}
That is, the asymptotic variance of $T(\bX)$ is typically much smaller than those of the tests in \cite{Bai1996Efiect} and \cite{Chen2010A}.
To appreciate this, note that in the expression \eqref{statisticForm1},
$(\bI_p-\bP_\bY)\bX \bJ \bC|\bP_{\bY}\sim \mathcal{N}_p(\mathbf{0}_p,(\bI_p-\bP_\bY)\bSigma(\bI_p-\bP_\bY))$.
However, $\bI_p-\bP_\bY$ tends to be orthogonal to $\bU_1 \bU_1^\top$, which is the projection matrix onto the eigenspace corresponding to the leading eigenvalues of $\bSigma$.
Hence, the projection by $\bI_p-\bP_\bY$ helps reduce the variance of $\bX \bJ \bC$.

Thus, if $\bTheta$ satisfies
\begin{equation*}
    \liminf_{n\to \infty}\frac{
    \bC^\top \bTheta^\top \bTheta \bC
}{
    \sqrt{
        rn^{-2} \mytr^2 (\bLambda_2) + \mytr(\bLambda_2^2)
    }
}
>0,
\quad
    \limsup_{n\to \infty}\frac{
    \bC^\top \bTheta^\top \bTheta \bC
}{
    \sqrt{
        \mytr(\bSigma^2)
    }
}
=0,
\end{equation*}
then the LFD test has nontrivial power, whereas the tests in~\cite{Bai1996Efiect} and~\cite{Chen2010A} exhibit trivial power.
Hence, the LFD test tends to be more powerful than those of~\cite{Bai1996Efiect} and~\cite{Chen2010A}.


In practice, we may not know whether the covariance matrix is spiked. Furthermore, even if we know that it is spiked, the spike number $r$ may be unknown.
Therefore, we propose an adaptive test procedure.
%Now we consider the detection of the spiked covariance and the estimation of $r$.
Note that Theorem \ref{fenTheorem1} requires $n\blambda_1/\mytr(\bSigma)\to 0$, and Theorem \ref{thm1} requires $\mytr(\bLambda_2)/n\blambda_r\to 0$ and $n\blambda_{r+1}/\mytr(\bLambda_2)\to 0$.
This motivates us to consider the following adaptive test procedure.
Let $\tau>1$ be a hyperparameter.
If
\begin{equation*}
    \frac{
        n\lambda_1(\hat{\bSigma})
    }{
\mytr(\hat{\bSigma})}
<\tau,
\end{equation*}
then we reject the null hypothesis if $Q_1 > F^{-1}(1-\alpha)$.
Otherwise, we reject the null hypothesis if $Q_2> F_2^{-1}(1-\alpha;\widehat{\mytr(\bLambda_2,)},\widehat{\mytr(\bLambda_2^2)})$, where the unknown $r$ is substituted by the estimator
\begin{equation*}
    \hat{r}=\min
    \left\{
1\leq i< n:
    \frac{
        n\lambda_{i+1}(\hat{\bSigma})
}
{    
    \sum_{j=i+1}^n
\lambda_j(\hat{\bSigma})}
<
\tau
\right\}
.
\end{equation*}
We have the following proposition.
\begin{proposition}\label{numberConsistency}
    Let $\tau>1$ be a constant.
    \begin{enumerate}[(i)]
        \item 
    Under the conditions of Theorem \ref{fenTheorem1}, 
    \begin{equation*}
        \Pr\left(
    \frac{
        n\lambda_1(\hat{\bSigma})
    }{
    \mytr(\hat{\bSigma})}
<\tau
\right)\to 1
;
    \end{equation*}
\item
    Under the conditions of Theorem \ref{thm1},
    \begin{equation*}
        \Pr\left(
    \frac{
        n\lambda_1(\hat{\bSigma})
    }{
    \mytr(\hat{\bSigma})}
<\tau
\right)\to 0,\quad
\Pr(\hat{r}= r)\to 1
.
    \end{equation*}
    \end{enumerate}
\end{proposition}
Proposition \ref{numberConsistency} implies that the spiked covariance structure can be detected consistently.
Therefore, the proposed adaptive LFD test procedure can indeed adapt to the unknown covariance structure.


%\section{Schott's method}

%$$
%E=ZZ^\top-\sum_{i=1}^k n_i \bar{X}_i \bar{X}_i^\top.
%$$

%$$
%H=\sum_{i=1}^{k} n_i \bar{X}_i \bar{X}_i^\top - n\bar{X}\bar{X}^\top.
%$$

%$$
%\mytr E = \mytr Z^\top Z - \mytr J^\top Z^\top Z J.
%$$


%$$
%\mytr H = \mytr J^\top Z^\top Z J - \frac{1}{n} 1_n^\top Z^\top Z 1_n
%$$

%$$
%T_{SC}=\frac{1}{\sqrt{n-1}}(
%\frac{1}{k-1}\mytr H-\frac{1}{n-k} \mytr E
%)
%$$
%
%Our new test statistic comes from construction ii in step 1, the likelihood ratio test statistics in step 2 and strategy II in step 3.
%Theorems~\ref{nonSpiked} and~\ref{thm1} allow us to analyze the properties of the proposed test.
%Suppose $\sqrt{n_i}\mu_i$ is from prior distribution $\mathcal{N}_p(0,\psi \bI_p)$, $i=1,\ldots, k$.
%Then $\psi^{-1}\bC^\top \bTheta^\top \bTheta \bC$ is distributed as $\text{Wishart}_{k-1}(p,\bI_{k-1})$ (Wishart distribution with freedom $p$ and parameter $\bI_{k-1}$) and $\psi^{-1}\bC^\top \bTheta^\top \bP_{\bY}\bTheta \bC$ is distributed as $\text{Wishart}_{k-1}(n-k,\bI_{k-1})$.
%In this case, we have
%$$
%\psi^{-1}\bC^\top \bTheta^\top (\bI_P-\bP_{\bY})\bTheta \bC=
%(1+o_P(1))\psi^{-1}\bC^\top \bTheta^\top \bTheta \bC.
%$$
%If the conditions of Theorem~\ref{nonSpiked} hold and $k=2$, the asymptotic power of the proposed test is the same as that of~\cite{Bai1996Efiect}and~\cite{Chen2010A}'s method.
%Since the method of~\cite{Schott2007Some} is a direct generalization of~\cite{Bai1996Efiect}'s method, it can be shown the asymptotic power of the proposed test is the same as that of~\cite{Schott2007Some} for general $k$.
%Next, suppose the covariance matrix is spiked and the conditions of Theorem~\ref{thm1} hold.
%Theorem~\ref{thm1} implies that the proposed test does not depend on large eigenvalues $\lambda_1,\ldots,\lambda_r$ while other existing test procedures are negatively affected by large eigenvalues $\lambda_1,\ldots,\lambda_r$.   
%Thus, the new test has particular good power behavior when $\lambda_1,\ldots,\lambda_r$ are large.
% This property is not surprising since our statistic is from construction ii.
%As a result, our statistic has a wider applicable range compared with the tests from construction i.


%%%%%%%%%%%%%%%%% Hotelling-Lawley trace %%%%%%%%%%%%%%%%%%%%%%%%%
%Another classical test statistic, Hotelling-Lawley trace,  can also be derived by Roy's union intersection principle. This is shown by~\cite{Mudholkar1974}.
 %In that paper, they consider the transformed data $\{M^\top \bX: M \textrm{ is } \text{$(k-1)\times p$ matrix}\}$ and the decomposition of hypotheses:
 %$$H_{0}=\bigcap_{M}H_{0M} \quad\text{and}\quad H_1=\bigcup_{M}H_{1M} ,$$
 %where
 %$$
 %H_{0M}: \mytr( M \bTheta C) = 0\quad \text{and}\quad H_{1M} : \mytr( M \bTheta C )> 0.
 %$$
%
%Note that $\myE Z=\bTheta J^\top$, hence
%the uniformly minimum variance unbiased estimator of $\mytr(M\bTheta C)$ is $\mytr(MZJ C)$.
%It can be seen that
%$
%\mytr \big(MZJ C\big)
%\sim
%\mathcal{N}\big(\mytr(M\bTheta C),\mytr(M\bSigma M^\top )\big)
%$.

%=====================
%$$
%\mytr \big(MZJC\big)
%=
%\mytr \big(CMZJ\big)
%$$
%$ZJ=(\sqrt{n_1}\bar{\bX}_1,\ldots,\sqrt{n_k}\bar{\bX}_k)$.
%Note that $CM\sqrt{n_i}\bar{\bX}_i\sim \mathcal{N}_{k-1}(\sqrt{n_i}CM\theta_i,CM\bSigma M^\top C^\top)$.
%Hence we have that
%$$
%\mytr \big(CMZJ\big)
%\sim
%\mathcal{N}(\mytr(CM\bTheta),\mytr(CM\bSigma M^\top C^\top))
%\sim
%\mathcal{N}(\mytr(M\bTheta C),\mytr(M\bSigma M^\top )).
%$$
%==================

%Hence it's natural to use one side $t$ type statistic
%$$
%T_M = \frac{
%\mytr \big(MZJC\big)
%}{
    %\sqrt{\mytr(M G M^\top)}
%}
%$$
%to test $H_M$ against $K_M$.



%By Cauchy inequality $\max_B \mytr(AB^\top)/\mytr^{1/2}(BB^\top)=\mytr^{1/2}(AA^\top)$, we have
%$$
%\begin{aligned}
    %\max_M T_M &=\max_M \frac{\mytr \big(MG^{1/2}G^{-1/2}ZJC\big)
%}{\sqrt{\mytr(M G^{1/2} (M G^{1/2})^\top)}
%}
    %=\mytr^{1/2}((ZJC)^\top G^{-1}ZJC)\\
    %&=\mytr^{1/2}( ZJC(ZJC)^\top G^{-1})
    %=\mytr^{1/2}(H G^{-1}).
%\end{aligned}
%$$

%====================
\section{Numerical study}\label{numerical}
\setcounter{equation}{0} %-1

In this section, we compare the numerical performance of the adaptive LFD test procedure with that of the MANOVA tests in~\citet{Schott2007Some},~\citet{Cai2014High}, \cite{Hu2017}, and \cite{ZHANG2017200}.
These competing tests are denoted by Sc, CX, HBWW, and ZGZ, respectively.
Throughout the simulations, we take the nominal test level $\alpha =0.05$ and the group number $k=3$.
For the adaptive LFD test, we take $\tau=5$.
For CX, we use their oracle procedure.
All simulation results are based on $5000$ replications.

First, we simulate the empirical level and power under various models of $\bSigma$ and $\bTheta$.
To characterize the signal strength,
we define the signal-to-noise ratio (SNR) as
$$
\textrm{SNR}=\frac{ \bC^\top \bTheta^\top \bTheta\bC}{\sqrt{\mytr(\bSigma^2)}}.
$$
We consider four models for $\bSigma$, where the first two are nonspiked, and the last two are spiked.
\begin{itemize}
    \item Model I:
        $\bSigma= \bI_p$.
    \item Model II:
        $\bSigma = (\sigma_{ij})$, where $\sigma_{ij}=0.6^{|i-j|}$.
    \item Model III:
        $\bSigma= \bU \bLambda \bU^\top$, where $\bU$ is a $p\times p$ orthogonal matrix generated from the Haar distribution and $\bLambda=\mydiag(3p,2p,p,1,\ldots,1)$.
    \item Model IV:
        $\bSigma=  \bU \bLambda \bU^\top+\bA \bA^\top$, where $\bU$ is a $p\times p$ orthogonal matrix generated from the Haar distribution, $\bLambda=\mydiag(p,p,1,\ldots,1)$, and $\bA$ is a $p\times p$ matrix, the elements of which are independently generated from the Bernoulli distribution with success probability $0.01$.
\end{itemize}
%and the two sample tests of~\cite{Srivastava2007Multivariate},~\cite{Chen2010A},~\cite{Tony2013} and~\cite{Feng2014Two}.
Under the null hypothesis, we always take $\theta_1=\cdots=\theta_k=\mathbf{0}_p$.
We consider two structures for the alternative hypotheses: the nonsparse alternative, and the sparse alternative.
In the nonsparse case, we take $\theta_1=\kappa \mathbf 1_p$, $\theta_2=-\kappa \mathbf 1_p$, and $\theta_3=\mathbf{0}_p$, where $\kappa$ is selected to make the SNR equal to specific values.
In the sparse case, we take $\theta_1=\kappa (\mathbf 1_{p/5}^\top,\mathbf{0}_{4p/5}^\top)^\top$, $\theta_2=\kappa (\mathbf{0}_{p/5}^\top, \mathbf 1_{p/5}^\top,\mathbf{0}_{3p/5}^\top)^\top$, and $\theta_3=\mathbf{0}_p$.
Again, $\kappa$ is selected to make the SNR equal to specific values.
The simulation results are summarized in Figures \ref{newfigure1}--\ref{newfigure4}, and show that in all scenarios, the empirical sizes of the LFD test are reasonably close to the nominal level $0.05$.
Under model I and model II, where the covariance matrices are nonspiked, the empirical power of the LFD test is slightly lower than that of the sum-of-squares tests, but is higher than that of the CX test.
%Nevertheless, the power performance of the LFD test improves as $n$ and $p$ increase.
Under model III and model IV, where the covariance matrices are spiked, the empirical power of the LFD test is significantly higher than that of the sum-of-squares tests.
In addition, the LFD test exhibits higher empirical power than that of the CX test in most cases, except for model IV with sparse means.
These simulation results verify our theoretical results that the LFD test is particularly powerful under the spiked covariance.

In our second simulation study, we investigate the effect of correlations between the variables.
%We take $k=2$ so that we can compare our test with some existing two sample tests.
%For comparison, we carry out simulations for the test of~\cite{Srivastava2007Multivariate},~\cite{Chen2010A},~\cite{Tony2013} and~\cite{Feng2014Two}.
%We denote these tests by Sr, CQ, CLX and FZWZ, respectively.
We consider the compound symmetry structure; that is, the diagonal elements of $\bSigma$ are one, and the off-diagonal elements are $\rho$, with $0\leq \rho<1$.
The parameter $\rho$ characterizes the correlations between the variables.
We take 
$\theta_1=\kappa (\mathbf 1_{p/5}^\top,\mathbf{0}_{4p/5}^\top)^\top$, $\theta_2=\kappa (\mathbf{0}_{p/5}^\top, \mathbf 1_{p/5}^\top,\mathbf{0}_{3p/5}^\top)^\top$,
and $\theta_3=\mathbf{0}_p$, where $\kappa$ is selected such that ${ \bC^\top \bTheta^\top \bTheta\bC}/(\sum_{i=2}^p\blambda_i^2)^{1/2}=5$.
Figure~\ref{figure1} plots the empirical power for various tests versus $\rho$.
We can see that the empirical power of the LFD test remains nearly constant as $\rho$ varies, whereas the empirical power of the competing sum-of-squares tests decreases rapidly as $\rho$ increases.
When $\rho$ is nonzero, the LFD test outperforms the competing tests significantly.

\begin{figure}[htbp]
    \centering
    \subfigure[Model I, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t1l}
}
    \subfigure[Model II, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t1r}
}
    \\
    \subfigure[Model I, sparse case]{
        \includegraphics[width=0.45\textwidth]{t1lb}
}
    \subfigure[Model II, sparse case]{
        \includegraphics[width=0.45\textwidth]{t1rb}
}
    \caption{Empirical size and power of tests under model I and model II; $n_1=n_2=n_3=20$, $p=300$. }
    \label{newfigure1}
\end{figure}
\begin{figure}[htbp]
    \centering
    \subfigure[Model I, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t3l}
}
    \subfigure[Model II, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t3r}
}
    \\
    \subfigure[Model I, sparse case]{
        \includegraphics[width=0.45\textwidth]{t3lb}
}
    \subfigure[Model II, sparse case]{
        \includegraphics[width=0.45\textwidth]{t3rb}
}
    \caption{Empirical size and power of tests under model I and model II; $n_1=n_2=n_3=25$, $p=800$. }
    \label{newfigure2}
\end{figure}
\begin{figure}[htbp]
    \centering
    \subfigure[Model III, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t2l}
}
    \subfigure[Model IV, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t2r}
}
    \\
    \subfigure[Model III, sparse case]{
        \includegraphics[width=0.45\textwidth]{t2lb}
}
    \subfigure[Model IV, sparse case]{
        \includegraphics[width=0.45\textwidth]{t2rb}
}
    \caption{Empirical size and power of tests under model III and model IV; $n_1=n_2=n_3=20$, $p=300$. }
    \label{newfigure3}
\end{figure}
\begin{figure}[htbp]
    \centering
    \subfigure[Model III, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t4l}
}
    \subfigure[Model IV, nonsparse case]{
        \includegraphics[width=0.45\textwidth]{t4r}
}
    \\
    \subfigure[Model III, sparse case]{
        \includegraphics[width=0.45\textwidth]{t4lb}
}
    \subfigure[Model IV, sparse case]{
        \includegraphics[width=0.45\textwidth]{t4rb}
}
    \caption{Empirical size and power of tests under model III and model IV; $n_1=n_2=n_3=25$, $p=800$. }
    \label{newfigure4}
\end{figure}



%$$
%SNR=\frac{\|\bTheta \bC\|_F^2}{\sqrt{\mytr (\bSigma^2)}}
%$$


%\begin{table}[!hbp]
%    \caption{Empirical sizes and powers of tests. $\alpha=0.05$, $n_1=n_2=n_3=20$, $p=300$. }
%    \label{table1}
%    \centering
%    \begin{tabular}{*{11}{l}}
%    \toprule
%    \multirow{2}{*}{SNR} &\multicolumn{5}{c}{Model $1$}&
%    \multicolumn{5}{c}{Model $2$} \\
%        \cmidrule(r){2-6}\cmidrule(r){7-11}
%& Sc & CX & HBWW & ZGZ & LFD & Sc & CX & HBWW & ZGZ & LFD \\ 
%    \midrule
%    \multicolumn{5}{l}{Non-sparse case}
%    \\
%    \midrule
%0 & 0.0544 & 0.0362 & 0.0554 & 0.0542 & 0.0446 & 0.0604 & 0.0316 & 0.0598 & 0.0578 & 0.0554 \\ 
%  0.2 & 0.0672 & 0.0350 & 0.0664 & 0.0670 & 0.0500 & 0.0710 & 0.0340 & 0.0704 & 0.0690 & 0.0640 \\ 
%  0.4 & 0.0828 & 0.0380 & 0.0820 & 0.0826 & 0.0610 & 0.0874 & 0.0364 & 0.0872 & 0.0858 & 0.0700 \\ 
%  0.8 & 0.1078 & 0.0420 & 0.1096 & 0.1078 & 0.0776 & 0.1254 & 0.0324 & 0.1250 & 0.1232 & 0.0930 \\ 
%  1.6 & 0.2070 & 0.0490 & 0.2064 & 0.2070 & 0.1594 & 0.2112 & 0.0316 & 0.2112 & 0.2074 & 0.1420 \\ 
%  3.2 & 0.4728 & 0.0628 & 0.4752 & 0.4720 & 0.4280 & 0.4668 & 0.0354 & 0.4666 & 0.4640 & 0.3098 \\ 
%
%    \midrule
%    \multicolumn{5}{l}{Sparse case}
%    \\
%    \midrule
%
%  0 & 0.0516 & 0.0308 & 0.0520 & 0.0516 & 0.0412 & 0.0570 & 0.0324 & 0.0566 & 0.0560 & 0.0544 \\ 
%  0.2 & 0.0662 & 0.0360 & 0.0652 & 0.0662 & 0.0478 & 0.0708 & 0.0340 & 0.0708 & 0.0684 & 0.0632 \\ 
%  0.4 & 0.0782 & 0.0418 & 0.0790 & 0.0780 & 0.0554 & 0.0972 & 0.0336 & 0.0968 & 0.0952 & 0.0714 \\ 
%  0.8 & 0.1042 & 0.0374 & 0.1032 & 0.1040 & 0.0726 & 0.1252 & 0.0322 & 0.1252 & 0.1234 & 0.0856 \\ 
%  1.6 & 0.2074 & 0.0460 & 0.2088 & 0.2072 & 0.1318 & 0.2148 & 0.0314 & 0.2150 & 0.2116 & 0.1378 \\ 
%  3.2 & 0.4670 & 0.0802 & 0.4650 & 0.4668 & 0.3214 & 0.4594 & 0.0318 & 0.4598 & 0.4558 & 0.2718 \\ 
%
%
%
%\bottomrule
%\end{tabular}
%\end{table}
%\begin{table}[!hbp]
%    \caption{Empirical sizes and powers of tests. $\alpha=0.05$, $n_1=n_2=n_3=30$, $p=800$. }
%    \label{table3}
%    \centering
%    \begin{tabular}{*{11}{l}}
%    \toprule
%    \multirow{2}{*}{SNR} &\multicolumn{5}{c}{Model $1$}&
%    \multicolumn{5}{c}{Model $2$} \\
%        \cmidrule(r){2-6}\cmidrule(r){7-11}
%& Sc & CX & HBWW & ZGZ & LFD & Sc & CX & HBWW & ZGZ & LFD \\ 
%    \midrule
%    \multicolumn{5}{l}{Non-sparse case}
%    \\
%    \midrule
%0 & 0.0522 & 0.0244 & 0.0530 & 0.0522 & 0.0460 & 0.0536 & 0.0268 & 0.0534 & 0.0512 & 0.0498 \\ 
%  0.2& 0.0624 & 0.0300 & 0.0626 & 0.0624 & 0.0546 & 0.0722 & 0.0250 & 0.0712 & 0.0704 & 0.0616 \\ 
%  0.4& 0.0792 & 0.0300 & 0.0782 & 0.0792 & 0.0686 & 0.0790 & 0.0286 & 0.0798 & 0.0774 & 0.0726 \\ 
%  0.8& 0.1142 & 0.0308 & 0.1152 & 0.1142 & 0.0962 & 0.1172 & 0.0278 & 0.1170 & 0.1154 & 0.0992 \\ 
%  1.6& 0.1990 & 0.0356 & 0.1984 & 0.1990 & 0.1882 & 0.2080 & 0.0228 & 0.2074 & 0.2058 & 0.1608 \\ 
%  3.2& 0.4710 & 0.0476 & 0.4710 & 0.4710 & 0.4954 & 0.4772 & 0.0328 & 0.4796 & 0.4754 & 0.3978 \\ 
%
%
%    \midrule
%    \multicolumn{5}{l}{Sparse case}
%    \\
%    \midrule
%
%    
%  0& 0.0502 & 0.0278 & 0.0498 & 0.0502 & 0.0448 & 0.0570 & 0.0302 & 0.0570 & 0.0556 & 0.0484 \\ 
%  0.2& 0.0652 & 0.0254 & 0.0662 & 0.0652 & 0.0612 & 0.0630 & 0.0236 & 0.0626 & 0.0626 & 0.0624 \\ 
%  0.4& 0.0712 & 0.0252 & 0.0714 & 0.0712 & 0.0636 & 0.0856 & 0.0232 & 0.0862 & 0.0854 & 0.0704 \\ 
%  0.8& 0.1096 & 0.0286 & 0.1104 & 0.1096 & 0.0848 & 0.1114 & 0.0250 & 0.1122 & 0.1104 & 0.0900 \\ 
%  1.6& 0.1992 & 0.0368 & 0.1992 & 0.1990 & 0.1550 & 0.2068 & 0.0294 & 0.2054 & 0.2038 & 0.1452 \\ 
%  3.2& 0.4822 & 0.0422 & 0.4828 & 0.4822 & 0.3836 & 0.4722 & 0.0246 & 0.4718 & 0.4696 & 0.3230 \\ 
%
%
%
%
%\bottomrule
%\end{tabular}
%\end{table}
%
%\begin{table}[!hbp]
%    \caption{Empirical sizes and powers of tests. $\alpha=0.05$, $n_1=n_2=n_3=20$, $p=300$. }
%    \label{table2}
%    \centering
%    \begin{tabular}{*{11}{l}}
%    \toprule
%    \multirow{2}{*}{SNR} &\multicolumn{5}{c}{Model $3$}&
%    \multicolumn{5}{c}{Model $4$} \\
%        \cmidrule(r){2-6}\cmidrule(r){7-11}
%& Sc & CX & HBWW & ZGZ & LFD & Sc & CX & HBWW & ZGZ & LFD \\ 
%    \midrule
%    \multicolumn{5}{l}{Non-sparse case}
%    \\
%    \midrule
%
%0 & 0.0724 & 0.0374 & 0.0728 & 0.0634 & 0.0430 & 0.0776 & 0.0352 & 0.0772 & 0.0706 & 0.0530 \\ 
%  0.2& 0.0906 & 0.2228 & 0.0902 & 0.0764 & 0.9990 & 0.0824 & 0.0426 & 0.0824 & 0.0760 & 0.0860 \\ 
%  0.4& 0.0920 & 0.5272 & 0.0938 & 0.0772 & 1.0000 & 0.0982 & 0.0596 & 0.0990 & 0.0910 & 0.1726 \\ 
%  0.8& 0.1214 & 0.9612 & 0.1202 & 0.1002 & 1.0000 & 0.1166 & 0.0932 & 0.1168 & 0.1084 & 0.4016 \\ 
%  1.6& 0.1900 & 1.0000 & 0.1884 & 0.1562 & 1.0000 & 0.1942 & 0.2238 & 0.1930 & 0.1784 & 0.8486 \\ 
%  3.2& 0.4050 & 1.0000 & 0.4078 & 0.3462 & 1.0000 & 0.4164 & 0.5358 & 0.4178 & 0.3826 & 1.0000 \\ 
%
%
%    \midrule
%    \multicolumn{5}{l}{Sparse case}
%    \\
%    \midrule
%
%  0 & 0.0680 & 0.0344 & 0.0688 & 0.0550 & 0.0466 & 0.0694 & 0.0360 & 0.0696 & 0.0636 & 0.0560 \\ 
%  0.2 & 0.0864 & 0.3582 & 0.0860 & 0.0700 & 0.9894 & 0.0876 & 0.0604 & 0.0882 & 0.0804 & 0.0912 \\ 
%  0.4 & 0.0930 & 0.8438 & 0.0932 & 0.0748 & 1.0000 & 0.0908 & 0.1280 & 0.0918 & 0.0840 & 0.1718 \\ 
%  0.8 & 0.1134 & 0.9998 & 0.1138 & 0.0910 & 1.0000 & 0.1200 & 0.3744 & 0.1204 & 0.1086 & 0.4450 \\ 
%  1.6 & 0.1828 & 1.0000 & 0.1818 & 0.1518 & 1.0000 & 0.1794 & 0.7412 & 0.1792 & 0.1628 & 0.9428 \\ 
%  3.2 & 0.4028 & 1.0000 & 0.4032 & 0.3426 & 1.0000 & 0.4104 & 0.9998 & 0.4110 & 0.3806 & 1.0000 \\ 
%
%
%
%\bottomrule
%\end{tabular}
%\end{table}
%
%
%
%\begin{table}[!hbp]
%    \caption{Empirical sizes and powers of tests. $\alpha=0.05$, $n_1=n_2=n_3=30$, $p=800$. }
%    \label{table4}
%    \centering
%    \begin{tabular}{*{11}{l}}
%    \toprule
%    \multirow{2}{*}{SNR} &\multicolumn{5}{c}{Model $3$}&
%    \multicolumn{5}{c}{Model $4$} \\
%        \cmidrule(r){2-6}\cmidrule(r){7-11}
%& Sc & CX & HBWW & ZGZ & LFD & Sc & CX & HBWW & ZGZ & LFD \\ 
%    \midrule
%    \multicolumn{5}{l}{Non-sparse case}
%    \\
%    \midrule
%
%0 & 0.0748 & 0.0250 & 0.0750 & 0.0566 & 0.0426 & 0.0756 & 0.0266 & 0.0750 & 0.0698 & 0.0528 \\ 
%  0.2 & 0.0872 & 0.1984 & 0.0874 & 0.0688 & 1.0000 & 0.0906 & 0.0310 & 0.0900 & 0.0824 & 0.0814 \\ 
%  0.4 & 0.0954 & 0.5260 & 0.0948 & 0.0768 & 1.0000 & 0.0884 & 0.0366 & 0.0886 & 0.0828 & 0.1118 \\ 
%  0.8 & 0.1096 & 0.9672 & 0.1106 & 0.0886 & 1.0000 & 0.1184 & 0.0390 & 0.1192 & 0.1098 & 0.2058 \\ 
%  1.6 & 0.1904 & 1.0000 & 0.1908 & 0.1560 & 1.0000 & 0.1844 & 0.0676 & 0.1838 & 0.1742 & 0.4970 \\ 
%  3.2 & 0.3982 & 1.0000 & 0.3986 & 0.3276 & 1.0000 & 0.4348 & 0.1470 & 0.4346 & 0.4178 & 0.9188 \\ 
%
%
%
%    \midrule
%    \multicolumn{5}{l}{Sparse case}
%    \\
%    \midrule
%
%  0 & 0.0764 & 0.0230 & 0.0770 & 0.0564 & 0.0478 & 0.0718 & 0.0264 & 0.0716 & 0.0688 & 0.0548 \\ 
%  0. & 0.0794 & 0.3896 & 0.0796 & 0.0604 & 1.0000 & 0.0816 & 0.0548 & 0.0802 & 0.0758 & 0.0802 \\ 
%  0.4 & 0.0936 & 0.9104 & 0.0934 & 0.0762 & 1.0000 & 0.0978 & 0.1120 & 0.0976 & 0.0918 & 0.1424 \\ 
%  0.8 & 0.1246 & 1.0000 & 0.1250 & 0.0990 & 1.0000 & 0.1072 & 0.3346 & 0.1080 & 0.1014 & 0.2820 \\ 
%  1.6 & 0.1812 & 1.0000 & 0.1806 & 0.1450 & 1.0000 & 0.1788 & 0.7834 & 0.1796 & 0.1702 & 0.7376 \\ 
%  3.2 & 0.4058 & 1.0000 & 0.4060 & 0.3324 & 1.0000 & 0.4212 & 0.9998 & 0.4208 & 0.4028 & 0.9992 \\ 
%
%
%
%\bottomrule
%\end{tabular}
%\end{table}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{figure1}
    \caption{Empirical power of tests; $n_1=n_2=n_3=35$, $p=1000$.}
    \label{figure1}
\end{figure}





\section{Concluding remarks}\label{concluding}
\setcounter{equation}{0} %-1
Using the idea of the least favorable direction, we have proposed an LFD test for MANOVA in the high-dimensional setting.
We have derived the asymptotic distribution of the LFD test statistic under both nonspiked and spiked covariances.
The asymptotic local power functions are also given.
Our theoretical results and simulation studies show that the LFD test exhibits power behavior comparable with that of existing tests when the covariance matrix is nonspiked, and tends to be much more powerful than existing tests when the covariance matrix is spiked.

Several interesting, but challenging problems remain.
First, for the case of an unknown covariance structure,
we proposed an adaptive LFD test procedure by consistently detecting the unknown covariance structure and estimating the unknown $r$.
However, this procedure relies on a hyperparameter $\tau$. 
Determining an optimal $\tau$ remains an interesting problem.
Second, our theoretical results rely on the normality of the observations.
In fact, our proofs use the independence of $\bX \bJ \bC$ and $\bY$.
Note that $\bX \bJ \bC$ and $\bY =\bX \tilde{\bJ}$ are both linear combinations of independent random vectors $X_{ij}$.
It is known that the independence of linear combinations of independent random variables essentially characterizes the normality of the variables; see, for example, \cite{KAGAN}, Section 3.1.
Hence our strategy is not feasible without the normality assumption.
It is unclear whether the conclusions of our theorems hold without this assumption.
Third, our theoretical results require $p/n \to \infty$.
In fact, the asymptotic behavior of $T(\bX)$ will be different in the regime where $p/n \to$ constant.
Random matrix theory may be useful to investigate the asymptotic behavior of $T(\bX)$ in this regime.
We leave these topics for future research.



%Hence we use permutation method to determine the critical values throughout our simulations.
   %The test procedures resulting from permutation method have exact levels as long as the null distribution of observations are exchangeable~\citep{Romano1990On}.
   %The major down-side to permutation method is that it can be computationally intensive.
   %Fortunately, for LFD test statistic,  the permutation method has a simple implementation.
    %By expression~\eqref{statisticForm2}, a permuted statistic can be written as
    %\begin{equation}\label{permutedStatistic}
        %T(\bX\Gamma)=\lambda_{\max}\Big(\bC^\top{\big( \bJ^\top \Gamma^\top {(\bX^\top \bX)}^{-1} \Gamma \bJ \big)}^{-1}  \bC\Big),
    %\end{equation}
%where $\Gamma$ is an $n\times n$ permutation matrix.
   %Note that ${(\bX^\top \bX)}^{-1}$, the most time-consuming component, can be calculated aforehand.
   %The permutation procedure for LFD test statistic can be summarized as:
   %\begin{enumerate}
       %\item
           %Calculate $T(\bX)$ according to~\eqref{statisticForm2}, keep intermediate result ${(\bX^\top \bX)}^{-1}$.
       %\item For a large $M$, independently generate $M$ random permutation matrix $\Gamma_1,\ldots,\Gamma_M$ and calculate $T(\bX\Gamma_1),\ldots,T(\bX\Gamma_M)$ according to~\eqref{permutedStatistic}. 
       %\item Calculate the $p$-value by
           %$
           %\tilde{p}={(M+1)}^{-1}\big[1+\sum_{i=1}^M I\{T(\bX\Gamma_i)\geq T(\bX)\}\big]
           %$.
           %Reject the null hypothesis if $\tilde{p}\leq \alpha$.
   %\end{enumerate}
%
%Here $M$ is the permutation times. 
   %It can be seen that  step 1 and step 2 cost $O(n^2 p +n^3)$ and $O(n^2 M)$ operations respectively.
   %In large sample or high dimensional setting, step 2 has a negligible effect on total computational complexity.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Supplementary Material}

The online Supplementary Material presents proofs of the propositions and theorems.
\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Acknowledgments}
We thank the associate editor and two anonymous reviewers for their helpful comments and suggestions.
This work was supported by the National Natural Science Foundation of
China under Grant Nos. 11471035 and 11471030.
\par




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\markboth{\hfill{\footnotesize\rm RUI WANG AND XINGZHONG XU} \hfill}
{\hfill {\footnotesize\rm GENERALIZED LIKELIHOOD RATIO TEST} \hfill}

%\iffalse
\bibhang=1.7pc
\bibsep=2pt
\fontsize{9}{14pt plus.8pt minus .6pt}\selectfont
\renewcommand\bibname{\large \bf References}
%\begin{thebibliography}{11}
\expandafter\ifx\csname
natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL}\fi
%\fi

\bibliographystyle{chicago}      % Chicago style, author-year citations
\bibliography{mybibfile}   % name your BibTeX data base

%-------------------------------------------
\vskip .65cm
\noindent
School of Mathematics and Statistics, Beijing Institute of Technology, Beijing, 100081,China
\vskip 2pt
\noindent
E-mail: wangruiphd@bit.edu.cn
\vskip 2pt

\noindent
School of Mathematics and Statistics, Beijing Institute of Technology, Beijing, 100081,China
\\
and
Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing
100081,China
\vskip 2pt
\noindent
E-mail: xuxz@bit.edu.cn

\end{document}
