\documentclass[12pt]{article} %***
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
\usepackage[]{hyperref}  %<----modified by Ivan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{sectsty, secdot}
%\sectionfont{\fontsize{12}{15}\selectfont}
\sectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
\renewcommand{\theequation}{\thesection\arabic{equation}}
\subsectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth=31.9pc
\textheight=46.5pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
%\headheight=.2cm
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}

\usepackage{bm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{IEEEtrantools}
\usepackage{enumerate}



\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}

\newcommand{\ud}{\mathbf{d}}

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}
\def\blambda {\bfsym {\lambda}} 
\def\bLambda {\bfsym {\Lambda}} 
\def\bSigma {\bfsym {\Sigma}} 





\newtheorem{assumption}{Assumption}

\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{}
%\headrulewidth=0pt  %<-modified by Ivan

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{2}

\markright{ \hbox{\footnotesize\rm Statistica Sinica
%{\footnotesize\bf 24} (201?), 000-000
}\hfill\\[-13pt]
\hbox{\footnotesize\rm
%\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
}\hfill }

\markboth{\hfill{\footnotesize\rm Rui Wang AND Xingzhong Xu} \hfill}
{\hfill {\footnotesize\rm GENERALIZED LIKELIHOOD RATIO TEST} \hfill}

\renewcommand{\thefootnote}{}
$\ $\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont \vspace{0.8pc}
\centerline{\large\bf A GENERALIZED LIKELIHOOD RATIO TEST FOR }
\vspace{2pt} \centerline{\large\bf MULTIVARIATE ANALYSIS OF VARIANCE}
\vspace{2pt} \centerline{\large\bf IN HIGH DIMENSION}
\vspace{.4cm} \centerline{Author(s)} \vspace{.4cm} \centerline{\it
Affiliation(s)} \vspace{.55cm} \fontsize{9}{11.5pt plus.8pt minus
.6pt}\selectfont

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{quotation}
\noindent {\it Abstract:}
This paper considers in high dimensional setting a canonical testing problem, namely testing the equality of multiple mean vectors of normal distribution.
    Motivated by Roy's union-intersection principle, we propose a generalized likelihood ratio test.
    To investigate the asymptotic properties of the proposed test, we adopt a spiked covariance model which can characterize the strong correlation between variables.
    The limiting null distribution of the test statistic is derived and the local asymptotic power function is given.
    The asymptotic power function implies that the proposed test has particular high power when there are strong correlation between variables.
 We also carry out simulations to verify our theoretical results.

\vspace{9pt}
\noindent {\it Key words and phrases:}
%Balanced incomplete block design, Hadamard matrix, nearly balanced incomplete block design, orthogonal array.
\par
\end{quotation}\par



\def\thefigure{\arabic{figure}}
\def\thetable{\arabic{table}}

\renewcommand{\theequation}{\thesection.\arabic{equation}}


\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont

\setcounter{section}{1} %***
\setcounter{equation}{0} %-1
\noindent {\bf 1. Introduction}
Suppose there are $k$ ($k\geq 2$) groups of $p$ dimensional data.
Within the $i$th group ($1\leq i\leq k$), we have observations $\{X_{ij}\}_{j=1}^{n_i}$ which are independent and identically distributed (i.i.d.) as $N_p(\xi_i,\bSigma)$, the $p$ dimensional normal distribution with mean vector $\xi_i$ and common variance matrix $\bSigma$.
%Suppose there are $k$ normal populations with possibly different means $\xi_1,\ldots,\xi_k$, but all with the same variance $\bSigma$.
%Suppose we observe $k$ independent random samples, each from the distribution $N_p(\xi_i,\bSigma)$, where $1\leq i\leq k$, $k\geq 2$ is a fixed constant, $\xi_i$ and $\bSigma$ are unknown parameters.
%Denote by $X_{ij}\in \mathbb{R}^p$ the $j$th observation in group $i$, $j=1,\ldots,n_i$, $i=1,\ldots, k$,  where $n_i$ is the samples size of group $i$, $1\leq i \leq k$.
We would like to test the hypotheses
\begin{equation}\label{hypothesis}
    H_0: \xi_1=\xi_2=\cdots=\xi_k\quad \text{v.s.}\quadã€€H_1: \text{$\xi_i\neq \xi_j$ for some $i\neq j$}.
\end{equation}
This testing problem is known as one-way multivariate analysis of variance (MANOVA) and has been well studied when $p$ is small compared to $n$, where $n=\sum_{i=1}^k n_i$ is the total sample size.

Let $\bH=\sum_{i=1}^k n_i (\bar{\bX}_i-\bar{\bX})(\bar{\bX}_i-\bar{\bX})^T$ be the sum-of-squares between groups and $\bG=\sum_{i=1}^k \sum_{j=1}^{n_i}(X_{ij}-\bar{\bX}_i)(X_{ij}-\bar{\bX}_i)^T$ be the sum-of-squares within groups, where $\bar{\bX}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ is the sample mean of group $i$ and $\bar{\bX}=n^{-1}\sum_{i=1}^k\sum_{j=1}^{n_i}X_{ij}$ is the pooled sample mean.
   There are four classical test statistics for hypothesis~\eqref{hypothesis}, which are all based on the eigenvalues of $\bH\bG^{-1}$. 

   %\begin{IEEEeqnarray*}{CC}
       %\text{Wilks' Lambda:} \qquad& |G+H|/|G|\\
       %%\text{Pillai trace:} \qquad& \mytr[H(G+H)^{-1}]\\
       %\text{Hotelling-Lawley trace:}\qquad & \mytr[HG^{-1}]\\
       %\text{Roy's maximum root:} \qquad& \lambda_{\max}(HG^{-1})
   %\end{IEEEeqnarray*}

       \begin{center}
       \begin{tabular}{|cc|}
           \hline
       {Wilks' Lambda:} & $|\bG+\bH|/|\bG|$\\
       {Pillai trace:} & $\mytr[\bH(\bG+\bH)^{-1}]$\\
       {Hotelling-Lawley trace:} & $\mytr[\bH \bG^{-1}]$\\
       {Roy's maximum root:} & $\lambda_{\max}(\bH \bG^{-1})$\\
           \hline
           \end{tabular}
       \end{center}

%There are four classical tests for hypothesis~\eqref{hypothesis}: Wilks' Lambda (which is also the LRT), Hotelling-Lawley trace, Pillai Trace and Roy's maximum root.


In some modern scientific applications, people would like to test hypothesis~\eqref{hypothesis} in high dimensional setting, i.e., $p$ is greater than $n$.
See, for example,~\citet{Tsai2009,Verstynen1209}.
%However, when $p>n-k$, the LRT for hypothesis~\eqref{hypothesis} is not well defined.
However, when $p\geq n$, the four classical test statistics can not be defined.
  Researchers have done extensive work to study the testing problem~\eqref{hypothesis} in high dimensional setting.
 So far, most tests are designed for two sample case, i.e., $k=2$.
  See, for example,~\citet{Bai1996Efiect,Chen2010A,Srivastava2009A,Tony2013,Feng2015Multivariate}.
  For multiple sample case,~\cite{Schott2007Some} modified Hotelling-Lawley trace and proposed the test statistic
  %$$
  %T_{SC}=\frac{1}{\sqrt{n-1}}\big(
  %\frac{1}{k-1}\mytr\big(\sum_{i=1}^k n_i\bar{X}_i\bar{X}_i^T-n\bar{X}\bar{X}^T\big)-\frac{1}{n-k}\mytr\big(\sum_{i=1}^k \sum_{j=1}^{n_i}X_{ij}X_{ij}^T-\sum_{i=1}^k n_i\bar{X}_i\bar{X}_i^T\big)
  %\big),
  %$$
  %where $\bar{\bX}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ and $\bar{\bX}=n^{-1}\sum_{i=1}^k\sum_{j=1}^{n_i}X_{ij}$.
  $$
  T_{SC}=\frac{1}{\sqrt{n-1}}\Big(
  \frac{1}{k-1}\mytr\big(\bH\big)-\frac{1}{n-k}\mytr\big(\bG\big)
  \Big).
  $$
In another work,~\cite{Cai2014High} proposed a test statistic
  $$
  T_{CX}=\max_{1\leq i\leq p} \sum_{1\leq j<l\leq k}\frac{n_j n_l}{n_j+n_l}\frac{(\Omega(\bar{\bX}_j-\bar{\bX}_l))_i^2}{\omega_{ii}},
  $$
  Where $\Omega=(\omega)_{ij}=\bSigma^{-1}$ is the precision matrix. When $\Omega$ is unknown, they substitute it by an estimator $\hat{\Omega}$.
  Stitistics $T_{SC}$ and $T_{CX}$ are the representatives of two popular methodologies for high dimensional tests.
  $T_{SC}$ is a so-called {sum-of-squares} type statistic as it is based on an estimation of squared Euclidean norm $\sum_{i=1}^k n_i\|\xi_i-\bar{\xi}\|^2$, where $\bar{\xi}=n^{-1}\sum_{i=1}^k n_i \xi_i$.
  $T_{CX}$ is an {extreme value type} statistic.
  
  %Suppose $\{X_{i1},\ldots, X_{in_i}\}$ are i.i.d.\ distributed as $N(\xi_i,\bSigma)$ for $1\leq i\leq K$.
%The $k$ samples are independent.
%%$\xi_i$, $i=1\ldots, k$ and $\bSigma>0$ are unknown. An interesting problem in multivariate analysis is to test the hypotheses
%\begin{equation}
    %H: \xi_1=\xi_2=\cdots=\xi_k\quad v.s.\quad K: \textrm{$\xi_i\neq \xi_j$ for some $i\neq j$}.
%\end{equation}
%The likelihood ratio test (LRT) has a dominated position in classical multivariate analysis.
   Note that both sum-of-squares type statistic and extreme value type statistic are not based on likelihood function.
    While the likelihood ratio test (LRT), i.e., Wilks' Lambda, is not defined for $p>n-k$,
    it remains a problem how to construct likelihood-based tests in high dimensional setting.
    %Note that Roy's maximum root, one of the four classic test statistics, is derived by Roy's union intersection principle.
    In a recent work,~\cite{Zhao2016A} proposed a generalized likelihood ratio test in the context of one-sample mean vector test.
Inspired by Roy's union-intersection principle \citep{Roy1953},
    they wrote the null hypothesis as the intersection of a class of component hypotheses.
    For each component hypotheses, the likelihood ratio test is constructed.
    They use a {least favorable} argument to construct test statistic based on component tests.
    Their simulation results showed that their test has good power performance, especially when the variables are correlated.

    Following~\cite{Zhao2016A}'s methodology, we propose a generalized likelihood ratio test for hypothesis~\eqref{hypothesis}.
    To understand the power behavior of the new test, especially when variables are strongly correlated, the asymptotic distribution of the new statistic needs to be derived.
    An important correlation pattern is that the variation of variables are mainly driven by a small number of common factors.
    In this case, the covariance matrix has a few significantly large eigenvalues~\citep{Fan2008High,Cai2012Sparse,Shen2013Consistency,Ma2015A}.
    We assume there are $r$ significantly large eigenvalues and the other eigenvalues are bounded.
    We derive the asymptotic null distribution of the test statistic and give the asymptotic local power function.
    Our theoretical results implies that the asymptotic power of the new test is not affected by large eigenvalues, while most existing tests are negatively affected by large eigenvalues.
    Hence the new test is particularly powerful when there are strong correlations between variables.
    We also conduct a simulation study to examine the numerical performance of the test.

    The rest of the paper is organized  as follows.
    In Section 2, we propose a new test. Section 3 concerns the theoretical properties of the proposed test. In Section 4, the proposed test is compared with some existing tests. Section 5 complements our study with some numerical simulations. In Section 6, we give a short discussion. Finally, the proofs are gathered in the Appendix.
    %Higher criticism CX are special case of UIT.



%\setcounter{section}{2} %***
%\setcounter{equation}{0} %-1
%\noindent {\bf 2. The Second Section}

 
\section{Methodology}\label{methodology}
\subsection{Discussion about existing methods}
To facilitate the discussion, we introduce some notations.
 Let
 $$\bX=(X_{11},X_{12},\ldots,X_{1n_1},X_{21},X_{22},\ldots,X_{2n_2},\ldots,X_{k1},X_{k2},\ldots,X_{kn_k})$$
 be the pooled sample matrix.
 The sum-of-squares within groups $\bG$ can be written as $\bG=\bX(\bI_n-\bJ\bJ^T)\bX^T$ where
 $$
 \bJ=\begin{pmatrix}
     \frac{1}{\sqrt{n_1}}\mathbf{1}_{n_1}&\mathbf{0} & \mathbf{0}\\
     \mathbf{0}&\frac{1}{\sqrt{n_2}} \mathbf{1}_{n_2}& \mathbf{0}\\
     \vdots &\vdots &\vdots \\
     \mathbf{0}&\mathbf{0}&\frac{1}{\sqrt{n_k}}\mathbf{1}_{n_k}
 \end{pmatrix},
 $$
 and $\mathbf{1}_{n_i}$ is an $n_i$-dimensional vector with all elements equal to $1$.
 Let
 $$
 \tilde{\bJ}=\begin{pmatrix}
     \tilde{\bJ}_1&\mathbf{0} & \mathbf{0}\\
     \mathbf{0}&\tilde{\bJ}_2& \mathbf{0}\\
     \vdots &\vdots &\vdots \\
     \mathbf{0}&\mathbf{0}&\tilde{\bJ}_k
 \end{pmatrix},
 $$
 where $\tilde{\bJ}_i$ is a $n_i\times n_{i-1}$ matrix satisfying
 $$
\tilde{\bJ}_i=\begin{pmatrix}
    \frac{\sqrt{2}}{2}&&&0\\
    -\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}&&\\
    &-\frac{\sqrt{2}}{2}&\ddots&\\
    &&\ddots&\frac{\sqrt{2}}{2}\\
    0&&&-\frac{\sqrt{2}}{2}\\
\end{pmatrix}.
 $$
Then $\tilde{\bJ}$ is an $n\times (n-k)$ orthogonal matrix  satisfying $\tilde{\bJ}\tilde{\bJ}^T =\bI_n-\bJ\bJ^T$.
Let $\bY=\bX\tilde{\bJ}$.
Then $\bG$ has representation
$$\bG=
\bY \bY^T.
$$

On the other hand, the sum-of-squares between groups $\bH$ satisfies
$$
        \bH=\bX(\bJ\bJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)\bX^T
=\bX \bJ(\bI_k-\frac{1}{n}\bJ^T\mathbf{1}_n \mathbf{1}_n^T \bJ)\bJ^T \bX^T.
$$
We can write $\bI_k-\frac{1}{n}\bJ^T\mathbf{1}_n \mathbf{1}_n^T \bJ=\bC\bC^T$
where $\bC$ be a $k\times (k-1)$ matrix satisfying
$$
\bC=\begin{pmatrix}
    \sqrt{\frac{n_2}{n_1+n_2}}&&&0\\
    -\sqrt{\frac{n_1}{n_1+n_2}}&\sqrt{\frac{n_3}{n_2+n_3}}&&\\
    &-\sqrt{\frac{n_2}{n_2+n_3}}&\ddots&\\
    &&\ddots&\sqrt{\frac{n_k}{n_{k-1}+n_k}}\\
    0&&&-\sqrt{\frac{n_{k-1}}{n_{k-1}+n_k}}\\
\end{pmatrix}.
$$
Then $\bH$ has representation
\begin{equation*}
    \begin{aligned}
        \bH=\bX \bJ\bC \bC^T \bJ^T \bX^T.
    \end{aligned}
\end{equation*}
 Define $\Xi=(\sqrt{n_1}\xi_1,\ldots,\sqrt{n_k}\xi_k)$
 and the null hypothesis $H_0$ is equivalent to $\Xi \bC=\bO_{p\times (k-1)}$, where $\bO_{p\times (k-1)}$ is a $p\times (k-1)$ matrix with all elements equal to $0$.
 Thus the problem becomes testing hypotheses
 $$
 H_0:\Xi \bC=\bO_{p\times (k-1)}\quad \text{v.s.}\quad H_1: \Xi \bC\neq \bO_{p\times (k-1)}
 $$
 based on data matrix $\bX$ when $p$ is larger.

In low dimensional setting, the testing problem~\ref{hypothesis} is well studied.
The difficulty occurs when $p\geq n$, where the four classical test statistics can not be defined.
As the high dimensional problem is hard to deal with, a simple idea is to reduce the problem to a class of univariate problems.
Following this idea, a general strategy to propose a test statistic can be summarized as three steps.
\begin{enumerate}
    \item
       Construct a class of projected univariate data $\{\bX_{\gamma}:\gamma\in\Gamma\}$ which contains all the information of data $\bX$.
        This induces a decomposition of the null hypothesis and the alternative hypothesis:
        $$
        H_0=\bigcap_{\gamma\in\Gamma} H_{0\gamma} \quad \text{v.s.} \quad 
        H_1=\bigcup_{\gamma\in \Gamma} H_{1\gamma}.
        $$
    \item
        Construct a test statistic $T_\gamma$ for $H_{0\gamma}$ against $H_{1\gamma}$ such that $H_{0\gamma}$ is rejected if $T_{\gamma}$ is large.
    \item
        Summarize the component test statistics $\{T_{\gamma}:\gamma\in\Gamma\}$ into a global test statistic.
\end{enumerate}
It turns out that many tests in the literature can be derived by the above strategy.
While the LRT may be the best choice of univariate problems in step 2,
 there are more choices in step 1 and step 3.
In step 3, Roy's union intersection principle suggests to use $\max_{\gamma\in\Gamma}T_{\gamma}$ as global test statistic~\citep{Roy1953},
while another choice is to integrate $T_\gamma$ according some measure $\mu(\gamma)$ and use $\int_\gamma T_{\gamma}\,\mu(d\gamma)$ as global test statistic.
For step 1, we consider two different constructions of data projection.
\begin{enumerate}[i]
    \item
        Consider the class of univariate data $\{\bX_{i}=e_i^T \bX:i=1,\ldots,p\}$, where $e_i$ is the $i$th standard basis in $\mathbb{R}^p$.
        Hence $H_0=\bigcap_{i=1}^p H_{0i}$ and $H_1=\bigcup_{i=1}^p H_{1i}$, where
 $$
 H_{0i}: e_i^T \Xi \bC = \bO_{1\times (k-1)}\quad \text{and}\quad H_{1a} : e_i^T \Xi \bC \neq \bO_{1\times (k-1)}.
 $$
\item
    Consider the class of univariate data $\{\bX_{a}=a^T \bX:i=1,a\in\mathbb{R}^p, a^T a=1\}$.
        Hence $H_0=\bigcap_{a\in \mathbb{R}^p, a^T a=1}H_{0a}$ and $H_1=\bigcup_{a\in \mathbb{R}^p,a^T a=1}H_{1a}$, where
 $$
 H_{0a}: a^T \Xi \bC = \bO_{1\times (k-1)}\quad \text{and}\quad H_{1a} : a^T \Xi \bC \neq \bO_{1\times (k-1)}.
 $$
\end{enumerate}

First, we consider the construction i in step 1.
Suppose component test statistics 
$$T_i={(k-1)^{-1}} e_i^T \bH e_i-(n-k)^{-1}e_i^T \bG e_i\quad i=1,\ldots, p$$
are used in step 2, and in step 3 we integrate $T_i$ according to the uniform measure on $1,\ldots,p$. Then the resulting statistic is $p^{-1}\sum_{i=1}^p T_i$ which is equivalent to $T_{SC}$.
If instead the likelihood ratio test statistic $e_i^T \bH e_i/e_i^T \bG e_i$ is used in step 2, one obtains a scalar invariant test statistic which is a direct generalization of~\citet{Srivastava2009A}.
On the other hand, by using data $\Omega^{-1}\bX$ and component test statistics
$$
T_i^*=\sum_{1\leq j<l \leq k} \frac{n_j n_l}{n_j+n_l}\frac{(\Omega(\bar{\bX}_j-\bar{\bX}_l))^2_i}{\omega_{ii}},
$$
we have $T_{CX}=\max_{1\leq i\leq p}T_i^*$.
Here the component test statistic $T_i^*$ is similar to likelihood ratio tests.

While many existing tests can be derived by the construction i in step 1, this construction has limitation in that it relies on the choice of an orthogonal basis of $\mathbb{R}^p$.
In fact, test statistics resulting from this construction mostly requires certain prior information about the covariance matrix.
For example,~\citet{Schott2007Some} requires that $\mytr(\bSigma^{2j})/p\to \tau_j\in(0,\infty)$, $j=1,2$, and~\citet{Cai2014High} requires a consistent estimator of $\Omega$.

Next, we consider using construction ii in step 1, which does not rely on the basis of $\mathbb{R}^p$.
Suppose the likelihood ratio test statistic $T_a=a^T \bH a/a^T \bG a$ is used in step 2.
If we use the integrating strategy in step 3 and choose $\mu$ equal to the uniform distribution on the sphere, then the test statistic becomes
$$
\int_{a^T a=1} \frac{a^T \bH a}{a^T \bG a}\, \mu(da).
$$
Although it is hard to give an explicit form of the integration, it can be approximated by random projection.
More specifically, one can randomly generate unit vectors $a_1,\ldots,a_M$ and the statistics can be approximated by $M^{-1}\sum_{i=1}^M a_i^T \bH a_i/a_i^T \bG a_i$.
This statistic is well defined in high dimensional setting.
A similar method is proposed by~\cite{Lopes2015A} for $k=2$ from a different point of view.
Their analysis and simulations show that such random projection method has relative good performance especially when variables are correlated.
On the other hand, if $n-k\geq p$, Roy's union intersection principle can be used in step 3, the resulting  statistic is the  well known Roy's maximum root:
$$
\max_{a^T a=1}T_a=\lambda_{\max}(\bH \bG^{-1}).
$$
In fact, this statistic is first derived in~\cite{Roy1953} as an example of his union intersection principle.

\subsection{A new test statistic}

Roy's maximum root is constructed from the component statistics $\{T_a :a^T a=1\}$ and doesn't require prior knowledge of covariance matrix.
However, it can only be defined when $n-k\geq p$.
In fact, if $p>n-k$, $G$ is not invertible and $T_a$ is not defined for some $a$. 
We will follow~\citet{Zhao2016A}'s idea and propose a new test statistic for $p>n-k$.

Let $L_0(a)$ and $L_1(a)$ be the maximum likelihood of $\bX_a$ under $H_{01}$ and $H_{01}$, respectively.
The log likelihood ratio 
$$\log \frac{L_1(a)}{L_0(a)}=\Big(\frac{a^T(\bG+\bH) a}{a^T \bG a}\Big)^{n/2}$$
is an increase function of $T_a$.
From a likelihood point view, log likelihood ratio is an estimator of the Kullback-Leibler divergence between the true distribution and the null distribution.
Thus, the component LRT statistic characterize the discrepancy between  $H_{0a}$ and $H_{1a}$.
Thus, by maximizing $\log L_1(a)-\log L_0 (a)$, or equivalently maximizing $T_a$, one obtains
 component hypothesis $H_{0a^*}$, where $a^*=\argmax_{a^T a=1}T_a$. 
We shall call $H_{0a^*}$ the least favorable hypothesis since it is the component null hypothesis most like to be not true.




%Note that $\max_{a^T a=1}\text{LRT}_a=\text{LRT}_{a^*}$, where 
%$a^*=\argmax_{a^T a=1}\text{LRT}_a$
%is the direction achieving the maximum discrepancy between the null hypothesis and alternative hypothesis.
%We shall call $a^*$ the rejection direction.
%In the derivation of Roy's maximum root, we have
%$$
%\text{LRT}_a=\Big(1+\frac{a^T H a}{a^T G a}\Big)^{n/2}.
%$$
%In another viewpoint, union intersection principle finds an direction $a$ along which the evidence against null hypothesis is maximized.
%Such an $a$ is data dependent.
%In the classical setting, the evidence of direction $a$ is $\text{LRT}_a$.
%In the current context, there are a class of $a$ such that $\text{LRT}_a$ achieve the infinity, the largest evidence in classical sense.
%We need to further choose a single $a$ from $\{a\,|\,\text{LRT}_a=+\infty\text{ and }a^T a =1\}$.
 %From the expression of $\text{LRT}_a$, we would like to make the largest discrepancy between $a^T H a$ and $a^T G a$.
%Note that if $\text{LRT}_{\alpha}=+\infty$, then $a^T G a=0$.
%Hence it's natural to choose $a$ as
%$$
        %a^{*}=
        %\argmax_{a^T a=1, a^T G a=0} 
        %a^T H a.
%$$
%Since $a^{*T}G a^*=0$, we propose the following test statistic for $H$:


While it is hard to generalize Roy's maximum root to high dimensional setting, the least favorable hypothesis $a^*$ can be formally generalized to high dimensional setting.
In what follows,  we shall assume $p>n-k$.
%The essence of Roy's union intersection principle is to find a direction $a$ along which $\text{LRT}_a$ is maximized.
%Based on this idea, we have the following heuristic argument to propose a new test statistic.
In this case, 
Roy's maximum root is not defined since
$$\mathcal{A}\overset{def}{=}\{a:L_1(a)=+\infty, \, a^T a=1\}=\{a:a^T \bG a=0, \, a^T a=1\}$$
is not empty. 
Note that 
$$\{a:L_0(a)<+\infty, \, a^T a=1\}=\{a:a^T (\bG+\bH)a\neq 0, \, a^T a=1\}.$$
By the independence of $\bG$ and $\bH$, with probability $1$, we have $\mathcal{A}\cap \{a:L_0(a)<+\infty,\, a^T a=1\}\neq \emptyset$.
This suggests that the $a^*$ which makes the discrepancy between $L_1(a^*)$ and $L_0(a^*)$ the most should be the one which is in the set $\mathcal{A}$ and minimize $L_0(a)$.
Thus, we define $a^* = \argmin_{a\in\mathcal{A}} L_0 (a)$ and take $H_{a^*}$ as the least favorable hypothesis.
Equivalently,
$$
\begin{aligned}
    a^*&=\argmin_{a\in \mathcal{A}} L_0(a) = \argmax_{a^T a=1,a^T Ga=0} {a^T \bH a}.
\end{aligned}
$$
This motivates us to propose the new test statistic as
\begin{equation*}
    \begin{aligned}
        T(\bX)=a^{*T} \bH a^*
        =
        \max_{a^T a=1, a^T \bG a=0} 
        a^T \bH a.
    \end{aligned}
\end{equation*}
We reject the null hypothesis when $T$ is large enough.
%The above method is first proposed by~\cite{Zhao2016A} in the context of testing one sample mean vector.


Now we derive the explicit forms of the test statistic. 
%Let $J=\mathrm{diag}(n_1^{-1/2}\mathbf{1}_{n_1},\ldots,n_k^{-1/2}\mathbf{1}_{n_k})$.
%Then the matrices $I_n-JJ^T$, $JJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ and $\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ are three $n\times n$ projection matrices which are pairwise orthogonal with rank $n-k$, $k-1$ and $1$.
%Let $\tilde{J}$ be a $n\times (n-k)$ matrix satisfying $\tilde{J}\tilde{J}^T =I-JJ^T$.
%Note that $I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J$ is a $k\times k$ projection matrix with rank $k-1$.
%Let $C$ be a $k\times (k-1)$ matrix satisfying $CC^T=I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J$.
%Then 
%$$
%G=Z(I_n-JJ^T)Z^T=
%Z\tilde{J}\tilde{J}^T Z^T.
%$$
%and
%\begin{equation*}
    %\begin{aligned}
        %F=\sum_{i=1}^k n_i (\bar{\mathbf{X}}_i-\bar{\mathbf{X}})(\bar{\mathbf{X}}_i-\bar{\mathbf{X}})^T 
        %=Z(JJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)Z^T
%=ZJ(I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J)J^T Z^T
%=ZJC C^T J^T Z^T.
    %\end{aligned}
%\end{equation*}
Let $\bY=\bU_{\bY}\bD_{\bY}\bV_{\bY}^T$ be the singular value decomposition of $\bY$, where $\bU_{\bY}$ and $\bV_{\bY}$ are $p\times (n-k)$ and $(n-k)\times(n-k)$ both column orthogonal matrices, $\bD_{Y}$ is an $(n-k)\times (n-k)$ diagonal matrix.
Let $\bP_{\bY}=\bU_{\bY}\bU_{\bY}^T$ be the projection matrix on the column space of $\bY$.
Then Proposition~\ref{optProp} implies that
\begin{equation}\label{statisticForm1}
\begin{aligned}
    T(\bX)
    %&=\lambda_{\max}\big(\bX \bJ\bC\bC^T\bJ^T\bX^T (\bI_p-
    %\bP_{\bY})
    %\big)
    =\lambda_{\max}\big(\bC^T\bJ^T\bX^T (\bI_p-
    \bP_{\bY}
    )\bX\bJ\bC\big).
\end{aligned}
\end{equation}

While~\eqref{statisticForm1} is convenient for theoretical analysis, it involves a projection matrix $\bP_{\bY}$ which is not easy to compute.
Next we derive another simple form of $T(\bX)$.
%where $\bP_A=Z\tilde{J}{(\tilde{J}^T Z^T Z\tilde{J})}^{-1}\tilde{J}^T Z^T$.
By the relationship
\begin{equation*}
    \begin{aligned}
        \begin{pmatrix}
            \bJ^T \bX^T \bX\bJ & \bJ^T \bX^T \bX\tilde{\bJ}\\
            \tilde{\bJ}^T \bX^T \bX \bJ & \tilde{\bJ}^T \bX^T \bX \tilde{\bJ}
        \end{pmatrix}^{-1}
        =
        \Big(
        \begin{pmatrix}
            \bJ^T\\
            \tilde{\bJ}^T
        \end{pmatrix}
        \bX^T \bX
        \begin{pmatrix}
            \bJ &\tilde{\bJ}
        \end{pmatrix}
        \Big)^{-1}
        =
        \begin{pmatrix}
            \bJ^T {(\bX^T \bX)}^{-1}\bJ & \bJ^T {(\bX^T \bX)}^{-1}\tilde{\bJ}\\
            \tilde{\bJ}^T {(\bX^T \bX)}^{-1}\bJ & \tilde{\bJ}^T {(\bX^T \bX)}^{-1} \tilde{\bJ}
        \end{pmatrix}
    \end{aligned}
\end{equation*}
and matrix inverse formula, we have that
\begin{equation*}
    \begin{aligned}
        &{\big( \bJ^T {(\bX^T \bX)}^{-1}\bJ \big)}^{-1}
        =\bJ^T \bX^T \bX \bJ - \bJ^T \bX^T \bX\tilde{\bJ}{(\tilde{\bJ}^T \bX^T \bX \tilde{\bJ})}^{-1}
            \tilde{\bJ}^T \bX^T \bX\bJ 
        = \bJ^T \bX^T( \bI_p- \bP_{\bY}) \bX \bJ.
    \end{aligned}
\end{equation*}
Thus, 
\begin{equation}\label{statisticForm2}
    \begin{aligned}
        T(\bX)=
        \lambda_{\max}\Big(\bC^T\big( \bJ^T (\bX^T \bX)^{-1}\bJ \big)^{-1}\bC\Big).
    \end{aligned}
\end{equation}
Compared with~\eqref{statisticForm1},~\eqref{statisticForm2} doesn't involve $\bP_{\bY}$.
Hence~\eqref{statisticForm2} is convenient for computation.


\section{Main results}
To understand the statistical properties of $T(\bX)$, we derive the asymptotic distribution of $T(\bX)$.
We are specially interested in the case when variables are correlated.
For some real world problems, variables are heavily correlated with common factors, then the covariance matrix $\bSigma$ is spiked in the sense that a few eigenvalues of $\bSigma$ are significantly larger than the others~\citep{Fan2008High,Cai2012Sparse,Shen2013Consistency,Ma2015A}.
To characterize this correlation pattern, we make the following assumption for the eigenvalues of $\bSigma$.
%In this section, we investigate the asymptotic behavior of the new test statistic when $p$ is much larger than $n$.
%In high dimensional setting, it is a common phenomenon that the asymptotic distribution of a statistic relies on the covariance structure~\citep{Ma2015A}.
%We shall derive the asymptotic distribution of our statistic under two different covariance structures: non-spiked covariance and spiked covariance.




\begin{assumption}\label{assumpEigen}
    Let $\blambda_1\geq \blambda_2\geq \cdots \geq \blambda_p$ be the eigenvalues of $\bSigma$.
    Suppose the first $r$ ($r\geq 0$) eigenvalues are significantly larger than the others.
    We assume $r=o(n)$.
    We assume
 $c_1 \geq \blambda_{r+1} \geq \ldots \geq \blambda_{p} \geq c_2$ for some absolute constants $c_1$ and $c_2$.
    If $r\neq 0$, we assume 
    $$\frac{\blambda_r n}{p}\to \infty,\quad
    \frac{\blambda_1^2 pr^2}{\blambda_r^2 n^2}\to 0\quad
        \frac{\log \blambda_r}{n}\to 0.$$
\end{assumption}

\begin{remark}
The spiked covariance model is commonly assumed in the study of PCA theory.
Most existing work assumed $r$ is fixed.
    Here we allow $r$ to vary as a smaller order of $n$.
    The condition $\blambda_r n/p\to \infty$ requires $\blambda_r$ to be much larger than $p/n$. This is satisfied, for example, for the factor model adopted by~\citet{Ma2015A}.
    The most harsh condition is $\blambda_1^2 p r^2 /(\blambda_r^2 n^2)\to 0$.
    If $\blambda_1$ and $\blambda_r$ are of same order and $r$ is fixed, this condition is equivalent to $p/n^2\to 0$.
    We require this condition since the PCA consistency results are not valid when $p$ is too large. See, for example,~\citep{Cai2012Sparse}.
    If $r>0$, this condition is unavoidable and the asymptotic behavior of $T(\bX)$ is different if this condition is violated.
\end{remark}


To establish the asymptotic distribution of $T(\bX)$ under Assumption~\ref{assumpEigen},
we need following notations.
Let $\bW_{k-1}$ be a $(k-1)\times(k-1)$ symmetric random matrix whose entries above the main diagonal are i.i.d.\ $N(0,1)$ and the entries on the diagonal are i.i.d.\ $N(0,2)$.
Let $\bSigma= \bU\bLambda \bU^T$ denote the eigenvalue decomposition of $\bSigma$, where $\bLambda =\mydiag (\blambda_1,\ldots,\blambda_p)$.
We denote $\bU=(\bU_1,\bU_2)$ where $\bU_1$ is $p\times r$ and $\bU_2$ is $p\times (p-r)$.
Denote $\bLambda_1=\mydiag(\blambda_1,\ldots,\blambda_r)$ and $\bLambda_2=\mydiag(\blambda_{r+1},\ldots,\blambda_p)$.
Then $\bSigma=\bU_1\bLambda_1 \bU_1^T+\bU_2\bLambda_2 \bU_2^T$.

The following theorem establishes the asymptotic distribution of $T(\bX)$.
\begin{theorem}\label{thm1}
    Under Assumption~\ref{assumpEigen}, suppose $p/n\to \infty$ and
    $$
    \mytr\Big(\bLambda_2-\frac{1}{p-r}\mytr (\bLambda_2)\bI_{p-r}\Big)^2=o\big(\frac{p}{n}\big).
    $$
    Then under local alternative hypothesis
    \begin{equation*}
        \frac{1}{\sqrt{p}}\|\Xi \bC\|_F^2=O(1),
    \end{equation*}
    we have
    $$
    \begin{aligned}
        \frac{T(\bX)-\frac{p-r-n+k}{p-r}\mytr(\bLambda_2)}{\sqrt{\mytr (\bLambda_2^2)}}
        \sim
        \lambda_{\max}\Big(\bW_{k-1}+\tfrac{1}{\sqrt{\mytr(\bLambda_2^2)}} \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC\Big)
        +o_P(1),
    \end{aligned}
    $$
    where $\sim$ means they have the same distribution.
\end{theorem}
To gain some insight into the asymptotic behavior of $T(\bX)$, suppose null hypothesis holds and $k=2$, then Theorem~\ref{thm1} implies that
$$
        \frac{T(\bX)-\frac{p-r-n+k}{p-r}\mytr(\bLambda_2)}{\sqrt{2\mytr (\bLambda_2^2)}}\xrightarrow{\mathcal{L}}N(0,1).
$$
The asymptotic variance of $T(\bX)$ is ${2\mytr (\bLambda_2^2)}$.
If $r=0$, this equals to ${2\mytr (\bSigma^2)}$ which is the asymptotic variance of~\citet{Bai1996Efiect} and~\citet{Chen2010A}'s statistic.
While for $r>0$, ${2\mytr (\bLambda_2^2)}$ is smaller than $2\mytr(\bSigma^2)$.
In fact, if $\liminf_{n\to \infty} \blambda_1/p \in (0,+\infty]$, we have
$$
\liminf_{n\to \infty}\frac{2\mytr(\bSigma^2)}{2\mytr(\bLambda_2^2)}\in (1,+\infty].
$$
The reason for this is because the projection $\bI_p-\bP_{\bY}$ appeared in expression~\eqref{statisticForm1} can remove large variance terms of $\bX\bJ \bC$.

To formulate a test procedure with asymptotic correct level, $\mytr(\bLambda_2)$ and $\mytr(\bLambda_2^2)$ should be estimated.
Since they relies on the unknown parameter $r$, we use the following statistic to estimate $r$:
    $$
    \begin{aligned}
    \hat{r}=
            \begin{cases}
                \argmax_{1\leq i\leq n-k-1} \frac{\lambda_i(\bY^T \bY)}{\lambda_{i+1}(\bY^T \bY)}\geq \gamma_n\quad& \text{if } \max_{1\leq i\leq n-k-1}\frac{\lambda_i(\bY^T \bY)}{\lambda_{i+1}(\bY^T \bY)}\geq \gamma_n\\
                0\quad&\text{otherwise}\\
            \end{cases}
    \end{aligned}
    $$
where $\gamma_n$ is slowly tends to $+\infty$ as $n\to \infty$.
The following proposition establishes the consistency of $\hat{r}$.
\begin{proposition}\label{numberConsistency}
    Suppose $p/n\to \infty$, $r=o(n)$, $\blambda_r n/p\to \infty$ and $c_1\geq\blambda_{r+1}\geq \ldots \geq \blambda_p\geq c_2$. If $\gamma_n\to \infty$ and $\gamma_n = o(n\blambda_r/p)$, then
 $\Pr(\hat{r}=r)\to 1$.
\end{proposition}
\begin{remark}
    For the factor model adopted by~\cite{Ma2015A}, $\gamma$ is of order $p$. Then we can set $\gamma_n=\sqrt{n}$.
\end{remark}
    We use the following statistic to estimate $\mytr(\bLambda_2)$:
    $$\widehat{\mytr(\bLambda_2)}=\frac{1}{n-k}\sum_{i=\hat{r}+1}^{n-k}\lambda_i(\bY^T \bY).$$
\begin{proposition}\label{traceProp1}
    Under the assumptions of Theorem~\ref{thm1}, suppose $\gamma_n\to \infty$ and $\gamma_n=o({n\blambda_r}/{p})$, then
    $$\widehat{\mytr(\bLambda_2)}=\mytr(\bLambda_2)+o_P(\sqrt{p}).$$
\end{proposition}



%    Under non-spiked covariance, an unbiased estimator of $\mytr(\bSigma^2)$ is
%    $$
%    \widehat{\mytr(\bSigma^2)}=\frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T Y_j)^2.
%    $$
%    \begin{proposition}\label{prop:nonSpiked}
%    As $n\to \infty$,
%    $$
%    \frac{\widehat{\mytr(\bSigma^2)}}{\mytr(\bSigma^2)}\xrightarrow{P} 1.
%    $$
%    \end{proposition}
   % \begin{proof}
   %     Since $\myE (Y_i^T Y_j)^2=\mytr(\bSigma^2)$, $i\leq j$, we have
   % $$
   %     \myE\big(\widehat{\mytr(\bSigma^2)}\big)=\mytr(\bSigma^2).
   %     $$
   %     To prove the proposition, we only need to show that 
   %     $$
   %     \myVar\big(\widehat{\mytr(\bSigma^2)}\big) =o(\mytr^2(\bSigma^2)).
   %     $$
   %      Note that
   %      $$
   %     \begin{aligned}
   %         &\myE\big(\widehat{\mytr(\bSigma^2)}\big)^2 =
   %         \frac{4}{(n-k)^2(n-k-1)^2}\Big(\sum_{1\leq i < j\leq n-k}(Y_i^T Y_j)^2\Big)^2\\
   %         =&
   %         \frac{4}{(n-k)^2(n-k-1)^2}\myE\Big( \sum_{i<j}(Y_i^T Y_j)^4+
   %         \sum_{i<j,k<l:\{i,j\}\{k,l\}=\emptyset}(Y_i^T Y_j)^2 (Y_k^T Y_l)^2\\
   %         &+2\sum_{i<j<k}\big((Y_i^T Y_j)^2(Y_i Y_k)^2+(Y_i^T Y_j)^2(Y_j Y_k)^2+(Y_i^T Y_k)^2(Y_j Y_k)^2\big) \Big)\\
   %         =&
   % \frac{4}{(n-k)^2(n-k-1)^2}\Big(
   %         \frac{(n-k)(n-k-1)}{2}\big(6\mytr(\bSigma^4)+3\mytr^2(\bSigma^2)\big)\\
   %         &+\frac{(n-k)(n-k-1)(n-k-2)(n-k-3)}{4}\mytr^2(\bSigma^2)\\
   %         &+(n-k)(n-k-1)(n-k-2)\big(2\mytr(\bSigma^4)
   %         +\mytr^2(\bSigma^2)\big)
   %         \Big)\\
   %         =&
   %         \mytr^2(\bSigma^2)(1+o(1)).
   %     \end{aligned}
   %      $$
   %      Then
   %     $$
   %     \myVar\big(\widehat{\mytr(\bSigma^2)}\big) =
   %         \myE\big(\widehat{\mytr(\bSigma^2)}\big)^2 - 
   %         \big(\myE\big(\widehat{\mytr(\bSigma^2)}\big)\big)^2=
   %         o(\mytr^2(\bSigma^2)).
   %     $$
   %     This completes the proof.
   % \end{proof}
    %Under the assumptions of Theorem~\ref{nonSpiked}.


 To estimate $\mytr(\bLambda_2^2)$,
    we use the idea of leave-two-out.
    Let $\bY_{(i,j)}$ be the matrix obtained from deleting the $i$th and $j$th columns from $\bY$.
    Let $\bY_{(i,j)}=\bU_{\bY;{(i,j)}}\bD_{\bY;{(i,j)}}\bV_{\bY;{(i,j)}}^T$ be the singular value decomposition of $\bY_{(i,j)}$.
    Here $\bU_{\bY;{(i,j)}}$ is $p\times (n-k-2)$.
    Let $\tilde{\bU}_{\bY;{(i,j)}}$ be a $p\times (p-n+k+2)$ orthogonal matrix satisfying $\tilde{\bU}_{\bY;{(i,j)}}\tilde{\bU}_{\bY;{(i,j)}}^T=\bI_{p}-\bU_{\bY;{(i,j)}}\bU_{\bY;{(i,j)}}^T$.



    Let $w_{ij}$ be the $(i,j)$th element of $\bY^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bY$.
    Define
    $$
    \widehat{\mytr(\bLambda_2^2)}=\frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}w_{ij}^2.
    $$
    We use $\widehat{\mytr(\bLambda_2^2)}$ to estimate $\mytr(\bLambda_2^2)$.
    The following proposition shows that $\widehat{\mytr(\bLambda_2^2)}$ is a ratio consistent estimator.
    \begin{proposition}\label{Taichangle}
        Under the assumptions of Theorem~\ref{thm1}, we have
        $$
        \frac{\widehat{\mytr(\bLambda^2_2)}}{\mytr(\bLambda^2_2)} \xrightarrow{P} 1.
        $$

    \end{proposition}


%\section{Some results for random projection}
%The prove of our main results relies on the properties of random projection.
%There have been studies on the random projection. See xxxxx.
%
%However, sometimes the random projection is not driven by homoscedastic variables.
%In this section, we will study such cases.
%
%Let $\bZ$ be a $p\times n$ random matrix with i.i.d. $N(0,1)$ entries.
%Let $\bLambda=\mydiag(\lambda_1,\ldots,\lambda_p)$ with $c_1\geq \lambda_1,\ldots,\lambda_p\geq c_2$ and $\bU$ be an $p\times p$ orthogonal matrix.
%
%Let $\bU\bLambda^{1/2}\bZ=\bU_{\bZ}\bD_{\bZ} \bV_{\bZ}^T$ be the singular value decomposition of $\bU \bLambda^{1/2}\bZ$, where $\bU_{\bZ}$ and $\bV_{\bZ}$ are $p\times n$ and $n\times n$ orthogonal matrices and $\bD_{\bZ}$ is a $n\times n$ diagonal matrix.
%Similarly, let $\big(\frac{\mytr(\bLambda)}{p}\big)^{1/2}\bU \bZ=\bU_{\bZ}^* \bD_{\bZ}^* \bV_{\bZ}^{*T}$ be the singular value decomposition of $\big(\frac{\mytr(\bLambda)}{p}\big)^{1/2}\bU \bZ$.
%Let $\bP_{\bZ}=\bU_{\bZ}\bU_{\bZ}^T$ and $\bP_{\bZ}^*=\bU_{\bZ}^* \bU_{\bZ}^{*T}$ be the projection matrices on the column spaces of $\bU\bLambda^{1/2}\bZ$ and $\big(\frac{\mytr(\bLambda)}{p}\big)^{1/2}\bU \bZ$ respectively.
%
%
%Note that $\bU \bLambda^{1/2}\bZ \bZ^T \bLambda^{1/2}\bU^T= \bU_{\bZ}\bD_{\bZ}^2 \bU_{\bZ}^T$ and $\frac{\mytr(\bLambda)}{p}\bU \bZ \bZ^T \bU^T= \bU_{\bZ}^*\bD_{\bZ}^{*2} \bU_{\bZ}^{*T}$.
%By Lemma~\ref{sinTheta}, we have
%\begin{equation}\label{random:eq:1}
%\|\bP_{\bZ}-\bP_{\bZ}^*\|\leq 
%\frac{
%\|\bU \bLambda^{1/2}\bZ \bZ^T \bLambda^{1/2}\bU^T-\frac{\mytr(\bLambda)}{p}\bU \bZ \bZ^T \bU^T\|
%}{
    %\lambda_{n}\Big(\frac{\mytr(\bLambda)}{p}\bU\bZ \bZ^T\bU^T\Big)
%}.
%\end{equation}
%By Lemma~\ref{DSbound}, if $\sqrt{p}-\sqrt{n}\to \infty$, we have
%$$
    %\lambda_{n}\Big(\frac{\mytr(\bLambda)}{p}\bU\bZ \bZ^T\bU^T\Big)\geq
    %\frac{\mytr(\bLambda)}{p}(\sqrt{p}-\sqrt{n})^2+o_P(1).
%$$
%On the other hand,
%$$
%\begin{aligned}
%&\|\bU \bLambda^{1/2}\bZ \bZ^T \bLambda^{1/2}\bU^T-\frac{\mytr(\bLambda)}{p}\bU \bZ \bZ^T \bU^T\|^2\\
%\leq&
%\|\bU \bLambda^{1/2}\bZ \bZ^T \bLambda^{1/2}\bU^T-\frac{\mytr(\bLambda)}{p}\bU \bZ \bZ^T \bU^T\|^2_F\\
    %=&
    %\mytr(\bZ^T \bLambda \bZ)^2
    %+
    %\frac{\big(\mytr(\bLambda)\big)^2}{p^2}\mytr(\bZ^T \bZ)^2
    %-2\frac{\mytr(\bLambda)}{p}\mytr(\bZ^T \bLambda^{1/2}\bZ)^2.
%\end{aligned}
%$$
%By taking expectation, we have
%$$
%\begin{aligned}
%&\myE\|\bU \bLambda^{1/2}\bZ \bZ^T \bLambda^{1/2}\bU^T-\frac{\mytr(\bLambda)}{p}\bU \bZ \bZ^T \bU^T\|^2\\
    %\leq&
    %%n(\mytr \bLambda)^2+n(n+1)\mytr(\bLambda^2)+\frac{\big(\mytr(\bLambda)\big)^2}{p^2}(np^2+n(n+1)p)\\
    %%&-2\frac{\mytr(\bLambda)}{p}(n(\mytr\bLambda^{1/2})^2+n(n+1)\mytr(\bLambda))\\
    %%=&
    %2n\mytr(\bLambda)\big(\mytr(\bLambda)-\frac{1}{p}(\mytr \bLambda^{1/2})^2\big)
    %+n(n+1)(\mytr(\bLambda)^2-\frac{1}{p}(\mytr\bLambda)^2).
%\end{aligned}
%$$
%
%
%
%
%\begin{lemma}[$\sin\theta$ theorem for symmetric matrices~\citep{Davis1970The}]\label{sinTheta}
    %Let $\bA$ and $\bA +\bE$ be symmetric matrices satisfying
%$$
    %\bA=\begin{pmatrix}
        %\bF_0&\bF_1
    %\end{pmatrix}
    %\begin{pmatrix}
        %\bA_0& 0\\
        %0&\bA_1
    %\end{pmatrix}
    %\begin{pmatrix}
        %\bF_0^T\\
        %\bF_1^T
    %\end{pmatrix},
    %\quad
    %\bA+\bE=\begin{pmatrix}
        %\bF_0^*&\bF_1^*
    %\end{pmatrix}
    %\begin{pmatrix}
        %\bA_0^*& 0\\
        %0&\bA_1^*
    %\end{pmatrix}
    %\begin{pmatrix}
        %\bF_0^{*T}\\
        %\bF_1^{*T}
    %\end{pmatrix},
%$$
    %where $(\bF_0, \bF_1)$ and $(\bF_0^*, \bF_1^*)$ are orthogonal matrices.
    %If the eigenvalues of $\bA_0$ are contained in an interval $(a,b)$, and the eigenvalues of $\bA_1^*$ are excluded from the interval $(a-\delta,b+\delta)$ for some $\delta>0$, then
    %$$
    %\|\bF_0 \bF_0^T -\bF_0^*\bF_0^{*T}\| \leq
    %\frac{\min(\|\bF_1^T\bE \bF_0^*\|,\|\bF_0^T\bE \bF_1^*\|)}{\delta}.
    %$$
%
%\end{lemma}
Now we can construct a test procedure with asymptotic correct level $\alpha$. Let 
$$
Q=\frac{T(\bX)-\frac{p-r-n+k}{p-r}\widehat{\mytr(\bLambda_2)}}{\sqrt{\widehat{\mytr(\bLambda_2^2)}}}.
$$
Let $F(x)$ be the cumulative distribution function of $\lambda_{\max}(\bW_{k-1})$.
We reject the null hypothesis if $Q> F^{-1}(1-\alpha)$.

Theorem~\ref{thm1}, Proposition~\ref{traceProp1} and Proposition~\ref{Taichangle} implies that the resulting test procedure has asymptotic correct level under the assumptions of Theorem~\ref{thm1}.
And by Theorem~\ref{thm1}, the asymptotic local power function of the proposed test procedure is
$$
\Pr\Big(
        \lambda_{\max}\big(\bW_{k-1}+\tfrac{1}{\sqrt{\mytr(\bLambda_2^2)}} \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC\big)\geq F_{\bW}^{-1}(1-\alpha)
\Big).
$$
If $k=2$, the asymptotic local power function of~\cite{Bai1996Efiect} and~\cite{Chen2010A}'s method can be written as
$$
\Pr\Big(
        \lambda_{\max}\big(\bW_{1}+\tfrac{1}{\sqrt{\mytr(\bSigma^2)}} \bC^T \Xi^T \Xi \bC\big)\geq F_{\bW}^{-1}(1-\alpha)
\Big).
$$
Hence the asymptotic relative efficiency between our method and~\cite{Bai1996Efiect} and~\cite{Chen2010A}'s method is
$$
 \sqrt{\frac{{{\mytr(\bSigma^2)}}}{{\mytr(\bLambda_2^2)}}}
\frac{ \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC}{ {\bC^T \Xi^T \Xi \bC}}.
$$
There's a random term $\bP_{\bY}$ in the expression.
To gain some insight into this,
suppose, for example, $\sqrt{n_i} \xi_i$ is from prior distribution $N_p(0,\psi \bI_p)$, $i=1,\ldots,k$. Then
$
\psi^{-1}\bC^T \Xi^T \Xi \bC$
is distributed as $ \text{Wishart}_{k-1}(p,\bI_{k-1})$, the $k-1$ dimensional Wishart distribution with degree of freedom $p$ and parameter $\bI_{k-1}$.
On the other hand,
$\bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC$  is distributed as $\text{Wishart}_{k-1}(p-n+k,\bI_{k-1})$.
In this case, we have
$$
\frac{\bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC}{\bC^T \Xi^T \Xi \bC}\xrightarrow{P}1.
$$
Thus, when
$$
\liminf_{n\to \infty}\frac{{{\mytr(\bSigma^2)}}}{{\mytr(\bLambda_2^2)}}\in(1,+\infty],
$$
the new test tends to be more powerful than~\cite{Chen2010A}'s test.



%\section{Schott's method}

%$$
%E=ZZ^T-\sum_{i=1}^k n_i \bar{X}_i \bar{X}_i^T.
%$$

%$$
%H=\sum_{i=1}^{k} n_i \bar{X}_i \bar{X}_i^T - n\bar{X}\bar{X}^T.
%$$

%$$
%\mytr E = \mytr Z^T Z - \mytr J^T Z^T Z J.
%$$


%$$
%\mytr H = \mytr J^T Z^T Z J - \frac{1}{n} 1_n^T Z^T Z 1_n
%$$

%$$
%T_{SC}=\frac{1}{\sqrt{n-1}}(
%\frac{1}{k-1}\mytr H-\frac{1}{n-k} \mytr E
%)
%$$
%
%Our new test statistic comes from construction ii in step 1, the likelihood ratio test statistics in step 2 and strategy II in step 3.
%Theorems~\ref{nonSpiked} and~\ref{thm1} allow us to analyze the properties of the proposed test.
%Suppose $\sqrt{n_i}\mu_i$ is from prior distribution $N_p(0,\psi \bI_p)$, $i=1,\ldots, k$.
%Then $\psi^{-1}\bC^T \Xi^T \Xi \bC$ is distributed as $\text{Wishart}_{k-1}(p,\bI_{k-1})$ (Wishart distribution with freedom $p$ and parameter $\bI_{k-1}$) and $\psi^{-1}\bC^T \Xi^T \bP_{\bY}\Xi \bC$ is distributed as $\text{Wishart}_{k-1}(n-k,\bI_{k-1})$.
%In this case, we have
%$$
%\psi^{-1}\bC^T \Xi^T (\bI_P-\bP_{\bY})\Xi \bC=
%(1+o_P(1))\psi^{-1}\bC^T \Xi^T \Xi \bC.
%$$
%If the conditions of Theorem~\ref{nonSpiked} hold and $k=2$, the asymptotic power of the proposed test is the same as that of~\cite{Bai1996Efiect}and~\cite{Chen2010A}'s method.
%Since the method of~\cite{Schott2007Some} is a direct generalization of~\cite{Bai1996Efiect}'s method, it can be shown the asymptotic power of the proposed test is the same as that of~\cite{Schott2007Some} for general $k$.
%Next, suppose the covariance matrix is spiked and the conditions of Theorem~\ref{thm1} hold.
%Theorem~\ref{thm1} implies that the proposed test does not depend on large eigenvalues $\lambda_1,\ldots,\lambda_r$ while other existing test procedures are negatively affected by large eigenvalues $\lambda_1,\ldots,\lambda_r$.   
%Thus, the new test has particular good power behavior when $\lambda_1,\ldots,\lambda_r$ are large.
% This property is not surprising since our statistic is from construction ii.
%As a result, our statistic has a wider applicable range compared to the tests from construction i.


%%%%%%%%%%%%%%%%% Hotelling-Lawley trace %%%%%%%%%%%%%%%%%%%%%%%%%
%Another classical test statistic, Hotelling-Lawley trace,  can also be derived by Roy's union intersection principle. This is shown by~\cite{Mudholkar1974}.
 %In that paper, they consider the transformed data $\{M^T \bX: M \textrm{ is } \text{$(k-1)\times p$ matrix}\}$ and the decomposition of hypotheses:
 %$$H_{0}=\bigcap_{M}H_{0M} \quad\text{and}\quad H_1=\bigcup_{M}H_{1M} ,$$
 %where
 %$$
 %H_{0M}: \mytr( M \Xi C) = 0\quad \text{and}\quad H_{1M} : \mytr( M \Xi C )> 0.
 %$$
%
%Note that $\myE Z=\Xi J^T$, hence
%the uniformly minimum variance unbiased estimator of $\mytr(M\Xi C)$ is $\mytr(MZJ C)$.
%It can be seen that
%$
%\mytr \big(MZJ C\big)
%\sim
%N\big(\mytr(M\Xi C),\mytr(M\bSigma M^T )\big)
%$.

%=====================
%$$
%\mytr \big(MZJC\big)
%=
%\mytr \big(CMZJ\big)
%$$
%$ZJ=(\sqrt{n_1}\bar{\bX}_1,\ldots,\sqrt{n_k}\bar{\bX}_k)$.
%Note that $CM\sqrt{n_i}\bar{\bX}_i\sim N_{k-1}(\sqrt{n_i}CM\xi_i,CM\bSigma M^T C^T)$.
%Hence we have that
%$$
%\mytr \big(CMZJ\big)
%\sim
%N(\mytr(CM\Xi),\mytr(CM\bSigma M^T C^T))
%\sim
%N(\mytr(M\Xi C),\mytr(M\bSigma M^T )).
%$$
%==================

%Hence it's natural to use one side $t$ type statistic
%$$
%T_M = \frac{
%\mytr \big(MZJC\big)
%}{
    %\sqrt{\mytr(M G M^T)}
%}
%$$
%to test $H_M$ against $K_M$.



%By Cauchy inequality $\max_B \mytr(AB^T)/\mytr^{1/2}(BB^T)=\mytr^{1/2}(AA^T)$, we have
%$$
%\begin{aligned}
    %\max_M T_M &=\max_M \frac{\mytr \big(MG^{1/2}G^{-1/2}ZJC\big)
%}{\sqrt{\mytr(M G^{1/2} (M G^{1/2})^T)}
%}
    %=\mytr^{1/2}((ZJC)^T G^{-1}ZJC)\\
    %&=\mytr^{1/2}( ZJC(ZJC)^T G^{-1})
    %=\mytr^{1/2}(H G^{-1}).
%\end{aligned}
%$$

%====================
\section{Numerical study}

\subsection{Permutation method}
 Most existing test can not be used under spiked covariance model, since spiked covariance model violated their assumption.
 To compare the new test with other tests, we use permutation method to determine the critical value in our simulation.
 
Permutation method is a powerful tool to determine the critical value of a test statistic.
   The test procedure resulting from permutation method is exact as long as the null distribution of observations are exchangeable~\citep{Romano1990On}.
   The major down-side to permutation method is that it can be computationally intensive.
   Fortunately, for the test statistic proposed in this paper,  the permutation method can be computationally fast.
    By expression~\eqref{statisticForm2}, a permuted statistic can be written as
    \begin{equation}\label{permutedStatistic}
        T(\bX\Gamma)=\lambda_{\max}\Big(\bC^T{\big( \bJ^T \Gamma^T {(\bX^T \bX)}^{-1} \Gamma \bJ \big)}^{-1}  \bC\Big),
    \end{equation}
where $\Gamma$ is an $n\times n$ permutation matrix.
   Note that ${(\bX^T \bX)}^{-1}$, the most time-consuming component, can be calculated aforehand.
   The permutation procedure for our statistic can be summarized as:
   \begin{enumerate}
       \item
           Calculate $T(\bX)$ according to~\eqref{statisticForm2}, hold intermediate result ${(\bX^T \bX)}^{-1}$.
       \item For a large $M$, independently generate $M$ random permutation matrix $\Gamma_1,\ldots,\Gamma_M$ and calculate $T(\bX\Gamma_1),\ldots,T(\bX\Gamma_M)$ according to~\eqref{permutedStatistic}. 
       \item Calculate the $p$-value by
           $
           \tilde{p}={(M+1)}^{-1}\big[1+\sum_{i=1}^M I\{T(\bX\Gamma_i)\geq T(\bX)\}\big]
           $.
           Reject the null hypothesis if $\tilde{p}\leq \alpha$.
   \end{enumerate}

Here $M$ is the permutation times. 
   %It can be shown that for any integer $M>0$, the resulting test controls the Type I error. More precisely, we have $\Pr(\tilde{p}\leq u)\leq u$ for all $0\leq u\leq 1$.
   %Moreover, as $M$ tends to $\infty$, $\lim_{M\to \infty}\Pr(\tilde{p}\leq u)= u$.
   %See, for example,~\cite{Lehmann}, Chapter $15$.
   It can be seen that  step 1 and step 2 cost $O(n^2 p +n^3)$ and $O(n^2 M)$ operations respectively.
   In large sample or high dimensional setting, step 2 has negligible effect on total computational complexity.




   
   
   



   \subsection{Simulation results}
In this section, we evaluate the numerical performance of the new test.
For comparison, we also carry out simulations for the test of~\citet{Cai2014High} and the test of~\citet{Schott2007Some}.
These tests are denoted respectively by NEW, CX and SC.
Since the critical value of $CX$ and $SC$ may not be valid under spiked covariance model, we use permutation method to determine the critical value for all three test.
The empirical power is computed based on $1000$ simulations.

In the simulations, we set $k=3$.
Note that the new test is invariant under orthogonal transformation.
Without loss of generality, we only consider diagonal $\bSigma$.
We consider two different structure of $\bSigma$.
\begin{itemize}
    \item
        Covariance structure I: $\bSigma=\mydiag(p,1,\ldots,1)$.
    \item
        Covariance structure II: $\bSigma=\mydiag(\rho_1,\ldots,\rho_p)$, where $\rho_1\geq \cdots\geq \rho_p$ are order statistics of $p$ i.i.d.\ random variables which have uniform distribution between $0$ and $1$.
\end{itemize}

Define signal-to-noise ratio (SNR) as
$$
\textrm{SNR}=\frac{\|\xi_f\|_F^2}{\sqrt{\sum_{i=2}^{p}\lambda_i(\bSigma)^2}}.
$$
We use SNR to characterize the signal strength.
We consider two structure of alternative hypotheses: the non-sparse alternative and the sparse alternative.
In the non-sparse case, we set $\xi_1=\kappa 1_p$, $\xi_2=-\kappa 1_p$ and $\xi_3=\mathbf{0}_p$, where $\kappa$ is selected to make the SNR equal to the given value.
In the sparse case, we set $\xi_1=\kappa (1_{p/5}^T,\mathbf{0}_{4p/5}^T)^T$, $\xi_2=\kappa (\mathbf{0}_{p/5}^T, 1_{p/5}^T,\mathbf{0}_{3p/5}^T)^T$ and $\xi_3=\mathbf{0}_p$. Again, $\kappa$ is selected to make the SNR equal to the given value.

The simulation results are summarized in Tables~\ref{table1}-\ref{table6}. It can be seen from the results that under spiked covariance, the proposed test outperforms the other two tests for both non-sparse and sparse alternatives.
Under non-spiked covariance, the power of the new test is a little lower than that of SC.\ 
As $p$ increase, the power of the new test approaches to that of SC.

%$$
%SNR=\frac{\|\xi_f\|_F^2}{\sqrt{\mytr (\bSigma^2)}}
%$$


\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and non-sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=10$. }
    \label{table1}
    \centering
    \begin{tabular}{*{10}{c}}
    \toprule
    \multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=50$}&\multicolumn{3}{c}{$p=75$}&\multicolumn{3}{c}{$p=100$} \\
        \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
    \midrule
0 & 0.035 & 0.048 & 0.052 & 0.057 & 0.052 & 0.057 & 0.053 & 0.048 & 0.045 \\ 
1 & 0.060 & 0.049 & 0.096 & 0.081 & 0.050 & 0.092 & 0.063 & 0.062 & 0.085 \\ 
2 & 0.100 & 0.058 & 0.140 & 0.073 & 0.045 & 0.169 & 0.086 & 0.055 & 0.171 \\ 
3 & 0.145 & 0.066 & 0.234 & 0.119 & 0.070 & 0.266 & 0.117 & 0.056 & 0.307 \\ 
4 & 0.126 & 0.064 & 0.317 & 0.121 & 0.059 & 0.380 & 0.122 & 0.061 & 0.402 \\ 
5 & 0.179 & 0.072 & 0.392 & 0.178 & 0.068 & 0.541 & 0.141 & 0.071 & 0.579 \\ 
6 & 0.198 & 0.070 & 0.513 & 0.189 & 0.071 & 0.639 & 0.143 & 0.066 & 0.717 \\ 
7 & 0.249 & 0.085 & 0.629 & 0.227 & 0.084 & 0.777 & 0.206 & 0.073 & 0.822 \\ 
8 & 0.268 & 0.092 & 0.685 & 0.252 & 0.084 & 0.822 & 0.217 & 0.078 & 0.894 \\ 
9 & 0.324 & 0.100 & 0.786 & 0.256 & 0.090 & 0.911 & 0.246 & 0.074 & 0.949 \\ 
10 & 0.342 & 0.115 & 0.828 & 0.303 & 0.097 & 0.937 & 0.270 & 0.075 & 0.973 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and non-sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table2}
\centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.050 & 0.043 & 0.050 & 0.056 & 0.066 & 0.048 & 0.062 & 0.045 & 0.054 \\ 
1 & 0.069 & 0.048 & 0.063 & 0.046 & 0.052 & 0.091 & 0.068 & 0.048 & 0.095 \\ 
2 & 0.097 & 0.046 & 0.131 & 0.086 & 0.053 & 0.164 & 0.068 & 0.057 & 0.173 \\ 
3 & 0.113 & 0.061 & 0.200 & 0.117 & 0.057 & 0.270 & 0.101 & 0.045 & 0.313 \\ 
4 & 0.135 & 0.053 & 0.247 & 0.130 & 0.054 & 0.402 & 0.118 & 0.066 & 0.485 \\ 
5 & 0.158 & 0.065 & 0.357 & 0.134 & 0.066 & 0.526 & 0.134 & 0.073 & 0.616 \\ 
6 & 0.198 & 0.081 & 0.433 & 0.161 & 0.052 & 0.668 & 0.138 & 0.067 & 0.765 \\ 
7 & 0.217 & 0.068 & 0.514 & 0.191 & 0.067 & 0.759 & 0.174 & 0.068 & 0.862 \\ 
8 & 0.229 & 0.063 & 0.582 & 0.223 & 0.075 & 0.853 & 0.187 & 0.060 & 0.927 \\ 
9 & 0.264 & 0.094 & 0.680 & 0.218 & 0.080 & 0.918 & 0.227 & 0.067 & 0.966 \\ 
10 & 0.298 & 0.091 & 0.758 & 0.245 & 0.076 & 0.934 & 0.228 & 0.052 & 0.982 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=10$. }
    \label{table3}
\centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=50$}&\multicolumn{3}{c}{$p=75$}&\multicolumn{3}{c}{$p=100$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.063 & 0.056 & 0.052 & 0.048 & 0.049 & 0.048 & 0.057 & 0.047 & 0.042 \\ 
1 & 0.087 & 0.058 & 0.071 & 0.069 & 0.044 & 0.096 & 0.076 & 0.051 & 0.080 \\ 
2 & 0.091 & 0.066 & 0.116 & 0.113 & 0.037 & 0.133 & 0.080 & 0.058 & 0.139 \\ 
3 & 0.155 & 0.065 & 0.177 & 0.131 & 0.062 & 0.228 & 0.113 & 0.058 & 0.218 \\ 
4 & 0.184 & 0.065 & 0.246 & 0.174 & 0.076 & 0.308 & 0.144 & 0.061 & 0.310 \\ 
5 & 0.225 & 0.081 & 0.337 & 0.214 & 0.075 & 0.386 & 0.176 & 0.083 & 0.417 \\ 
6 & 0.270 & 0.088 & 0.425 & 0.266 & 0.085 & 0.507 & 0.228 & 0.071 & 0.508 \\ 
7 & 0.364 & 0.080 & 0.501 & 0.307 & 0.078 & 0.571 & 0.302 & 0.087 & 0.629 \\ 
8 & 0.405 & 0.105 & 0.549 & 0.381 & 0.080 & 0.698 & 0.362 & 0.089 & 0.721 \\ 
9 & 0.470 & 0.121 & 0.634 & 0.408 & 0.078 & 0.774 & 0.391 & 0.070 & 0.797 \\ 
10 & 0.547 & 0.128 & 0.702 & 0.484 & 0.109 & 0.819 & 0.415 & 0.088 & 0.877 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table4}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.048 & 0.045 & 0.046 & 0.053 & 0.046 & 0.043 & 0.051 & 0.034 & 0.046 \\ 
1 & 0.079 & 0.055 & 0.082 & 0.066 & 0.063 & 0.079 & 0.063 & 0.059 & 0.100 \\ 
2 & 0.097 & 0.054 & 0.119 & 0.088 & 0.055 & 0.138 & 0.085 & 0.055 & 0.160 \\ 
3 & 0.133 & 0.069 & 0.167 & 0.113 & 0.066 & 0.223 & 0.114 & 0.054 & 0.235 \\ 
4 & 0.149 & 0.062 & 0.212 & 0.126 & 0.084 & 0.298 & 0.132 & 0.057 & 0.344 \\ 
5 & 0.204 & 0.060 & 0.281 & 0.169 & 0.066 & 0.427 & 0.154 & 0.057 & 0.469 \\ 
6 & 0.252 & 0.060 & 0.352 & 0.227 & 0.070 & 0.548 & 0.195 & 0.072 & 0.641 \\ 
7 & 0.310 & 0.072 & 0.429 & 0.252 & 0.059 & 0.614 & 0.220 & 0.061 & 0.711 \\ 
8 & 0.372 & 0.088 & 0.529 & 0.314 & 0.085 & 0.719 & 0.297 & 0.060 & 0.800 \\ 
9 & 0.427 & 0.083 & 0.547 & 0.362 & 0.085 & 0.794 & 0.300 & 0.057 & 0.881 \\ 
10 & 0.449 & 0.093 & 0.619 & 0.396 & 0.072 & 0.853 & 0.340 & 0.076 & 0.911 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure II and non-sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table5}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.063 & 0.054 & 0.058 & 0.052 & 0.040 & 0.042 & 0.045 & 0.049 & 0.070 \\ 
1 & 0.141 & 0.120 & 0.115 & 0.126 & 0.120 & 0.112 & 0.103 & 0.110 & 0.102 \\ 
2 & 0.181 & 0.209 & 0.169 & 0.330 & 0.260 & 0.210 & 0.200 & 0.227 & 0.201 \\ 
3 & 0.692 & 0.367 & 0.244 & 0.759 & 0.385 & 0.341 & 0.468 & 0.413 & 0.394 \\ 
4 & 0.753 & 0.539 & 0.420 & 0.744 & 0.573 & 0.515 & 0.516 & 0.554 & 0.561 \\ 
5 & 0.828 & 0.690 & 0.509 & 0.871 & 0.697 & 0.693 & 0.556 & 0.724 & 0.727 \\ 
6 & 0.809 & 0.812 & 0.622 & 0.822 & 0.824 & 0.766 & 0.959 & 0.838 & 0.859 \\ 
7 & 1.000 & 0.882 & 0.780 & 0.979 & 0.916 & 0.903 & 0.990 & 0.923 & 0.947 \\ 
8 & 0.993 & 0.955 & 0.789 & 1.000 & 0.965 & 0.954 & 0.999 & 0.972 & 0.971 \\ 
9 & 1.000 & 0.979 & 0.911 & 0.999 & 0.981 & 0.979 & 0.964 & 0.986 & 0.987 \\ 
10 & 1.000 & 0.991 & 0.877 & 0.989 & 0.996 & 0.988 & 0.996 & 0.996 & 0.997 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure II and sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table6}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.052 & 0.055 & 0.047 & 0.055 & 0.057 & 0.053 & 0.044 & 0.055 & 0.057 \\ 
1 & 0.068 & 0.124 & 0.065 & 0.070 & 0.130 & 0.085 & 0.049 & 0.116 & 0.087 \\ 
2 & 0.085 & 0.233 & 0.112 & 0.076 & 0.239 & 0.149 & 0.067 & 0.241 & 0.161 \\ 
3 & 0.110 & 0.388 & 0.161 & 0.090 & 0.408 & 0.215 & 0.097 & 0.417 & 0.227 \\ 
4 & 0.120 & 0.530 & 0.184 & 0.112 & 0.552 & 0.282 & 0.103 & 0.556 & 0.309 \\ 
5 & 0.167 & 0.708 & 0.238 & 0.142 & 0.699 & 0.387 & 0.140 & 0.687 & 0.394 \\ 
6 & 0.196 & 0.807 & 0.261 & 0.168 & 0.820 & 0.472 & 0.162 & 0.823 & 0.547 \\ 
7 & 0.217 & 0.875 & 0.318 & 0.177 & 0.892 & 0.505 & 0.173 & 0.896 & 0.646 \\ 
8 & 0.234 & 0.935 & 0.378 & 0.220 & 0.951 & 0.625 & 0.195 & 0.948 & 0.749 \\ 
9 & 0.312 & 0.965 & 0.407 & 0.222 & 0.970 & 0.672 & 0.224 & 0.979 & 0.809 \\ 
10 & 0.334 & 0.976 & 0.505 & 0.292 & 0.987 & 0.773 & 0.254 & 0.989 & 0.881 \\ 
\bottomrule
\end{tabular}
\end{table}

\section{Concluding remarks}
In this paper, motivated by Roy's union intersection principle, we proposed a generalized likelihood ratio statistic for MANOVA in high dimensional setting.
We derive the asymptotic distribution of the new test statistic. We also gives the asymptotic local power function.
Our theoretic work and simulation study shows that when the covariance matrix is spiked, the  proposed test 
tends to be more powerful than existing tests.

%In Section~\label{sc:compare}, we consider a random projection test.
%The theoretical properties of random projection test may be a.





\section*{Appendix}

\begin{proposition}\label{optProp}
    Suppose $\bA$ is a $p\times r$ matrix with rank $r$ and $\bB$ is a $p\times p$  non-zero semi-definite matrix.
    Denote by $\bA=\bU_\bA \bD_\bA \bV_\bA^T$ the singular value decomposition of $\bA$, where $\bU_\bA$ and $\bV_\bA$ are $p\times r$ and $r\times r$ column orthogonal matrix, $\bD_\bA$ is a $r\times r$ diagonal matrix.
    Let $\bP_\bA=\bU_\bA \bU_\bA^T$ be the projection on the column space of $\bA$.
    Then
    \begin{equation}
        \max_{a^T a=1, a^T \bA \bA^T a=0}a^T \bB a=
        \lambda_{\max}\big(\bB(\bI_p-\bP_\bA)\big).
    \end{equation}
\end{proposition}
\begin{proof}
    Note that $a^T \bA \bA^T a=0$ is equivalent to $\bP_\bA a=0$ which in turn is equivalent to $a= (\bI_p-\bP_\bA)a$.
    Then
    \begin{equation}\label{eq:prop1eq1}
        \begin{aligned}
        \max_{a^T a=1, a^T \bA \bA^T a=0}a^T \bB a
            &=
        \max_{a^T a=1, \bP_\bA a=0}a^T(\bI_p-\bP_\bA) \bB (\bI_p-\bP_\bA)a,
        \end{aligned}
    \end{equation}
    which is obviously no greater than $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)$.
    To prove that they are equal,  without loss of generality, we can assume $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)>0$.
    Let $\alpha_1$ be one eigenvector corresponding to the largest eigenvalue of $(\bI-\bP_\bA)\bB(\bI-\bP_\bA)$.
    Since $(\bI-\bP_\bA)\bB(\bI-\bP_\bA)\bP_\bA=(\bI-\bP_\bA)\bB(\bP_\bA-\bP_\bA)=\bO_{p\times p}$ and $\bP_\bA$ is symmetric, the rows of $\bP_\bA$ are eigenvetors of $(\bI-\bP_\bA)\bB(\bI-\bP_\bA)$ corresponding to eigenvalue $0$.
    It follows that $\bP_\bA\alpha_1=0$.
    Therefore, $\alpha_1$ satisfies the constraint of~\eqref{eq:prop1eq1} and~\eqref{eq:prop1eq1} is no less than $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)$.
    The conclusion now follows by noting that $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)=\lambda_{\max}\big( \bB(\bI-\bP_\bA)\big)$.
    
\end{proof}
\begin{lemma}[\citet{DAVIDSON2001317} Theorem II.7]\label{DSbound}
    Let $\bA$ be $m\times n$ with iid $N(0,1)$ entries.
    If $m>n$, then for any $t>0$,
    \begin{align*}
        \Pr(\sqrt{\lambda_1(\bA \bA^T)}>\sqrt{m}+\sqrt{n}+t)\leq \exp(-t^2/2),\\
        \Pr(\sqrt{\lambda_n(\bA \bA^T)}<\sqrt{m}-\sqrt{n}-t)\leq \exp(-t^2/2).
    \end{align*}
\end{lemma}




\paragraph{Proves of the main results}

It can be seen that $\bX\bJ\bC$ is independent of $\bY$.
Since
$
\myE \bY = \bO_{p\times (n-k)}
$,
we can write
$
\bY = \bU\bLambda^{1/2} \bG_1
$,
where $\bG_1$ is a $p\times (n-k)$ matrix with i.i.d.\ $N(0,1)$ entries.
We write
$
\bX\bJ\bC = \xi_f + \bU\bLambda^{1/2} \bG_2
$, 
where $\bG_2$ is a $p\times (k-1)$ matrix with i.i.d. $N(0,1)$ entries.

Then 
\begin{equation}\label{eq:maindec}
\begin{aligned}
\bC^T\bJ^T \bX^T(\bI_p-\bP_{\bY}) \bX\bJ\bC
=&
\bG_2^T \bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}\bG_2+
\xi_f^T (\bI_p -\bP_{\bY})\xi_f+\\
&\xi_f^T (\bI_p -\bP_{\bY})\bU\bLambda^{1/2}\bG_2+
\bG_2^T \bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\xi_f.
\end{aligned}
\end{equation}
    The first term of~\eqref{eq:maindec} can be represented as
\begin{equation}\label{eq:firstTerm}
\bG_2^T \bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}\bG_2=
\sum_{i=1}^p \lambda_i (\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2})\xi_i \xi_i^T,
\end{equation}
where $\xi_i\overset{i.i.d.}{\sim} N(0,\bI_{k-1})$.




Let $\bG_1=(\bG_{1A}^T,\bG_{1B}^T)^T$, where $\bG_{1A}$ is the first $r$ rows of $\bG_1$ and $\bG_{1B}$ is the last $p-r$ rows of $\bG_1$.
The following lemma gives the asymptotic property of $\lambda_{i}(\bY^T \bY)$, $i=1,\ldots, r$.
\begin{lemma}\label{PCAlemma1}
    Suppose $p/n\to \infty$, $r=o(n)$, $\blambda_r n/p\to \infty$ and $c_1\geq\blambda_{r+1}\geq \ldots \geq \blambda_p\geq c_2$. 
    Then
    \begin{align}
        &\sup_{1\leq i\leq r}\Big|\frac{\lambda_{i}(\bY^T \bY)}{n\blambda_i}-1\Big|\to 0,\label{PCAlemma1:eq1}\\
        &\limsup_{n\to +\infty}\frac{\lambda_{r+1}(\bY^T\bY)}{p}\leq c_1,\label{PCAlemma1:eq2}\\
        &\liminf_{n\to +\infty}\frac{\lambda_{n-k}(\bY^T\bY)}{p}\geq c_2,\label{PCAlemma1:eq3}
    \quad
    \end{align}
    almost surely.
\end{lemma}
\begin{proof}
    %For a matrix $\bA$, we denote by  $\bA_{[a:b,c:d]}$ the $a$-to-$b$-th row, $c$-to-$d$-th column of matrix $\bA$, by $\bA_{[a:b,:]}$ and $\bA_{[:,c:d]}$ the $a$-to-$b$-th full rows and $c$-to-$d$-th full columns of $\bA$ respectively.
%$
%\bY^T \bY=
%\bG_1^T \bLambda \bG_1   = \bV_{\bY} \bD_{\bY}^2 \bV_{\bY}^T
%$, and 
%$
%\bG_1^T \bLambda \bG_1=
%\bG_{1A}^T \bLambda_1 \bG_{1A}+
%\bG_{1B}^T \bLambda_2 \bG_{1B}
%$. We have
    Note that
$$
\bY^T \bY =\bG_1^T \bLambda \bG_1
=
\bG_{1A}^T \bLambda_1 \bG_{1A}+
\bG_{1B}^T \bLambda_2 \bG_{1B}.
$$
For $1\leq i \leq r$, we have
    \begin{equation}\label{eq:DLU}
\lambda_i(\bG_{1A}^T \bLambda_1 \bG_{1A})
    \leq \lambda_i(\bY^T \bY) \leq \lambda_i(\bG_{1A}^T \bLambda_1 \bG_{1A})+
    c_1 \lambda_1(\bG_{1B}^T  \bG_{1B}).
    \end{equation}
 Using Weyl's inequality, we can derive a lower bound for $\lambda_i(\bG_{1A}^T \bLambda_1 \bG_{1A})$, $ i=1,\ldots, r$.
\begin{equation}\label{eq:DLower}
\begin{aligned}
&\lambda_i(\bG_{1A}^T \bLambda_1 \bG_{1A})
\geq
\lambda_i(\bG_{1A}^T \mydiag(\blambda_i \bI_{i},\bO_{(r-i)\times(r-i)}) \bG_{1A})
\\
    =&
    \lambda_i\Big( \blambda_i \bG_{1A}^T \bG_{1A}-\blambda_i\bG_{1A}^T \mydiag(\bO_{i\times i}, \bI_{r-i}) \bG_{1A}\Big)\\
    \geq&
    \lambda_r\Big( \blambda_i \bG_{1A}^T \bG_{1A}\Big)+\lambda_{p+i-r}\Big(-\blambda_i\bG_{1A}^T \mydiag(\bO_{i\times i}, \bI_{r-i}) \bG_{1A}\Big)\\
= &
\blambda_i \lambda_r(\bG_{1A}\bG_{1A}^T).
\end{aligned}
\end{equation}
Similarly, we can obtain the upper bound.
\begin{equation}\label{eq:DUpper}
\begin{aligned}
&\lambda_i(\bG_{1A}^T \bLambda_1 \bG_{1A})
\\
=&\lambda_i\Big(
\bG_{1A}^T \big(
\mydiag(\blambda_1,\ldots,\blambda_{i-1},\bO_{(r-i+1)\times(r-i+1)})+
\mydiag(\bO_{(i-1)\times(i-1)},\blambda_i,\ldots,\blambda_r)
\big)
\bG_{1A}
\Big)\\
%\leq&
%\lambda_1(\bG_{1[1:r,:]}^T \mydiag(\bO_{(i-1)\times(i-1)},\lambda_i,\ldots,\lambda_r) \bG_{1[1:r,:]})
    \leq&
\lambda_1(\bG_{1A}^T \mydiag(\bO_{(i-1)\times(i-1)},\blambda_i \bI_{r-i+1}) \bG_{1A})
\leq  \blambda_i \lambda_1(\bG_{1A}\bG_{1A}^T).
\end{aligned}
\end{equation}
    The inequality~\eqref{eq:DLU},~\eqref{eq:DLower} and~\eqref{eq:DUpper} implies that
    $$
   \sup_{1\leq i \leq r} \Big|\frac{\lambda_i(\bY^T \bY)}{n\blambda_i}-1\Big|\leq
    \max\Big(\Big|\frac{\lambda_1(\bG_{1A}\bG_{1A}^T)}{n}-1\Big|,\Big|\frac{\lambda_r(\bG_{1A}\bG_{1A}^T)}{n}-1\Big|\Big)+\frac{c_1}{n\blambda_r}\lambda_1(\bG_{1B}^T \bG_{1B}).
    $$
    We only need to prove the right hand side converges to $0$ almost surely.

    By Lemma~\ref{DSbound}, for every $t>0$, we have
    \begin{equation}\label{yaohuo2}
    \begin{aligned}
        &\Pr\Big(\sqrt{1-\frac{k}{n}}-\sqrt{\frac{r}{n}}-\frac{t}{\sqrt{n}}\leq \sqrt{\frac{\lambda_r(\bG_{1A}\bG_{1A}^T)}{n}}\leq \sqrt{\frac{\lambda_1(\bG_{1A}\bG_{1A}^T)}{n}}\leq
        \sqrt{1-\frac{k}{n}}+\sqrt{\frac{r}{n}}+\frac{t}{\sqrt{n}} \Big) \\
        &\geq 1-2\exp(-\frac{t^2}{2}).
    \end{aligned}
    \end{equation}
    Let $t=n^{1/4}$. Since $r=o(n)$, we have
    $$
\sqrt{1-\frac{k}{n}}-\sqrt{\frac{r}{n}}-\frac{t}{\sqrt{n}}\to 1\quad\text{and}\quad
\sqrt{1-\frac{k}{n}}+\sqrt{\frac{r}{n}}+\frac{t}{\sqrt{n}}\to 1.
    $$
    This, together with Borel-Cantelli lemma, yields 
    $$
    \frac{\lambda_r(\bG_{1A}\bG_{1A}^T)}{n}\to 1\,\quad
    \frac{\lambda_1(\bG_{1A}\bG_{1A}^T)}{n}\to 1,
    $$
    almost surely.
    As for $\lambda_1(\bG_{1B}^T \bG_{1B})$, by Lemma~\ref{DSbound}, we have
   \begin{equation}\label{yaohuo3}
   \Pr\Big( \frac{c_1}{n\blambda_r}{\lambda_1(\bG_{1B}\bG_{1B}^T)}\leq \frac{c_1}{n\blambda_r}(\sqrt{n-k}+\sqrt{p-r}+t)^2 \Big) 
   \geq 1-\exp(-\frac{t^2}{2}).
   \end{equation}
Let $t=n^{1/2}$, since we have assumed $\blambda_r n/p\to \infty$, we have $$\frac{c_1}{n\blambda_r}{\lambda_1(\bG_{1B}\bG_{1B}^T)}\to 0$$
almost surely.
Then~\eqref{PCAlemma1:eq1} follows.

Inequality~\eqref{PCAlemma1:eq2} and~\eqref{PCAlemma1:eq3} follows from  the fact
\begin{align*}
\lambda_{r+1}(\bY^T \bY)\leq \lambda_1(\bG_{1B}^T \bLambda_2 \bG_{1B})\leq c_1 \lambda_1(\bG_{1B}^T \bG_{1B}),\\
    \lambda_{n-k}(\bY^T \bY)\geq \lambda_{n-k}(\bG_{1B}^T \bLambda_2 \bG_{1B})\geq c_2 \lambda_{n-k}(\bG_{1B}^T \bG_{1B}),
\end{align*}
and Lemma~\ref{DSbound}.
\end{proof}


Let $\bU_\bY=(\bU_{\bY,1},\bU_{\bY,2})$, where $\bU_{\bY,1}$ and $\bU_{\bY,2}$ are the first $r$ and last $p-r$ columns of $\bU_\bY$ respectively.
\begin{lemma}\label{PCAlemma2}
    Under the assumptions of Lemma~\ref{PCAlemma1}, we have
$$
\lambda_{\max}(\bI_r-\bU_1^T \bU_{\bY,1}\bU_{\bY,1}^T \bU_1)
=O_P(\frac{ p}{\blambda_r n}).
$$
If in addition, we assume
    \begin{equation}\label{newCondition}
        \frac{\log \blambda_r}{n}\to 0,
    \end{equation}
     then
$$
\myE\lambda_{\max}(\bI_r-\bU_1^T \bU_{\bY,1}\bU_{\bY,1}^T \bU_1)
=O(\frac{ p}{\blambda_r n}).
$$
\end{lemma}
\begin{proof}
    From
$
\bU\bLambda^{1/2} \bG_1 \bG_1^T \bLambda^{1/2} \bU^T 
=\bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T
$,
we have
%$$
 %\bLambda^{1/2}\bG_1 \bG_1^T  \bLambda^{1/2}
%= \bU^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU.
%$$ 
$$
    \begin{pmatrix}        
        \bLambda_{1}^{\frac{1}{2}}\bG_{1A} \bG_{1A}^T \bLambda_1^{\frac{1}{2}}&
        \bLambda_{1}^{\frac{1}{2}} \bG_{1A}\bG_{1B}^T\bLambda_2^{\frac{1}{2}}\\
        \bLambda_{2}^{\frac{1}{2}} \bG_{1B} \bG_{1A}^T\bLambda_1^{\frac{1}{2}} &
        \bLambda_{2}^{\frac{1}{2}}\bG_{1B}\bG_{1B}^T\bLambda_2^{\frac{1}{2}}\\
    \end{pmatrix}
    =
    \begin{pmatrix}        
         \bU_1^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_1&
         \bU_1^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_2\\
        \bU_2^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_1&
         \bU_2^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_2\\
    \end{pmatrix}
$$
It follows that
$$
\begin{aligned}
 \bLambda_{2}^{\frac{1}{2}}\bG_{1B} \bG_{1B}^T  \bLambda_{2}^{\frac{1}{2}}
\geq&
    \lambda_r(\bY^T \bY) \bU_{2}^T \bU_{\bY,1}\bU_{\bY,1}^T \bU_{2}.
\end{aligned}
$$
Hence
    \begin{equation}\label{ineq:eigenvector}
\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})\leq
\frac{c_1}{\lambda_r(\bY^T \bY)} \lambda_{1}
(\bG_{1B} \bG_{1B}^T).
%=O_P(\frac{p}{\lambda_r n}),
    \end{equation}
    %where the last equality follows by Lemma~\ref{PCAlemma1} and Weyl's inequality.
    By Lemma~\ref{DSbound}, for every $t>0$, we have
    \begin{equation}\label{yaohuo1}
    \Pr\Big(\frac{1}{p}(\sqrt{p-r}-\sqrt{n-k}-t)^2 \leq \frac{1}{p}{\lambda_1(\bG_{1B}\bG_{1B}^T)}\leq \frac{1}{p}(\sqrt{p-r}+\sqrt{n-k}+t)^2 \Big) 
   \geq 1-2\exp(-\frac{t^2}{2}).
    \end{equation}
    Let $t=n^{1/2}$, then Borel-Cantelli lemma implies that 
    \begin{equation}\label{eq:qu1}
    \frac{1}{p}\lambda_1(\bG_{1B}\bG_{1B}^T)\to 1
    \end{equation}
    almost surely.
    Then~\eqref{eq:qu1},~\eqref{ineq:eigenvector} and Lemma~\ref{PCAlemma1} implies that
    $$\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})=O_P(\frac{p}{\blambda_r n}).
    $$
 The first conclusion then follows by the following simple relationship
$$
\begin{aligned}
&\lambda_{\max}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})
=
\lambda_{\max}(\bU_{\bY,1}^T \bU_{2} \bU_{2}^T \bU_{\bY,1})\\
=&
    \lambda_{\max}(\bU_{\bY,1}^T (\bI_p-\bU_{1} \bU_{1}^T) \bU_{\bY,1})
    =
\lambda_{\max}(\bI_r-\bU_{\bY,1}^T\bU_{1} \bU_{1}^T \bU_{\bY,1})\\
=&
1-\lambda_{\min}(\bU_{\bY,1}^T\bU_{1} \bU_{1}^T \bU_{\bY,1})
=
1-\lambda_{\min}( \bU_{1}^T \bU_{\bY,1}\bU_{\bY,1}^T\bU_{1})\\
=&
\lambda_{\max}(\bI_r-\bU_{1}^T \bU_{\bY,1}\bU_{\bY,1}^T\bU_{1}).
\end{aligned}
$$

Next we prove the second conclusion of the lemma.
In~\eqref{yaohuo2},~\eqref{yaohuo3} and~\eqref{yaohuo1}, we take $t=\sqrt{2\log(\blambda_r n/p)}$.
Then these inequalities,~\eqref{ineq:eigenvector} and condition~\eqref{newCondition} implies that
$$
\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})\leq
\frac{c^*p}{\blambda_r n}
$$
with probability at least $1-3p/\blambda_r n$ for some constant $c^*$.
Since $\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})\leq 1$, we have
$$
\myE\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})\leq
\frac{c^* p}{\blambda_r n}(1-\frac{3p}{\blambda_r n})
+\frac{3p}{\blambda_r n}=O(\frac{p}{\blambda_r n}).
$$
This completes the proof.
\end{proof}

\begin{lemma}\label{gg:Lemma1}
    Under the assumptions of Lemma~\ref{PCAlemma1},
    we have the following upper and lower bound for $\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}$.
    \begin{align}
        &\lambda_i (\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2})\geq
        \blambda_{i+n-k},\quad 
        \text{$i=1,\ldots, p-n+k$},
        \label{eq:mybound2}\\
        &\lambda_i\big(\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}\big)= O_P(\frac{\blambda_1 p}{\blambda_r n}) + c_1,\quad
        \text{$i=1,\ldots, r$},\label{eq:mybound3}\\
        &\lambda_i (\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2})\leq
        \blambda_i, \quad 
        \text{$i=r+1,\ldots, p$}.\label{eq:mybound1}
    \end{align}
\end{lemma}
\begin{proof}
    The inequality~\eqref{eq:mybound1} follows from the fact $\bI_p-\bP_{\bY}\leq \bI_p$.
    The inequality~\eqref{eq:mybound2} follows from the fact that $\myrank(\bP_{\bY})\leq n-k$ and Weyl's inequality.
    As for inequality~\eqref{eq:mybound3}, note that the positive eigenvalues of $\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}$  equal to the positive eigenvalues of $(\bI_p-\bP_{\bY})\bU\bLambda \bU^T (\bI_p-\bP_{\bY})$.
We write $(\bI_p-\bP_{\bY})\bU\bLambda \bU^T (\bI_p-\bP_{\bY})$ as the sum of two terms
$$
\begin{aligned}
&(\bI_p-\bP_{\bY})\bU\bLambda \bU^T (\bI_p-\bP_{\bY})
\\
=&
(\bI_p-\bP_{\bY})\bU_1\bLambda_1 \bU_1^T(\bI_p-\bP_{\bY})+(\bI_p-\bP_{\bY})\bU_2\bLambda_2 \bU_2^T (\bI_p-\bP_{\bY})
\overset{def}{=}\bR_1+\bR_2.
\end{aligned}
$$
Lemma~\ref{PCAlemma2} can be applied to control the largest eigenvalue of $\bR_1$:
$$
\begin{aligned}
&\lambda_{1}\big( \bR_1 \big)
=
\lambda_{1}\big(\bLambda_1^{1/2} \bU_1^T(\bI_p-\bP_{\bY}) \bU_1 \bLambda_1^{1/2}\big)
\leq 
\lambda_{1}\big(\bLambda_1^{1/2} \bU_1^T(\bI_p-\bU_{\bY,1}\bU_{\bY,1}^T) \bU_1 \bLambda_1^{1/2}\big)\\
\leq &
\blambda_1
\lambda_{1}\big(\bU_1^T(\bI_p-\bU_{\bY,1}\bU_{\bY,1}^T) \bU_1 \big)
= 
\blambda_1
\lambda_{1}\big(\bI_r - \bU_1^T\bU_{\bY,1}\bU_{\bY,1}^T \bU_1 \big)
=
O_P(\frac{\blambda_1 p}{\blambda_r n}).
\end{aligned}
$$
Thus, for $i=1,\ldots, r$, we have
    $$\lambda_i(\bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2})
        \leq \lambda_1(\bR_1)+\lambda_1(\bR_2)= O_P(\frac{\blambda_1 p}{\blambda_r n}) + c_1.
    $$
\end{proof}
\begin{lemma}\label{gg:Lemma2}
    Under the assumptions of Theorem~\ref{thm1}, we have
    \begin{equation}\label{eq:spiketrace2}
        \mytr\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)
= \frac{p-r-n+k}{p-r}\mytr(\bLambda_2)+o_P(\sqrt{p}),
\end{equation}
and
    \begin{equation}\label{eq:spiketrace1}
    \mytr\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)^2 
 =(1+o_P(1))\mytr(\bLambda_2^2).
    \end{equation}
\end{lemma}
\begin{proof}
    By Lemma~\ref{gg:Lemma1}, we have
$$
    \sum_{i=n-k+1}^p \blambda_i^2\leq \mytr\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)^2 \leq  r(O_P(\frac{\blambda_1 p}{\blambda_r n})+c_1)^2+\sum_{i=r+1}^p \blambda_i^2.
$$
    Hence
    \begin{equation*}
        \begin{aligned}
        &\big|\mytr\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)^2 - \sum_{i=r+1}^p \blambda_i^2 \big|\\
        \leq &
            \max\Big(\sum_{i=r+1}^{n-k} \blambda_i^2 , r(O_P(\frac{\blambda_1 p}{\blambda_r n})+c_1)^2\Big)\\
        \leq &
            r(O_P(\frac{\blambda_1 p}{\blambda_r n})+c_1)^2+O(n)=o_P(p).
        \end{aligned}
    \end{equation*}
    Then~\eqref{eq:spiketrace1} holds.

    Now we prove~\eqref{eq:spiketrace2}. Note that
    $$
\mytr\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)
=
\mytr\big(\bLambda_1^{1/2} \bU_1^T (\bI_p-\bP_{\bY})\bU_1 \bLambda_1^{1/2}\big)
+
\mytr\big(\bLambda_2^{1/2} \bU_2^T (\bI_p-\bP_{\bY})\bU_2 \bLambda_2^{1/2}\big).
    $$
    By Lemma~\ref{gg:Lemma1}, we have 
    $$\mytr\big(\bLambda_1^{1/2} \bU_1^T (\bI_p-\bP_{\bY})\bU_1 \bLambda_1^{1/2}\big)=O_P(\frac{\blambda_1 p r}{\blambda_r n})=o_P(\sqrt{p}).$$
    The second term can be written as
    $
    \mytr\big(\bLambda_2^{1/2} \bU_2^T (\bI_p-\bP_{\bY})\bU_2 \bLambda_2^{1/2}\big)
    =
    \mytr(\bLambda_2)-\mytr(\bP_{\bY}\bU_2\bLambda_2 \bU_2^T)
    $.
    For $\mytr(\bP_{\bY}\bU_2\bLambda_2 \bU_2^T)$, we have
    $$
    \begin{aligned}
        &
        \big|
    \mytr(\bP_{\bY}\bU_2\bLambda_2 \bU_2^T)
    -\frac{n-k}{p-r}\mytr(\bLambda_2)
    \big|
    =
    \Big|
        \mytr\Big(\bP_{\bY} \bU \big(\bLambda_2-\frac{1}{p-r} (\mytr \bLambda_2) \bI_{p-r} \big) \bU^T\Big)
    \Big|
        \\
        \leq &
        \sqrt{\mytr \big(\bP_{\bY}^2\big)}
        \sqrt{\mytr \Big(\bLambda_2-\frac{1}{p-r}\big(\mytr \bLambda_2\big) \bI_{p-r}\Big)^2}
        =\sqrt{(n-k)\mytr \Big(\bLambda_2-\frac{1}{p-r}\big(\mytr \bLambda_2\big) \bI_{p-r}\Big)^2}
        =o(\sqrt{p}).
    \end{aligned}
    $$
    Hence 
    $$
    \mytr\big(\bLambda_2^{1/2} \bU_2^T (\bI_p-\bP_{\bY})\bU_2 \bLambda_2^{1/2}\big)
    =
    \frac{p-r-n+k}{p-r}\mytr(\bLambda_2)+o(\sqrt{p}).
    $$
    Then~\eqref{eq:spiketrace2} holds.
\end{proof}

\begin{proof}[\textrm{Proof of Theorem~\ref{thm1}}]

    We deal with the three terms of~\eqref{eq:maindec} separately.
    Lemma~\eqref{gg:Lemma1} implies that the first term satisfies the Lyapunov condition
$$
    \frac{\lambda_1\Big(\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)^2\Big)}{\mytr \Big( \big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)^2\Big)}
%\leq
%\frac{
%\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
%}{\sum_{i=1}^{p-r-n+k}\lambda_{i+n-k}^2}
%\leq
%\frac{
%\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
%}{c(p-r-n+k)}
=
\frac{
\big( O_P(\frac{\blambda_1 p}{\blambda_r n})+c_1\big)^2
}{
    (1+o_P(1))\mytr(\bLambda_2)
}
\xrightarrow{P} 0.
$$
Apply Lyapunov central limit theorem conditioning on $\bP_{\bY}$, we have
$$
\begin{aligned}
    &\Big(\mytr \Big(\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)^2\Big) \Big)^{-1/2}\\
    &\big( \bG_2^T \bLambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}\bG_2
    -\mytr\big(\bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU \bLambda^{1/2}\big)
     \bI_{k-1} \big)
\xrightarrow{\mathcal{L}} \bW_{k-1},
\end{aligned}
$$
where $\bW_{k-1}$ is a $(k-1)\times(k-1)$ symmetric random matrix whose entries above the main diagonal are i.i.d.\ $N(0,1)$ and the entries on the diagonal are i.i.d.\ $N(0,2)$.
    This, combined with Lemma~\ref{gg:Lemma2} and Slutsky's theorem, yields
$$
\begin{aligned}
    \frac{1}{\sqrt{\mytr(\bLambda_2^2)}}
    \big( \bG_2^T \bLambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU\bLambda^{1/2}\bG_2
    -\tfrac{p-r-n+k}{p-r}\mytr(\bLambda_2)\bI_{k-1} \big)
\xrightarrow{\mathcal{L}} \bW_{k-1}.
\end{aligned}
$$

Next we show that the cross term of~\eqref{eq:maindec} is negligible. Note that
$$
\begin{aligned}
    &\myE [\|\bC^T \Xi^T (\bI_p -\bP_{Z\tilde{J}})\bU\bLambda^{1/2}\bG_2\|_F^2|\bY]\\
    = &
    (k-1)\mytr(\bC^T \Xi^T (\bI_p -\bP_{\bY})\bU\bLambda \bU^T (\bI_p -\bP_{\bY})\Xi \bC)\\
    \leq &
    (k-1)\lambda_1\big((\bI_p -\bP_{\bY})\bU\bLambda \bU^T (\bI_p -\bP_{\bY})\big)\|\Xi \bC\|^2_F\\
    \leq &
    (k-1)\lambda_1\big(\bLambda^{1/2} \bU^T (\bI_p -\bP_{\bY})\bU \bLambda^{1/2}\big)\|\Xi \bC\|^2_F\\
    = &
    (k-1) O_P(\frac{\lambda_1 p}{\lambda_r n}+c_1)  \|\Xi \bC\|^2_F\\
    = &
    (k-1) O_P\big(\frac{\lambda_1 \sqrt{p}}{\lambda_r n}+\frac{c_1}{\sqrt{p}}\big) \sqrt{p}  \|\Xi \bC\|^2_F=o_P(p),
\end{aligned}
$$
where the last equality holds since we have assumed $\frac{1}{\sqrt{p}}\|\Xi \bC\|_F^2=O(1)$.
Hence $\|\bC^T \Xi^T (\bI_p -\bP_{\bY})\bU\bLambda^{1/2}\bG_2\|_F^2=o_P(p)$. Now,
$$
\begin{aligned}
\frac{1}{\sqrt{\mytr(\bLambda_2^2)}}
    \big( \bC^T\bY^T(\bI_p-\bP_{\bY}) \bY \bC
    -\tfrac{p-r-n+k}{p-r}\mytr(\bLambda_2)\bI_{k-1} -\bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC\big)
\xrightarrow{\mathcal{L}} \bW_{k-1}.
\end{aligned}
$$
    Equivalently,
    $$
    \begin{aligned}
        &\frac{1}{\sqrt{\mytr(\bLambda_2^2)}} {\Big(\bC^T\bY^T(\bI_p-\bP_{\bY}) \bY \bC- \frac{p-r-n+k}{p-r}\mytr(\bLambda_2)\bI_{k-1}}\Big)\\
        \sim&
\frac{1}{\sqrt{\mytr(\bLambda_2^2)}} \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC
        +\bW_{k-1}+o_P(1).
    \end{aligned}
    $$
    The conclusion follows by taking the maximum eigenvalue.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{numberConsistency}]
    First we consider the case of $r>0$. By the construction of $\hat{r}$,
    $$
    \{\hat{r}=r\}\supseteq 
    \{\frac{\lambda_{r}(\bY^T \bY)}{\lambda_{r+1}(\bY^T \bY)}\geq \gamma_n\}
    \cap
    \{\frac{\lambda_{r+1}(\bY^T \bY)}{\lambda_{n-k}(\bY^T \bY)}\leq \gamma_n\}.
    $$
    Suppose $0<\epsilon< 1$ is a fixed number.
    By assumption, there exists an $n_0^*$, for $n\geq n_0^*$, $\gamma_n\leq (1-\epsilon)n\blambda_r/(c_1 p)$ and $\gamma_n\geq (1+\epsilon) c_1/c_2 $. Thus
    $$
    \{\hat{r}=r\}\supseteq 
    \{\frac{\lambda_{r}(\bY^T \bY)}{\lambda_{r+1}(\bY^T \bY)}\geq (1-\epsilon)\frac{n\blambda_r}{c_1 p}\}
    \cap
    \{\frac{\lambda_{r+1}(\bY^T \bY)}{\lambda_{n-k}(\bY^T \bY)}\leq (1+\epsilon) \frac{c_1}{c_2} \}.
    $$
     Lemma~\ref{PCAlemma1} implies that almost surely, there exists an $n_0$, for $n\geq n_0$, we have
    $$\frac{\lambda_r(\bY^T \bY)}{\lambda_{r+1}(\bY^T \bY)}\geq (1-\epsilon)\frac{n\blambda_r}{c_1 p},
    \quad
    \frac{\lambda_{r+1}(\bY^T \bY)}{\lambda_{n-k}(\bY^T \bY)}\leq (1+\epsilon)\frac{c_1}{c_2}.
    $$
    This yields $\Pr(\hat{r}=r)\to 1$ for $r>0$.
    The case of $r=0$ can be similarly proved by noting that
    $$
    \{\hat{r}=r\}\supseteq 
    \{\frac{\lambda_{1}(\bY^T \bY)}{\lambda_{n-k}(\bY^T \bY)}\leq \gamma_n\}.
    $$
\end{proof}

\begin{proof}[Proof of Proposition~\ref{traceProp1}]
    Since $\hat{r}$ is a consistent estimator of $r$, we only need to prove
    $$
    \frac{1 }{n-k}
        \sum_{i=r+1}^{n-k}\lambda_i(\bY^T\bY)
        =\mytr(\bLambda_2)+O_P(\sqrt{p}).
    $$
    Note that
$$
\bY^T \bY =\bG_1^T \bLambda \bG_1
=
\bG_{1A}^T \bLambda_1 \bG_{1A}+
\bG_{1B}^T \bLambda_2 \bG_{1B}.
$$
By Weyl's inequality, for $i=r+1,\ldots, n-k$, we have
$$
\lambda_{i}(\bG_{1B}^T \bLambda_2 \bG_{1B})\leq \lambda_i(\bY^T \bY)\leq \lambda_{i-r}(\bG_{1B}^T \bLambda_2 \bG_{1B}).
$$
It follows that
$$
\sum_{i=r+1}^{n-k}\lambda_{i}(\bG_{1B}^T \bLambda_2 \bG_{1B})\leq \sum_{i=r+1}^{n-k}\lambda_i(\bY^T \bY)\leq \sum_{i=1}^{n-k-r}\lambda_{i}(\bG_{1B}^T \bLambda_2 \bG_{1B}).
$$
Hence
$$
 \big|\sum_{i=r+1}^{n-k}\lambda_i(\bY^T \bY)- \mytr(\bG_{1B}^T \bLambda_2 \bG_{1B})\big|\leq r \lambda_1(\bG_{1B}^T \bLambda_2 \bG_{1B})=O_P(rp).
$$
But central limit theorem implies that
$$\mytr(\bG_{1B}^T \bLambda_2 \bG_{1B})-(n-k)\mytr(\bLambda_2)%=\sqrt{(n-k)\mytr(\bLambda_2^2)}O_P(1)
=O_P(\sqrt{np}).$$
Thus
$$
    \begin{aligned}
        &\frac{1}{n-k}\sum_{i=r+1}^{n-k}\lambda_i (\bY^T \bY)
        =\mytr(\bLambda_2)+
\frac{1}{n-k}\big|\sum_{i=r+1}^{n-k}\lambda_i(\bY^T \bY)- (n-k)\mytr(\bLambda_2)\big|\\
        =&\mytr(\bLambda_2)+O_P(\frac{rp}{n})
        =\mytr(\bLambda_2)+O_P(\sqrt{p}),
    \end{aligned}
$$
    where the last equality follows form Assumption~\ref{assumpEigen}.
\end{proof}

    \begin{proof}[Proof of Proposition~\ref{Taichangle}]
        Let $\bU_{\bY,1;(i,j)}$ be the first $r$ columns of $\bU_{\bY;(i,j)}$.
        Let $\bU_{\bY,2;(i,j)}$ be a $p\times (p-r)$ orthogonal matrix satisfying $\bU_{\bY,2;(i,j)}\bU_{\bY,2;(i,j)}^T= \bI_p-\bU_{\bY,1;(i,j)}\bU_{\bY,1;(i,j)}^T$.
        Then by Lemma~\ref{PCAlemma2}, we have
        $$
        \lambda_1(\bU_1^T \bU_{\bY,2;(i,j)}\bU_{\bY,2;(i,j)}^T \bU_1)
        =
        \lambda_1(\bI_r-\bU_1^T \bU_{\bY,1;(i,j)}\bU_{\bY,1;(i,j)}^T \bU_1)
        =
        O_P(\frac{p}{\blambda_r n}).
        $$


        First, we prove that $w_{ij}^2$ is an approximation of $Y_i^T \bU_2 \bU_2^T Y_j$.
        For $1\leq i<j\leq n-k$, define $\epsilon_{ij}= w_{ij}-Y_i^T \bU_{2} \bU_{2}^T Y_j$, then we have
        \begin{equation}\label{emLong}
    \begin{aligned}
        &\epsilon_{ij}
        =Y_i^T (\tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_2 \bU_2^T) Y_j\\
        =&Y_i^T(\bU_1 \bU_1^T+\bU_2 \bU_2^T) (\tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_2 \bU_2^T)(\bU_1 \bU_1^T+\bU_2 \bU_2^T) Y_j\\
        =&Y_i^T \bU_2 \bU_2^T (\tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_2\bU_2^T) \bU_2 \bU_2^T Y_j\\
        &+Y_i^T \bU_1 \bU_1^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bU_2 \bU_2^T Y_j\\
        &+Y_i^T \bU_2 \bU_2^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bU_1 \bU_1^T Y_j\\
        &+Y_i^T \bU_1 \bU_1^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bU_1 \bU_1^T Y_j\\
        =&
            Y_i^T\bU_2 \bU_2^T ( \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T)\bU_2 \bU_2^T Y_j\\
            &+
            Y_i^T \bU_2 \bU_2^T(\bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T-\bU_2 \bU_2^T)\bU_2 \bU_2^T Y_j\\
        &+Y_i^T \bU_1 \bU_1^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bU_2 \bU_2^T Y_j\\
        &+Y_i^T \bU_2 \bU_2^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bU_1 \bU_1^T Y_j\\
        &+Y_i^T \bU_1 \bU_1^T \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T \bU_1 \bU_1^T Y_j\\
        \overset{def}{=} &
        \epsilon_{ij}^{(1)}+
        \epsilon_{ij}^{(2)}+
        \epsilon_{ij}^{(3)}+
        \epsilon_{ij}^{(4)}+
        \epsilon_{ij}^{(5)}.
    \end{aligned}
    \end{equation}
        We deal with the five terms separately.
        First we deal with $\epsilon_{ij}^{(1)}$.
        It can be seen that
         $Y_i$, $Y_j$ and $\tilde{\bU}_{\bY;(i,j)}$ are mutually independent and
        $\tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T$ is a projection matrix whose rank is not larger than $n-k-2-r$.
        Then
        $$
        \begin{aligned}
            &\myE (\epsilon_{ij}^{(1)})^2\\
            %&\myE\|Y_i^T\bU_2 \bU_2^T ( \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T)\bU_2 \bU_2^T Y_j\|^2\\
            =&\myE \mytr (\bLambda_2^{1/2}\bU_2^T(\tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T-\bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T)\bU_2 \bLambda_2^{1/2})^2\\
            \leq & c_1^2 (n-k-2-r)=o(p).
        \end{aligned}
        $$
        Next we deal with $\epsilon_{ij}^{(2)}$. we have
        $$
        \begin{aligned}
            &\myE (\epsilon_{ij}^{(2)})^2\\
            =&\myE \mytr (\bLambda_2^{1/2}\bU_2^T(
            \bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T-\bU_2\bU_2^T
            )\bU_2 \bLambda_2^{1/2})^2\\
            \leq & c_1^2 
            \myE \mytr (\bU_2^T(
            \bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T-\bU_2\bU_2^T
            )\bU_2 )^2\\
            = & c_1^2 
            \myE \mytr (
            \bI_{p-r}-\bU_2^T \bU_{\bY,2;(i,j)} \bU_{\bY,2;(i,j)}^T\bU_2
             )^2\\
            = & c_1^2
        \myE \mytr (\bI_r-\bU_1^T \bU_{\bY,1;(i,j)}\bU_{\bY,1;(i,j)}^T \bU_1)^2\\
            \leq  & c_1^2 r
         \myE \lambda_1^2 (\bI_r-\bU_1^T \bU_{\bY,1;(i,j)}\bU_{\bY,1;(i,j)}^T \bU_1)\\
            =&  O(r (\frac{p}{\blambda_r n})^2)=o(p).
        \end{aligned}
        $$

        Note that $\epsilon_{i,j}^{(3)}$ and $\epsilon_{i,j}^{(4)}$ have the same distribution, we have
        $$
        \begin{aligned}
            &\myE(\epsilon_{ij}^{(3)})^2 =
            \myE(\epsilon_{ij}^{(4)})^2 \\
            =&\myE \mytr ( \bLambda_1^{1/2}\bU_1^T\tilde{\bU}_{\bY;(i,j)}\tilde{\bU}_{\bY;(i,j)}^T \bU_2 \bLambda_2 \bU_2^T \tilde{\bU}_{\bY;(i,j)}\tilde{\bU}_{\bY;(i,j)}^T \bU_1 \bLambda_1^{1/2})\\
            \leq& c_1 \blambda_1  \myE \mytr ( \bU_1^T\tilde{\bU}_{\bY;(i,j)}\tilde{\bU}_{\bY;(i,j)}^T \bU_2 \bU_2^T \tilde{\bU}_{\bY;(i,j)}\tilde{\bU}_{\bY;(i,j)}^T \bU_1 )\\
            \leq& c_1 \blambda_1  \myE \mytr ( \bU_1^T\tilde{\bU}_{\bY;(i,j)}\tilde{\bU}_{\bY;(i,j)}^T \bU_1 )\\
            \leq& c_1 \blambda_1  \myE \mytr ( \bU_1^T{\bU}_{\bY,2;(i,j)}{\bU}_{\bY,2;(i,j)}^T \bU_1 )\\
            \leq& c_1 \blambda_1 r \frac{p}{\blambda_r n}=o(p).
        \end{aligned}
        $$
        As for $\epsilon_{i,j}^{(5)}$, we have
        $$
        \begin{aligned}
            &\myE(\epsilon_{ij}^{(5)})^2 \\
            =&\myE \mytr (\bLambda_1^{1/2}\bU_1^T
            \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T\bU_1 \bLambda_1^{1/2})^2\\
            \leq & \blambda_1^2\myE \mytr (\bU_1^T
            \tilde{\bU}_{\bY;(i,j)} \tilde{\bU}_{\bY;(i,j)}^T\bU_1 )^2\\
            \leq&  \blambda_1^2  \myE \mytr ( \bU_1^T{\bU}_{\bY,2;(i,j)}{\bU}_{\bY,2;(i,j)}^T \bU_1 )^2\\
            \leq& \blambda_1^2 r (\frac{p}{\blambda_r n})^2=o(p).
        \end{aligned}
        $$

        Note that
    $$
        \begin{aligned}
            &\widehat{\mytr(\bLambda_2^2)}\\
            =&
        \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 \\
            &+
            \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}\big((\sum_{l=1}^5\epsilon_{ij}^{(l)})(Y_i^T \bU_2 \bU_2^T Y_j)
            + (\sum_{l=1}^5\epsilon_{ij}^{(l)})^2\big).
        \end{aligned}
    $$
        We have
        $$
            \begin{aligned}
                &\myE\Big|\frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}\big((\sum_{l=1}^5\epsilon_{ij}^{(l)})(Y_i^T \bU_2 \bU_2^T Y_j)
                +
                (\sum_{l=1}^5\epsilon_{ij}^{(l)})^2\big)\Big|\\
                \leq&
                \myE\Big|(\sum_{l=1}^5\epsilon_{12}^{(l)})(Y_1^T \bU_2 \bU_2^T Y_2)
            +
            (\sum_{l=1}^5\epsilon_{12}^{(l)})^2\Big|\\
            \leq&
            \sqrt{\myE\big(\sum_{l=1}^5\epsilon_{12}^{(l)}\big)^2\myE\big(Y_1^T \bU_2 \bU_2^T Y_2\big)^2}
            +
            \myE (\sum_{l=1}^5\epsilon_{12}^{(l)})^2=o(p).
        \end{aligned}
    $$
    It follows that
$$
    \begin{aligned}
        \widehat{\mytr(\bLambda_2^2)}
        =
    \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 
        +o_P(p).
    \end{aligned}
$$

        Now we only need to prove that
        $$
    \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 
        $$
        is ratio consistent.
        Since $\myE (Y_i^T \bU_2 \bU_2^T Y_j)^2=\mytr(\bLambda_2^2)$ for $i< j$, we have
    $$
        \myE
    \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 
        =\mytr(\bLambda_2^2).
        $$
        To prove the proposition, we only need to show that 
        $$
        \myVar
    \Big(
    \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 
    \Big)
        =o(\mytr^2(\bLambda_2^2)).
        $$
         Note that
         $$
        \begin{aligned}
            &\myE\Big(
    \frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 
            \Big)^2\\
            =&
            \frac{4}{(n-k)^2(n-k-1)^2}\Big(\sum_{1\leq i < j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2\Big)^2\\
            =&
            \frac{4}{(n-k)^2(n-k-1)^2}\myE\Big( \sum_{i<j}(Y_i^T\bU_2 \bU_2^T Y_j)^4\\
            &+
            \sum_{i<j,k<l:\{i,j\}\{k,l\}=\emptyset}(Y_i^T\bU_2 \bU_2^T Y_j)^2 (Y_k^T\bU_2 \bU_2^T Y_l)^2\\
            &+2\sum_{i<j<k}\big((Y_i^T\bU_2 \bU_2^T Y_j)^2(Y_i\bU_2 \bU_2^T Y_k)^2\\
            &+(Y_i^T\bU_2 \bU_2^T Y_j)^2(Y_j\bU_2 \bU_2^T Y_k)^2+(Y_i^T\bU_2 \bU_2^T Y_k)^2(Y_j\bU_2 \bU_2^T Y_k)^2\big) \Big)\\
            =&
    \frac{4}{(n-k)^2(n-k-1)^2}\Big(
            \frac{(n-k)(n-k-1)}{2}\big(6\mytr(\bLambda_2^4)+3\mytr^2(\bLambda_2^2)\big)\\
            &+\frac{(n-k)(n-k-1)(n-k-2)(n-k-3)}{4}\mytr^2(\bLambda_2^2)\\
            &+(n-k)(n-k-1)(n-k-2)\big(2\mytr(\bLambda_2^4)
            +\mytr^2(\bLambda_2^2)\big)
            \Big)\\
            =&
            \mytr^2(\bLambda_2^2)(1+o(1)).
        \end{aligned}
         $$
        It follows that
        $$
        \myVar
    \Big(\frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}(Y_i^T \bU_2 \bU_2^T Y_j)^2 
    \Big)
            =
            o(\mytr^2(\bLambda_2^2)).
        $$
        This completes the proof.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Supplementary Materials}

Contain
the brief description of the online supplementary materials.
\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Acknowledgements}

Write the acknowledgements here.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\markboth{\hfill{\footnotesize\rm FIRSTNAME1 LASTNAME1 AND FIRSTNAME2 LASTNAME2} \hfill}
{\hfill {\footnotesize\rm GENERALIZED LIKELIHOOD RATIO TEST} \hfill}

%\iffalse
\bibhang=1.7pc
\bibsep=2pt
\fontsize{9}{14pt plus.8pt minus .6pt}\selectfont
\renewcommand\bibname{\large \bf References}
%\begin{thebibliography}{11}
\expandafter\ifx\csname
natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL}\fi
%\fi

\bibliographystyle{chicago}      % Chicago style, author-year citations
\bibliography{mybibfile}   % name your BibTeX data base

%-------------------------------------------
\vskip .65cm
\noindent
first author affiliation
\vskip 2pt
\noindent
E-mail: (first author email)
\vskip 2pt

\noindent
second author affiliation
\vskip 2pt
\noindent
E-mail: (second author email)

\end{document}
