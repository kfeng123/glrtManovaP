\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xeCJK}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}


\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\ud}{\mathbf{d}}


\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{Condition}

\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}


\begin{document}

\begin{frontmatter}

\title{A generalized likelihood ratio test for multivariate analysis of variance in high dimension}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
    This paper considers in the high dimensional setting a canonical testing problem, namely testing the equality of multiple mean vectors of normal distribution.
    Motivated by Roy's union-intersection principal, we propose a generalized likelihood ratio test.
    The critical value is determined by permutation method.
    We introduce an algorthm for permuting procedure, whose complexity does not depend on data dimension.
    The limiting distribution of the test statistic is derived in two different setting: non-spiked covariance and spiked covariance.
    Theoretical results and simulation studies show that the test is particularly powerful under spiked covariance.
\end{abstract}

\begin{keyword}
\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers
\section{Introduction}
Suppose we have independent observations $X_{ij}\in \mathbb{R}^p$ ($j=1,\ldots,n_k$; $i=1,\ldots, K$) with distribution $N_p(\mu_i,\Sigma)$, where $\mu_i$, $i=1,\ldots,K$, and $\Sigma>0$ are unknown. We would like to test
\begin{equation}\label{hypothesis}
    H: \mu_1=\mu_2=\cdots=\mu_k\quad \text{v.s.}\quad K: \text{$\mu_i\neq \mu_j$ for some $i\neq j$}.
\end{equation}
The problem is known as one-way multivariate analysis of variance (MANOVA).
There are four classical tests for hypothesis~\eqref{hypothesis}: Wilks' Lambda (which is also the LRT), Hotelling-Lawley trace, Pillai Trace and Roy's maximum root.


In some modern scientific applications, people would like to test hypothesis~\eqref{hypothesis} in high dimensional setting, i.e., $p$ is greater than $n=\sum_{i=1}^{K}n_i$. See, for example,~\cite{Tsai2009}.
However, when $p>n-K$, the LRT for hypothesis~\eqref{hypothesis} is not well defined.
  Researchers have done extensive work to study the testing problem~\eqref{hypothesis} in high dimensional setting.
 Sor far, most tests in the literature are designed for two sample case, i.e. $K=2$.
  See, for example,~\cite{Bai1996Efiect},~\cite{Chen2010A},~\cite{Srivastava2009A},~\cite{Feng2015Multivariate} and~\cite{Tony2013}.
  For the multiple sample case,~\cite{Schott2007Some} modified the Dempster's trace test and proposed a test statistic
  $$
  T_{SC}=\frac{1}{\sqrt{n-1}}\big(
  \frac{1}{K-1}\mytr\big(\sum_{i=1}^K n_i\bar{X}_i\bar{X}_i^T-n\bar{X}\bar{X}^T\big)-\frac{1}{n-K}\mytr\big(\sum_{i=1}^K \sum_{j=1}^{n_i}X_{ij}X_{ij}^T-\sum_{i=1}^K n_i\bar{X}_i\bar{X}_i^T\big)
  \big),
  $$
  where $\bar{X}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ and $\bar{X}=n^{-1}\sum_{i=1}^K\sum_{j=1}^{n_i}X_{ij}$.
  In another work,~\cite{Cai2014High} proposed a test statistic
  $$
  T_{CX}=\max_{1\leq i\leq p} \sum_{1\leq j<l\leq K}\frac{n_j n_l}{n_j+n_l}\frac{(\Omega(\bar{X}_j-\bar{X}_l))_i^2}{\omega_{ii}},
  $$
  Where $\Omega=(\omega)_{ij}=\Sigma^{-1}$ is the precision matrix. When $\Omega$ is unknown, they substitute it by an estimator $\hat{\Omega}$.
  Stitistics $T_{SC}$ and $T_{CX}$ are the representatives of two popular methodologies for high dimensional tests.
  $T_{SC}$ is a so-called sum-of-squares type statistic as it is based on an estimation of squared Euclidean norm $\sum_{i=1}^K n_i\|\mu_i-\bar{\mu}\|^2$, where $\bar{\mu}=n^{-1}\sum_{i=1}^K n_i \mu_i$.
  $T_{CX}$ is an extreme value type statistic.
  
  %Suppose $\{X_{i1},\ldots, X_{in_i}\}$ are i.i.d.\ distributed as $N(\mu_i,\Sigma)$ for $1\leq i\leq K$.
%The $k$ samples are independent.
%%$\mu_i$, $i=1\ldots, k$ and $\Sigma>0$ are unknown. An interesting problem in multivariate analysis is to test the hypotheses
%\begin{equation}
    %H: \mu_1=\mu_2=\cdots=\mu_k\quad v.s.\quad K: \textrm{$\mu_i\neq \mu_j$ for some $i\neq j$}.
%\end{equation}
%The likelihood ratio test (LRT) has a dominated position in classical multivariate analysis.
   Note that both sum-of-squares type statistic and extreme value type statistic are not based on likelihood function.
    It remains a problem how to construct likelihood-based tests in high dimensional setting.
    In a recent work,~\cite{Zhao2016A} proposed a generalized likelihood ratio test in the context of one-sample mean vector test.
Inspired by Roy's union-intersection tests (\cite{Roy1953}),
    they write the null hypothesis as the intersection of a class of component hypotheses. For each component hypotheses, the likelihood ratio test is constructed. Using a least favorable argument, they construct a test statistics based on these tests.
    Their simulation results showed that their test has particular good power performance when the variables are dependent.

    Following~\cite{Zhao2016A}'s methodology, we proposed a generalized likelihood ratio test for hypothesis~\eqref{hypothesis}.
    Most existing tests for hypothesis~\eqref{hypothesis} imposed conditions which prevent from large leading eigenvalues of $\Sigma$.
    However, when the correlations between variables are determined by a small number of factors, $\Sigma$ is spiked in the sense that a few leading eigenvalues are much larger than the others. See, for example~\cite{Cai2012Sparse} and~\cite{Shen2013Consistency}.
    We derive the asymptotic distribution of the test statistic under both spiked and non-spiked covariance.
    Our theoretical results imply that the new test is particularly powerful under spiked covariance.
    We conduct a simulation study to examine the numerical performance of the test.

    The rest of the paper is organized  as follows.

    Higher criticism CX are special case of UIT.
    
\section{Methodology}
\subsection{Roy's maximum root}
Roy's maximum root test statistic is derived in~\cite{Roy1953} as an example of Roy's union intersection principle.
The idea of Roy's union intersection principle is to reduce testing problem to a class of pseudo-univariate problems.
For $a\in \mathbb{R}^p$ and $a^T a=1$, define the hypothesis $H_a$ and $K_a$ as
$$
H_a: a^T\mu_1=a^T\mu_2=\cdots=a^T\mu_k\quad \text{v.s.}\quad K: \text{$a^T\mu_i\neq a^T\mu_j$ for some $i\neq j$}.
$$
Then the hypothesis~\eqref{hypothesis} $H=\cap_{a}H_a$.
The union-intersection principle tells that $H$ is rejected if and only if any one of $H_a$ is rejected.
Let $\bX_i=(X_{i1},\ldots,X_{in_i})$ be the $i$th sample, $i=1,\ldots,K$. Let $\bZ=(\bX_1,\ldots,\bX_k)$ be the pool sample.
Note that the likelihood function based on $a^T \bZ$ is 
$$
f_a(a^T \bZ;\mu_1,\ldots,\mu_k,\Sigma)=
    (2\pi)^{-n/2}|a^T \Sigma a|^{-n/2}\exp\Big(-\frac{1}{2 a^T \Sigma a}\sum_{i=1}^K\sum_{j=1}^{n_i}(a^T X_{ij}-a^T\mu_i)^2\Big).
$$

The likelihood ratio test statistic for $H_a$ v.s. $K_a$ (which is also uniformly most powerful unbiased test) is
%Note that
%\begin{equation*}
    %\begin{aligned}
        %\max_{\mu_1,\ldots,\mu_k,\Sigma}f_a(a^T Z,\mu_1,\ldots,\mu_k,\Sigma)
        %=
        %(2\pi)^{-n/2}\big(\frac{1}{n} a^T G a\big)^{-n/2}e^{-{n}/{2}},
    %\end{aligned}
%\end{equation*}
%%Let $S_i=\sum_{j=1}^{n_i}(x_{ij}-\bar{\mathbf{X}}_i)(x_{ij}-\bar{\mathbf{X}}_i)^T$ and $S=\sum_{i=1}^k S_i$.
%\begin{equation*}
    %\begin{aligned}
        %\max_{\mu,\Sigma}f_a(a^T Z,\mu,\ldots,\mu,\Sigma)
        %=
        %(2\pi)^{-n/2}\big(\frac{1}{n}a^T (G+F)a\big)^{-n/2}e^{-{n}/{2}},
    %\end{aligned}
%\end{equation*}
\begin{equation*}
    \begin{aligned}
        \text{LRT}_{a}=\frac{\max_{\mu_1,\ldots,\mu_k,\Sigma}f_a(a^T Z,\mu_1,\ldots,\mu_k,\Sigma)}{\max_{\mu,\Sigma}f_a(a^T Z,\mu,\ldots,\mu,\Sigma)}
        =
        \Big(1+\frac{a^T Fa}{a^T G a}\Big)^{n/2},
    \end{aligned}
\end{equation*}
where $G=\sum_{i=1}^K\sum_{j=1}^{n_i}(X_{ij}-\bar{\mathbf{X}}_i)(X_{ij}-\bar{\mathbf{X}}_i)^T$ and $F=\sum_{i=1}^K n_i (\bar{\mathbf{X}}_i-\bar{\mathbf{X}})(\bar{\mathbf{X}}_i-\bar{\mathbf{X}})^T$.
Follow the type II method of~\cite{Roy1953}, the union intersection test statistic is $\max_{a^T a=1}\text{LRT}_a=(1+\lambda_{\max}(FG^{-1}))^{n/2}$ which is an increase function of $\lambda_{\max}(FG^{-1}))$, Roy's maximum root test statistic.
Note that here we need the assumption $p\leq n-K$, or else $G$ is not invertible.
%%$$
%f(Z;\mu_1,\ldots,\mu_k,\Sigma)=\prod_{i=1}^k\Big[
    %(2\pi)^{-n_i p/2}|\Sigma|^{-n_i/2}\exp(-\frac{1}{2}\mathrm{tr}\Sigma^{-1}\sum_{j=1}^{n_i}(x_{ij}-\mu_i)(x_{ij}-\mu_i)^T)
    %\Big].
%$$
\subsection{The new test statistic}
We are interested in the case when $p> n-K$.
In this setting, $\max_{a^T a=1}\text{LRT}_a=+\infty$ and Roy's maximum root test is not defined. 
In another viewpoint, union intersection principal finds an direction $a$ along which the evidence against null hypothesis is maximized.
Such an $a$ is data dependent.
In the classical setting, the evidence of direction $a$ is $\text{LRT}_a$.
In the current context, there are a class of $a$ such that $\text{LRT}_a$ achieve the infinity, the largest evidence in classical sense.
We need to further choose a single $a$ from $\{a\,|\,\text{LRT}_a=+\infty\text{ and }a^T a =1\}$.
 From the expression of $\text{LRT}_a$, we would like to make the largest discrepancy between $a^T F a$ and $a^T G a$.
Note that if $\text{LRT}_{\alpha}=+\infty$, then $a^T G a=0$.
Hence it's natural to choose $a$ as
$$
        a^{*}=
        \argmax_{a^T a=1, a^T G a=0} 
        a^T F a.
$$
Since $a^{*T}G a^*=0$, we propose the following test statistic for $H$:
\begin{equation*}
    \begin{aligned}
        T=a^{*T} F a^*
        =
        \max_{a^T a=1, a^T G a=0} 
        a^T F a.
    \end{aligned}
\end{equation*}
When $T$ is large enough, we reject $H$.
The above strategy is first proposed by~\cite{Zhao2016A} in the context of testing one sample mean vector.


Next we derive the explicit forms of the test statistic. 
Let $J=\mathrm{diag}(n_1^{-1/2}\mathbf{1}_{n_1},\ldots,n_k^{-1/2}\mathbf{1}_{n_k})$.
Then the matrices $I_n-JJ^T$, $JJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ and $\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ are three $n\times n$ projection matrices which are pairwise orthogonal with rank $n-k$, $k-1$ and $1$.
Let $\tilde{J}$ be a $n\times (n-k)$ matrix satisfying $\tilde{J}\tilde{J}^T =I-JJ^T$.
Note that $I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J$ is a $k\times k$ projection matrix with rank $k-1$.
Let $C$ be a $k\times (k-1)$ matrix satisfying $CC^T=I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J$.
Then 
$$
G=Z(I_n-JJ^T)Z^T=
Z\tilde{J}\tilde{J}^T Z^T.
$$
and
\begin{equation*}
    \begin{aligned}
        F=\sum_{i=1}^k n_i (\bar{\mathbf{X}}_i-\bar{\mathbf{X}})(\bar{\mathbf{X}}_i-\bar{\mathbf{X}})^T 
        =Z(JJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)Z^T
=ZJ(I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J)J^T Z^T
=ZJC C^T J^T Z^T.
    \end{aligned}
\end{equation*}

Let $Z\tilde{J}=U_{Z\tilde{J}}D_{Z\tilde{J}}V_{Z\tilde{J}}^T$ be the singular value decomposition of $Z\tilde{J}$, where $U_{Z\tilde{J}}$ and $V_{Z\tilde{J}}$ are $p\times (n-K)$ and $(n-K)\times(n-K)$ column orthogonal matrices respectively, $D_{Z\tilde{J}}$ is $(n-K)\times (n-K)$ diagonal matrix.
Let $H_{Z\tilde{J}}=U_{Z\tilde{J}}U_{Z\tilde{J}}^T$ be the projection on the column space of $A$.
Then by Proposition~\ref{optProp}, 
\begin{equation}\label{statisticForm1}
\begin{aligned}
    T&=\lambda_{\max}\big(ZJCC^TJ^TZ^T (I_p-
    H_{Z\tilde{J}})
    \big)
    =\lambda_{max}\big(C^TJ^TZ^T (I_p-
    H_{Z\tilde{J}}
    )ZJC\big).
\end{aligned}
\end{equation}

Next we introduce another form of $T$.
%where $H_A=Z\tilde{J}{(\tilde{J}^T Z^T Z\tilde{J})}^{-1}\tilde{J}^T Z^T$.
By the relationship
\begin{equation*}
    \begin{aligned}
        \begin{pmatrix}
            J^T Z^T ZJ & J^T Z^T Z\tilde{J}\\
            \tilde{J}^T Z^T ZJ & \tilde{J}^T Z^T Z \tilde{J}
        \end{pmatrix}^{-1}
        =
        \Big(
        \begin{pmatrix}
            J^T\\
            \tilde{J}^T
        \end{pmatrix}
        Z^T Z
        \begin{pmatrix}
            J&\tilde{J}
        \end{pmatrix}
        \Big)^{-1}
        =
        \begin{pmatrix}
            J^T {(Z^T Z)}^{-1}J & J^T {(Z^T Z)}^{-1}\tilde{J}\\
            \tilde{J}^T {(Z^T Z)}^{-1}J & \tilde{J}^T {(Z^T Z)}^{-1} \tilde{J}
        \end{pmatrix}
    \end{aligned}
\end{equation*}
and matrix inverse formula, we have that
\begin{equation*}
    \begin{aligned}
        &{\big( J^T {(Z^T Z)}^{-1}J \big)}^{-1}
        =J^T Z^T ZJ - J^T Z^T Z\tilde{J}{(\tilde{J}^T Z^T Z \tilde{J})}^{-1}
            \tilde{J}^T Z^T ZJ 
        = J^T Z^T( I_p- H_{Z\tilde{J}}) ZJ.
    \end{aligned}
\end{equation*}
Thus, 
\begin{equation}\label{statisticForm2}
    \begin{aligned}
        T(Z)=
        \lambda_{\max}\Big(C^T{\big( J^T {(Z^T Z)}^{-1}J \big)}^{-1}C\Big).
    \end{aligned}
\end{equation}

While the form~\eqref{statisticForm1} is used for theoretical analysis, the form~\eqref{statisticForm2} is well suited for computation, as we shall see.
\subsection{Permutation method}
Permutation method is a powerful tool to determine the critical value of a test statistic.
   The test procedure resulting from permutation method is exact as long as the null distribution of observations are exchangeable. See, for example,~\cite{Romano1990On}.
   The major down-side to permutation method is that it can be computationally intensive.
   Fortunately, for our statistic, there is a fast implementation of the permutation method.
   Infact, using expression~\eqref{statisticForm2}, a permutation for our statistic can be written by
    \begin{equation}\label{permutedStatistic}
        T^{*}=\lambda_{\max}\Big(C^T{\big( J^T K^T {(Z^T Z)}^{-1} K J \big)}^{-1}  C\Big).
    \end{equation}
   for a random $n\times n$ permutation matrix $K$.
   Note that ${(Z^T Z)}^{-1}$, the most time-consuming component, can be calculated aforehand.
   The permutation procedure for our statistic can be summarized as:
   \begin{enumerate}[(I)]
       \item Calculate $T(Z)$ according to~\eqref{statisticForm2}, hold intermediate result ${(Z^T Z)}^{-1}$.
       \item Generate a random $n\times n$ permutation matrix $K$ and calculate $T^*(Z)$ according to~\eqref{permutedStatistic}. Repeat $M$ times to obtain $T^*_1(Z),\ldots,T^*_M(Z)$.
       \item Calculate the $p$-value by
           $
           \tilde{p}={(M+1)}^{-1}\big[1+\sum_{i=1}^B I\{T^*_i(Z)\geq T(Z)\}\big]
           $.
   \end{enumerate}

Here $M$ is the permutation times. 
   It can be shown that for any integer $M>0$, the resulting test controls the Type I error, more precisely, $\Pr(\tilde{p}\leq u)\leq u$ for all $0\leq u\leq 1$.
   Moreover, as $M$ tends to $\infty$, $\lim_{M\to \infty}\Pr(\tilde{p}\leq u)= u$.
   See, for example,~\cite{Lehmann}, Chapter $15$.

   It can be seen that the time complexities of step (I) and step (II) are $O(n^2 p +n^3)$ and $O(n^2 M)$, respectively.
   In large sample or high dimensional setting, $M/(p+n)$ is small. In this case, the permutation procedure has negligible effect on total time complexity.




   
   
   


%\section{Schott's method}

%$$
%E=ZZ^T-\sum_{i=1}^k n_i \bar{X}_i \bar{X}_i^T.
%$$

%$$
%H=\sum_{i=1}^{k} n_i \bar{X}_i \bar{X}_i^T - n\bar{X}\bar{X}^T.
%$$

%$$
%\mytr E = \mytr Z^T Z - \mytr J^T Z^T Z J.
%$$


%$$
%\mytr H = \mytr J^T Z^T Z J - \frac{1}{n} 1_n^T Z^T Z 1_n
%$$

%$$
%T_{SC}=\frac{1}{\sqrt{n-1}}(
%\frac{1}{k-1}\mytr H-\frac{1}{n-k} \mytr E
%)
%$$


\section{Theory}
In this section, we investigate the asymptotic behavior of our test statistic when $p$ is much larger than $n$.
More precisely, we shall assume $p/n\to\infty$.
In high dimensional setting, it is a common phenomenon that the asymptotic distribution of statistic relies on the covariance structure.
See, for example,~\cite{Ma2015A} and Rui Wang's paper.
We shall investigate the asymptotics of our statistic under two different covariance structures: non-spiked covariance and spiked covariance.




Let $\Sigma= U\Lambda U^T$ be the eigenvalue decomposition of $\Sigma$, where $\Lambda =\mydiag (\lambda_1,\ldots,\lambda_p)$.


The spiked covariance model assumes that a few eigenvalues of $\Sigma$ are significantly larger than the others. This model is a standard model in many problems and takes factor model as a special case.
See, for example,.

\begin{assumption}\label{assumpEigen}
    Let $r$ be a fixed integer.
    Assume ${\lambda_r n}/{p}\to \infty$ and $C \geq \lambda_{r+1} \geq \ldots \geq \lambda_{p} \geq c$, where $c$ and $C$ are absolute constant.
\end{assumption}
Let $U=(U_1,U_2)$ where $U_1$ is $p\times r$ and $U_2$ is $p\times (p-r)$. Let $\Lambda_1=\mydiag(\lambda_1,\ldots,\lambda_r)$ and $\Lambda_2=\mydiag(\lambda_{r+1},\ldots,\lambda_p)$.
Then $\Sigma=U_1\Lambda_1 U_1^T+U_2\Lambda_2 U_2^T$.
Let
$$
\mu_{f}\overset{def}{=}
\myE (ZJC) =(\sqrt{n_1}\mu_1,\ldots,\sqrt{n_k}\mu_k) C.
$$




\begin{theorem}\label{thm1}
    Suppose $p/n\to \infty$.
    Assume Assumption~\eqref{assumpEigen} holds.
    Further more, assume
\begin{equation}
    \frac{\lambda_1^2 p}{\lambda_r^2 n^2}\to 0.
\end{equation}
    Then under local alternative
    \begin{equation}
        \frac{1}{\sqrt{p}}\|\mu_f\|_F^2=O(1),
    \end{equation}
    we have
    \begin{equation}
        (\mytr \Lambda_2^2)^{-1/2}\big( C^TJ^T Z^T(I_p-H_{Z\tilde J}) ZJC-(\mytr \Lambda_2) I_{k-1} -\mu_f^T(I_p-H_{Z\tilde Z})\mu_f\big)\xrightarrow{\mathcal{L}} W_{k-1},
    \end{equation}
where $W_{k-1}$ is a $(k-1)\times(k-1)$ symmetric random matrix whose entries above the main diagonal are i.i.d.\ $N(0,1)$ and the entries on the diagonal are i.i.d.\ $N(0,2)$.
\end{theorem}

\section{Simulation Results}

In this section, we evaluate the numerical performance of the new test. For comparison, we also carried out simulation for the test of Tony Cai and Yin Xia and the test of Schott. These tests are denoted respectively by NEW, CX and SC.

In the simulations, we set $k=3$.
Note that the new test is invariant under orthogonal transformation.
Without loss of generality, we only consider diagonal $\Sigma$.
We set $\Sigma=\mydiag(p,1,\ldots,1)$.
Define signal-to-noise ratio (SNR) as
$$
\textrm{SNR}=\frac{\|\mu_f\|_F^2}{\sqrt{\sum_{i=2}^{p}\lambda_i(\Sigma)^2}}.
$$
We use SNR to characterize the signal strength.
We consider two alternative hypotheses: the non-sparse alternative and the sparse alternative.
In the non-sparse case, we set $\mu_1=\kappa 1_p$, $\mu_2=-\kappa 1_p$ and $\mu_3=0_p$, where $\kappa$ is selected to make the SNR equal to the given value.
In the sparse case, we set $\mu_1=\kappa (1_{p/5}^T,0_{4p/5}^T)^T$, $\mu_2=\kappa (0_{p/5}^T, 1_{p/5}^T,0_{3p/5}^T)^T$ and $\mu_3=0_p$. Again, $\kappa$ is selected to make the SNR equal to the given value.

%$$
%SNR=\frac{\|\mu_f\|_F^2}{\sqrt{\mytr (\Sigma^2)}}
%$$


\begin{table}[!hbp]
    \caption{Empirical powers of tests under non-sparse alternative with $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=10$. Based on $1000$ replications.}
    \centering
    \begin{tabular}{*{10}{c}}
    \toprule
    \multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=50$}&\multicolumn{3}{c}{$p=75$}&\multicolumn{3}{c}{$p=100$} \\
        \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
    \midrule
0 & 0.035 & 0.048 & 0.052 & 0.057 & 0.052 & 0.057 & 0.053 & 0.048 & 0.045 \\ 
1 & 0.060 & 0.049 & 0.096 & 0.081 & 0.050 & 0.092 & 0.063 & 0.062 & 0.085 \\ 
2 & 0.100 & 0.058 & 0.140 & 0.073 & 0.045 & 0.169 & 0.086 & 0.055 & 0.171 \\ 
3 & 0.145 & 0.066 & 0.234 & 0.119 & 0.070 & 0.266 & 0.117 & 0.056 & 0.307 \\ 
4 & 0.126 & 0.064 & 0.317 & 0.121 & 0.059 & 0.380 & 0.122 & 0.061 & 0.402 \\ 
5 & 0.179 & 0.072 & 0.392 & 0.178 & 0.068 & 0.541 & 0.141 & 0.071 & 0.579 \\ 
6 & 0.198 & 0.070 & 0.513 & 0.189 & 0.071 & 0.639 & 0.143 & 0.066 & 0.717 \\ 
7 & 0.249 & 0.085 & 0.629 & 0.227 & 0.084 & 0.777 & 0.206 & 0.073 & 0.822 \\ 
8 & 0.268 & 0.092 & 0.685 & 0.252 & 0.084 & 0.822 & 0.217 & 0.078 & 0.894 \\ 
9 & 0.324 & 0.100 & 0.786 & 0.256 & 0.090 & 0.911 & 0.246 & 0.074 & 0.949 \\ 
10 & 0.342 & 0.115 & 0.828 & 0.303 & 0.097 & 0.937 & 0.270 & 0.075 & 0.973 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under non-sparse alternative with $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. Based on $1000$ replications.}
\centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.050 & 0.043 & 0.050 & 0.056 & 0.066 & 0.048 & 0.062 & 0.045 & 0.054 \\ 
1 & 0.069 & 0.048 & 0.063 & 0.046 & 0.052 & 0.091 & 0.068 & 0.048 & 0.095 \\ 
2 & 0.097 & 0.046 & 0.131 & 0.086 & 0.053 & 0.164 & 0.068 & 0.057 & 0.173 \\ 
3 & 0.113 & 0.061 & 0.200 & 0.117 & 0.057 & 0.270 & 0.101 & 0.045 & 0.313 \\ 
4 & 0.135 & 0.053 & 0.247 & 0.130 & 0.054 & 0.402 & 0.118 & 0.066 & 0.485 \\ 
5 & 0.158 & 0.065 & 0.357 & 0.134 & 0.066 & 0.526 & 0.134 & 0.073 & 0.616 \\ 
6 & 0.198 & 0.081 & 0.433 & 0.161 & 0.052 & 0.668 & 0.138 & 0.067 & 0.765 \\ 
7 & 0.217 & 0.068 & 0.514 & 0.191 & 0.067 & 0.759 & 0.174 & 0.068 & 0.862 \\ 
8 & 0.229 & 0.063 & 0.582 & 0.223 & 0.075 & 0.853 & 0.187 & 0.060 & 0.927 \\ 
9 & 0.264 & 0.094 & 0.680 & 0.218 & 0.080 & 0.918 & 0.227 & 0.067 & 0.966 \\ 
10 & 0.298 & 0.091 & 0.758 & 0.245 & 0.076 & 0.934 & 0.228 & 0.052 & 0.982 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under sparse alternative with $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=10$. Based on $1000$ replications.}
\centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=50$}&\multicolumn{3}{c}{$p=75$}&\multicolumn{3}{c}{$p=100$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.063 & 0.056 & 0.052 & 0.048 & 0.049 & 0.048 & 0.057 & 0.047 & 0.042 \\ 
1 & 0.087 & 0.058 & 0.071 & 0.069 & 0.044 & 0.096 & 0.076 & 0.051 & 0.080 \\ 
2 & 0.091 & 0.066 & 0.116 & 0.113 & 0.037 & 0.133 & 0.080 & 0.058 & 0.139 \\ 
3 & 0.155 & 0.065 & 0.177 & 0.131 & 0.062 & 0.228 & 0.113 & 0.058 & 0.218 \\ 
4 & 0.184 & 0.065 & 0.246 & 0.174 & 0.076 & 0.308 & 0.144 & 0.061 & 0.310 \\ 
5 & 0.225 & 0.081 & 0.337 & 0.214 & 0.075 & 0.386 & 0.176 & 0.083 & 0.417 \\ 
6 & 0.270 & 0.088 & 0.425 & 0.266 & 0.085 & 0.507 & 0.228 & 0.071 & 0.508 \\ 
7 & 0.364 & 0.080 & 0.501 & 0.307 & 0.078 & 0.571 & 0.302 & 0.087 & 0.629 \\ 
8 & 0.405 & 0.105 & 0.549 & 0.381 & 0.080 & 0.698 & 0.362 & 0.089 & 0.721 \\ 
9 & 0.470 & 0.121 & 0.634 & 0.408 & 0.078 & 0.774 & 0.391 & 0.070 & 0.797 \\ 
10 & 0.547 & 0.128 & 0.702 & 0.484 & 0.109 & 0.819 & 0.415 & 0.088 & 0.877 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under sparse alternative with $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. Based on $1000$ replications.}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.048 & 0.045 & 0.046 & 0.053 & 0.046 & 0.043 & 0.051 & 0.034 & 0.046 \\ 
1 & 0.079 & 0.055 & 0.082 & 0.066 & 0.063 & 0.079 & 0.063 & 0.059 & 0.100 \\ 
2 & 0.097 & 0.054 & 0.119 & 0.088 & 0.055 & 0.138 & 0.085 & 0.055 & 0.160 \\ 
3 & 0.133 & 0.069 & 0.167 & 0.113 & 0.066 & 0.223 & 0.114 & 0.054 & 0.235 \\ 
4 & 0.149 & 0.062 & 0.212 & 0.126 & 0.084 & 0.298 & 0.132 & 0.057 & 0.344 \\ 
5 & 0.204 & 0.060 & 0.281 & 0.169 & 0.066 & 0.427 & 0.154 & 0.057 & 0.469 \\ 
6 & 0.252 & 0.060 & 0.352 & 0.227 & 0.070 & 0.548 & 0.195 & 0.072 & 0.641 \\ 
7 & 0.310 & 0.072 & 0.429 & 0.252 & 0.059 & 0.614 & 0.220 & 0.061 & 0.711 \\ 
8 & 0.372 & 0.088 & 0.529 & 0.314 & 0.085 & 0.719 & 0.297 & 0.060 & 0.800 \\ 
9 & 0.427 & 0.083 & 0.547 & 0.362 & 0.085 & 0.794 & 0.300 & 0.057 & 0.881 \\ 
10 & 0.449 & 0.093 & 0.619 & 0.396 & 0.072 & 0.853 & 0.340 & 0.076 & 0.911 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under non-sparse alternative with $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. The diagonal elements of $\Sigma$ are generated from sort(Unif(1,100)). Based on $1000$ replications.}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.063 & 0.054 & 0.058 & 0.052 & 0.040 & 0.042 & 0.045 & 0.049 & 0.070 \\ 
1 & 0.141 & 0.120 & 0.115 & 0.126 & 0.120 & 0.112 & 0.103 & 0.110 & 0.102 \\ 
2 & 0.181 & 0.209 & 0.169 & 0.330 & 0.260 & 0.210 & 0.200 & 0.227 & 0.201 \\ 
3 & 0.692 & 0.367 & 0.244 & 0.759 & 0.385 & 0.341 & 0.468 & 0.413 & 0.394 \\ 
4 & 0.753 & 0.539 & 0.420 & 0.744 & 0.573 & 0.515 & 0.516 & 0.554 & 0.561 \\ 
5 & 0.828 & 0.690 & 0.509 & 0.871 & 0.697 & 0.693 & 0.556 & 0.724 & 0.727 \\ 
6 & 0.809 & 0.812 & 0.622 & 0.822 & 0.824 & 0.766 & 0.959 & 0.838 & 0.859 \\ 
7 & 1.000 & 0.882 & 0.780 & 0.979 & 0.916 & 0.903 & 0.990 & 0.923 & 0.947 \\ 
8 & 0.993 & 0.955 & 0.789 & 1.000 & 0.965 & 0.954 & 0.999 & 0.972 & 0.971 \\ 
9 & 1.000 & 0.979 & 0.911 & 0.999 & 0.981 & 0.979 & 0.964 & 0.986 & 0.987 \\ 
10 & 1.000 & 0.991 & 0.877 & 0.989 & 0.996 & 0.988 & 0.996 & 0.996 & 0.997 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under sparse alternative with $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. The diagonal elements of $\Sigma$ are generated from sort(Unif(1,100)). Based on $1000$ replications.}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.052 & 0.055 & 0.047 & 0.055 & 0.057 & 0.053 & 0.044 & 0.055 & 0.057 \\ 
1 & 0.068 & 0.124 & 0.065 & 0.070 & 0.130 & 0.085 & 0.049 & 0.116 & 0.087 \\ 
2 & 0.085 & 0.233 & 0.112 & 0.076 & 0.239 & 0.149 & 0.067 & 0.241 & 0.161 \\ 
3 & 0.110 & 0.388 & 0.161 & 0.090 & 0.408 & 0.215 & 0.097 & 0.417 & 0.227 \\ 
4 & 0.120 & 0.530 & 0.184 & 0.112 & 0.552 & 0.282 & 0.103 & 0.556 & 0.309 \\ 
5 & 0.167 & 0.708 & 0.238 & 0.142 & 0.699 & 0.387 & 0.140 & 0.687 & 0.394 \\ 
6 & 0.196 & 0.807 & 0.261 & 0.168 & 0.820 & 0.472 & 0.162 & 0.823 & 0.547 \\ 
7 & 0.217 & 0.875 & 0.318 & 0.177 & 0.892 & 0.505 & 0.173 & 0.896 & 0.646 \\ 
8 & 0.234 & 0.935 & 0.378 & 0.220 & 0.951 & 0.625 & 0.195 & 0.948 & 0.749 \\ 
9 & 0.312 & 0.965 & 0.407 & 0.222 & 0.970 & 0.672 & 0.224 & 0.979 & 0.809 \\ 
10 & 0.334 & 0.976 & 0.505 & 0.292 & 0.987 & 0.773 & 0.254 & 0.989 & 0.881 \\ 
\bottomrule
\end{tabular}
\end{table}



\section{Appendix}

\begin{proposition}\label{optProp}
    Suppose $A$ is a $p\times r$ matrix with rank $r$ and $B$ is a $p\times p$  non-zero semi-definite matrix.
    Denote by $A=U_A D_A V_A^T$ the singular value decomposition of $A$, where $U_A$ and $V_A$ are $p\times r$ and $r\times r$ column orthogonal matrix, $D_A$ is a $r\times r$ diagonal matrix.
    Let $H_A=U_A U_A^T$ be the projection on the column space of $A$.
    Then
    \begin{equation}
        \max_{a^T a=1, a^T A A^T a=0}a^T B a=
        \lambda_{\max}\big(B(I_p-H_A)\big).
    \end{equation}
\end{proposition}
\begin{proof}
    Note that $a^T A A^T a=0$ is equivalent to $H_A a=0$ which in turn is equivalent to $a= (I_p-H_A)a$.
    Then
    \begin{equation}\label{eq:prop1eq1}
        \begin{aligned}
        \max_{a^T a=1, a^T A A^T a=0}a^T B a
            &=
        \max_{a^T a=1, H_A a=0}a^T(I_p-H_A) B (I_p-H_A)a,
        \end{aligned}
    \end{equation}
    which is obviously no greater than $\lambda_{\max}\big((I-H_A)B(I-H_A)\big)$.
    To prove that they are equal,  without loss of generality, we can assume $\lambda_{\max}\big((I-H_A)B(I-H_A)\big)>0$.
    Let $\alpha_1$ be one eigenvector corresponding to the largest eigenvalue of $(I-H_A)B(I-H_A)$.
    Since $(I-H_A)B(I-H_A)H_A=(I-H_A)B(H_A-H_A)=O_{p\times p}$ and $H_A$ is symmetric, the rows of $H_A$ are eigenvetors of $(I-H_A)B(I-H_A)$ corresponding to eigenvalue $0$.
    It follows that $H_A\alpha_1=0$.
    Therefore, $\alpha_1$ satisfies the constraint of~\eqref{eq:prop1eq1} and~\eqref{eq:prop1eq1} is no less than $\lambda_{\max}\big((I-H_A)B(I-H_A)\big)$.
    The conclusion now follows by noting that $\lambda_{\max}\big((I-H_A)B(I-H_A)\big)=\lambda_{\max}\big( B(I-H_A)\big)$.
    
\end{proof}

\begin{proof}[\textrm{Proof of Theorem~\ref{thm1}}]
It can be seen that $ZJC$ is independent of ${Z\tilde{J}}$.
Since
$
\myE (Z\tilde{J}) = O_{p\times (n-k)}
$,
we can write
$
Z\tilde{J} = U\Lambda^{1/2} G_1
$,
where $G_1$ is a $p\times (n-k)$ matrix with i.i.d.\ $N(0,1)$ entries.
We write
$
ZJC = \mu_f + U\Lambda^{1/2} G_2
$, 
where $G_2$ is a $p\times (k-1)$ matrix with i.i.d. $N(0,1)$ entries.

Then 
\begin{equation}\label{eq:maindec}
\begin{aligned}
C^TJ^T Z^T(I_p-H_{Z\tilde J}) ZJC
=&
G_2^T \Lambda^{1/2}U^T (I_P-H_{Z\tilde{J}})U\Lambda_{1/2}G_2+
\mu_f^T (I_p -H_{Z\tilde{J}})\mu_f+\\
&\mu_f^T (I_p -H_{Z\tilde{J}})U\Lambda^{1/2}G_2+
G_2^T \Lambda^{1/2}U^T (I_P-H_{Z\tilde{J}})\mu_f.
\end{aligned}
\end{equation}
To deal the first term, we note that
$$
G_2^T \Lambda^{1/2}U^T (I_p-H_{Z\tilde{J}})U\Lambda_{1/2}G_2\sim
\sum_{i=1}^p \lambda_i (\Lambda^{1/2}U^T (I_p-H_{Z\tilde{J}})U\Lambda^{1/2})\xi_i \xi_i^T,
$$
where $\xi_i\overset{i.i.d.}{\sim} N(0,I_{k-1})$. The key to its asymptotic behavior is the positive eigenvalues of $\Lambda^{1/2}U^T (I_p-H_{Z\tilde{J}})U\Lambda^{1/2}$, which in turn equal to the eigenvalues of $(I_p-H_{Z\tilde{J}})U\Lambda U^T (I_p-H_{Z\tilde{J}})$.
Write $(I_p-H_{Z\tilde{J}})U\Lambda U^T (I_p-H_{Z\tilde{J}})$ as the sum of two terms
$$
\begin{aligned}
&(I_p-H_{Z\tilde{J}})U\Lambda U^T (I_p-H_{Z\tilde{J}})
\\
=&
(I_p-H_{Z\tilde{J}})U_1\Lambda_1 U_1^T(I_p-H_{Z\tilde{J}})+(I_p-H_{Z\tilde{J}})U_2\Lambda_2 U_2^T (I_p-H_{Z\tilde{J}})
\overset{def}{=}R_1+R_2.
\end{aligned}
$$

Note that
$$
\begin{aligned}
&\lambda_{\max}\big( R_1 \big)
=
\lambda_{\max}\big(\Lambda_1^{1/2} U_1^T(I_p-H_{Z\tilde{J}}) U_1 \Lambda_1^{1/2}\big)
\leq 
\lambda_{\max}\big(\Lambda_1^{1/2} U_1^T(I_p-U_{Z\tilde{J}[,1:r]}U_{Z\tilde{J}[,1:r]}^T) U_1 \Lambda_1^{1/2}\big)\\
\leq &
\lambda_1
\lambda_{\max}\big(U_1^T(I_p-U_{Z\tilde{J}[,1:r]}U_{Z\tilde{J}[,1:r]}^T) U_1 \big)
= 
\lambda_1
\lambda_{\max}\big(I_r - U_1^TU_{Z\tilde{J}[,1:r]}U_{Z\tilde{J}[,1:r]}^T U_1 \big).
\end{aligned}
$$

To investigate the behavior of $U_{Z\tilde{J}}$, we need to investigate the behavior of $D_{Z\tilde{J}}$ first.
Note that 
$
G_1^T \Lambda G_1 = \tilde{J}^T Z^T Z\tilde{J} = V_{Z\tilde{J}} D_{Z\tilde{J}}^2 V_{Z\tilde{J}}^T
$, and 
$
G_1^T \Lambda G_1=
G_{1[1:r,]}^T \Lambda_1 G_{1[1:r,]}+
G_{1[(r+1):p,]}^T \Lambda_2 G_{1[(r+1):p,]}
$. We have
$$
V_{Z\tilde{J}} D_{Z\tilde{J}}^2 V_{Z\tilde{J}}^T=
G_{1[1:r,]}^T \Lambda_1 G_{1[1:r,]}+
G_{1[(r+1):p,]}^T \Lambda_2 G_{1[(r+1):p,]}.
$$
For $i=1,\ldots, r$,
\begin{equation}\label{eq:DLower}
\begin{aligned}
&\lambda_i(G_{1[1:r,]}^T \Lambda_1 G_{1[1:r,]})
\geq
\lambda_i(G_{1[1:r,]}^T \mydiag(\lambda_i I_{i},O_{(r-i)\times(r-i)}) G_{1[1:r,]})
\\
= &
\lambda_i \lambda_i(G_{1[1:i,]}G_{1[1:i,]}^T)=\lambda_i n(1+o_P(1)),
\end{aligned}
\end{equation}
where the last equality holds since $n^{-1}G_{1[1:i,]}G_{1[1:i,]}^T\xrightarrow{P}I_i$ by law of large numbers.
On the other hand, for $i=1,\ldots, r$,
\begin{equation}\label{eq:DUpper}
\begin{aligned}
&\lambda_i(G_{1[1:r,]}^T \Lambda_1 G_{1[1:r,]})
\\
=&\lambda_i\Big(
G_{1[1:r,]}^T \big(
\mydiag(\lambda_1,\ldots,\lambda_{i-1},O_{(r-i+1)\times(r-i+1)})+
\mydiag(O_{(i-1)\times(i-1)},\lambda_i,\ldots,\lambda_r)
\big)
G_{1[1:r,]}
\Big)\\
\leq&
\lambda_1(G_{1[1:r,]}^T \mydiag(O_{(i-1)\times(i-1)},\lambda_i,\ldots,\lambda_r) G_{1[1:r,]})
\leq
\lambda_1(G_{1[1:r,]}^T \mydiag(O_{(i-1)\times(i-1)},\lambda_i I_{r-i+1}) G_{1[1:r,]})
\\
= &
\lambda_i \lambda_1(G_{1[i:r,]}G_{1[i:r,]}^T)=\lambda_i n(1+o_P(1))
\end{aligned}
\end{equation}
where the first inequality holds by Weyl's inequality. It follows from~\eqref{eq:DLower} and~\eqref{eq:DUpper} that 
$\lambda_i(G_{1[1:r,]}^T \Lambda_1 G_{1[1:r,]})=\lambda_i n(1+o_P(1))$ for $i=1,\ldots, r$.

Note that
$\lambda_{\max}(G_{1[(r+1):p,]}^T \Lambda_2 G_{1[(r+1):p,]})\leq C\lambda_{\max}(G_{1[(r+1):p,]}^T G_{1[(r+1):p,]})=O_P(p)$ by Bai-Yin's law.
By assumption $\lambda_r n/p\to \infty$, we can deduce that $D_{Z\tilde{J}[i,i]}^2=\lambda_i(G_1^T \Lambda G_1)=\lambda_i n(1+o_P(1))$, $i=1,\ldots, r$.

Now we are ready to investigate the behavior of $U_{Z\tilde{J}}$.
Since
$
U\Lambda^{1/2} G_1 G_1^T \Lambda^{1/2} U^T 
=U_{Z\tilde{J}}D_{Z\tilde{J}}^2 U_{Z\tilde{J}}^T
$,
we have
$
G_1 G_1^T  
=\Lambda^{-1/2} U^T U_{Z\tilde{J}}D_{Z\tilde{J}}^2 U_{Z\tilde{J}}^TU\Lambda^{-1/2}
$, which further indicates
$$
\begin{aligned}
&G_{1[(r+1):p,]} G_{1[(r+1):p,]}^T  
=\Lambda_{2}^{-1/2} U_{[,(r+1):p]}^T U_{Z\tilde{J}}D_{Z\tilde{J}}^2 U_{Z\tilde{J}}^T U_{[,(r+1):p]}\Lambda_{2}^{-1/2}\\
\geq&
\Lambda_{2}^{-1/2} U_{[,(r+1):p]}^T U_{Z\tilde{J}[,1:r]}D_{Z\tilde{J}[1:r,1:r]}^2 U_{Z\tilde{J}[,1:r]}^T U_{[,(r+1):p]}\Lambda_{2}^{-1/2}\\
\geq&
D_{Z\tilde{J}[r,r]}^2
\Lambda_{2}^{-1/2} U_{[,(r+1):p]}^T U_{Z\tilde{J}[,1:r]} U_{Z\tilde{J}[,1:r]}^T U_{[,(r+1):p]}\Lambda_{2}^{-1/2}.
\end{aligned}
$$
Thus,
$$
\lambda_{\max}(U_{[,(r+1):p]}^T U_{Z\tilde{J}[,1:r]} U_{Z\tilde{J}[,1:r]}^T U_{[,(r+1):p]})\leq 
\frac{C}{D^2_{Z\tilde{J}[r,r]}} \lambda_{1}
(G_{1[(r+1):p,]} G_{1[(r+1):p,]}^T)
=O_P(\frac{p}{\lambda_r n}).
$$

Note that we have the simple relationship
$$
\begin{aligned}
&\lambda_{\max}(U_{[,(r+1):p]}^T U_{Z\tilde{J}[,1:r]} U_{Z\tilde{J}[,1:r]}^T U_{[,(r+1):p]})
=
\lambda_{\max}( U_{Z\tilde{J}[,1:r]}^T U_{[,(r+1):p]}U_{[,(r+1):p]}^T U_{Z\tilde{J}[,1:r]})\\
=&
\lambda_{\max}( U_{Z\tilde{J}[,1:r]}^T (I_p- U_1 U_1^T) U_{Z\tilde{J}[,1:r]})=
\lambda_{\max}(I_r- U_{Z\tilde{J}[,1:r]}^T  U_1 U_1^T U_{Z\tilde{J}[,1:r]})\\
=&
1-\lambda_{\min}( U_{Z\tilde{J}[,1:r]}^T  U_1 U_1^T U_{Z\tilde{J}[,1:r]})
=
1-\lambda_{\min}(U_1^T U_{Z\tilde{J}[,1:r]}U_{Z\tilde{J}[,1:r]}^T U_1)\\
=&
\lambda_{\max}(I_r-U_1^T U_{Z\tilde{J}[,1:r]}U_{Z\tilde{J}[,1:r]}^T U_1).
\end{aligned}
$$
Therefore 
$
\lambda_{\max}(I_r-U_1^T U_{Z\tilde{J}[,1:r]}U_{Z\tilde{J}[,1:r]}^T U_1)
=O_P(\frac{ p}{\lambda_r n})
$, and we can conclude
$\lambda_{\max}(R_1)=O_P(\frac{\lambda_1 p}{\lambda_r n})$.

We now deal with $R_1+R_2$.
For $i=1,\ldots, r$,
$$
\lambda_i(R_1+R_2)\leq
\lambda_1(R_1+R_2)\leq \lambda_1(R_1)+\lambda_1(R_2)\leq O_P(\frac{\lambda_1 p}{\lambda_r n}) + C.
$$
For $i=r+1,\ldots, p-r$,
$$
\begin{aligned}
&\lambda_i(R_1+R_2)\leq \lambda_{i-r}(R_2)
=
 \lambda_{i-r}\big( \Lambda_2^{1/2} U_2^T (I_p-H_{Z\tilde{J}})U_2\Lambda_2^{1/2}\big)
\leq
\lambda_{i-r}(\Lambda_2)
=\lambda_i.
\end{aligned}
$$
On the other hand,
for $i=1,\ldots, p-r-n+k$,
$$
\begin{aligned}
&\lambda_i(R_1+R_2)\geq \lambda_i(R_2)
=
 \lambda_i\big( \Lambda_2^{1/2} U_2^T (I_p-H_{Z\tilde{J}})U_2\Lambda_2^{1/2}\big)\\
=&
\lambda_i\big( \Lambda_2- \Lambda_2^{1/2} U_2^T H_{Z\tilde{J}}U_2 \Lambda_2^{1/2}\big)
\geq 
\lambda_{i+n-k}.
\end{aligned}
$$
The last equality holds since $U_2^T H_{Z\tilde{J}}U_2$ is at most of rank $n-k$.

As a consequence of these bounds, we have
$$
\sum_{i=1}^{p-r-n+k}\lambda_{i+n-k}^2\leq \mytr [(R_1+R_2)^2]\leq  r(O_P(\frac{\lambda_1 p}{\lambda_r n})+C)^2+\sum_{i=r+1}^{p-r}\lambda_i^2,
$$
or
$$
| \mytr [(R_1+R_2)^2]-\sum_{i=r+1}^{p}\lambda_{i}^2|\leq 
\sum_{i=r+1}^{n-k}\lambda_{i}^2+
\sum_{i=p-r+1}^{p}\lambda_{i}^2
+
r(O_P(\frac{\lambda_1 p}{\lambda_r n})+C)^2.
$$
Similarly,
$$
| \mytr [(R_1+R_2)]-\sum_{i=r+1}^{p}\lambda_{i}|\leq 
\sum_{i=r+1}^{n-k}\lambda_{i}+
\sum_{i=p-r+1}^{p}\lambda_{i}
+
r(O_P(\frac{\lambda_1 p}{\lambda_r n})+C).
$$
These, conbined with the assumptions, yield
$$
 \mytr [(R_1+R_2)^2]=(1+o_P(1))\sum_{i=r+1}^{p}\lambda_{i}^2,
$$
and
$$
 \mytr [(R_1+R_2)]=\sum_{i=r+1}^{p}\lambda_{i}+O(n)+O_P(\frac{\lambda_1 p}{\lambda_r n}).
$$

Now we have the Lyapunov condition
$$
\frac{\lambda_1[(R_1+R_2)^2]}{
\mytr [(R_1+R_2)^2]
}
%\leq
%\frac{
%\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
%}{\sum_{i=1}^{p-r-n+k}\lambda_{i+n-k}^2}
%\leq
%\frac{
%\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
%}{c(p-r-n+k)}
=
\frac{
\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
}{
(1+o_P(1))\sum_{i=r+1}^{p}\lambda_{i}^2
}
\xrightarrow{P} 0.
$$
Apply Lyapunov central limit theorem conditioning on $H_{Z\tilde{J}}$, we have
$$
\begin{aligned}
\big(\mytr[(R_1+R_2)^2]\big)^{-1/2}
\big( G_2^T \Lambda^{1/2}U^T (I_p-H_{Z\tilde{J}})U\Lambda_{1/2}G_2-\mytr(R_1+R_2) I_{k-1} \big)
\xrightarrow{\mathcal{L}} W_{k-1}
\end{aligned}
$$
where $W_{k-1}$ is a $(k-1)\times(k-1)$ symmetric random matrix whose entries above the main diagonal are i.i.d.\ $N(0,1)$ and the entries on the diagonal are i.i.d.\ $N(0,2)$.
By Slutsky's theorem, we have
$$
\begin{aligned}
\big(\sum_{i=r+1}^p \lambda_i^2\big)^{-1/2}
\big( G_2^T \Lambda^{1/2}U^T (I_p-H_{Z\tilde{J}})U\Lambda_{1/2}G_2-(\sum_{i=r+1}^p \lambda_i)I_{k-1} \big)
\xrightarrow{\mathcal{L}}W_{k-1}
\end{aligned}
$$


As for the cross term of~\eqref{eq:maindec}, we have
$$
\begin{aligned}
    &\myE [\|\mu_f^T (I_p -H_{Z\tilde{J}})U\Lambda^{1/2}G_2\|_F^2|Z\tilde{J}]\\
    = &
    (k-1)\mytr(\mu_f^T (I_p -H_{Z\tilde{J}})U\Lambda U^T (I_p -H_{Z\tilde{J}})\mu_f)\\
    \leq &
    (k-1)\lambda_1\big((I_p -H_{Z\tilde{J}})U\Lambda U^T (I_p -H_{Z\tilde{J}})\big)\|\mu_f\|^2_F\\
    = &
    (k-1) O_P(\frac{\lambda_1 p}{\lambda_r n})  \|\mu_f\|^2_F\\
    = &
    (k-1) O_P(\frac{\lambda_1 \sqrt{p}}{\lambda_r n}) \sqrt{p}  \|\mu_f\|^2_F=o_P(p)
\end{aligned}
$$
The last equality holds when we assume $\frac{1}{\sqrt{p}}\|\mu_f\|_F^2=O(1)$. Hence $\|\mu_f^T (I_p -H_{Z\tilde{J}})U\Lambda^{1/2}G_2\|_F^2=o_P(p)$.
This completes the proof of the theorem.

\end{proof}






\section*{References}

\bibliography{mybibfile}

\end{document}
