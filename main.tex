\documentclass[12pt]{article} %***
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
\usepackage[]{hyperref}  %<----modified by Ivan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{sectsty, secdot}
%\sectionfont{\fontsize{12}{15}\selectfont}
\sectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
\renewcommand{\theequation}{\thesection\arabic{equation}}
\subsectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth=31.9pc
\textheight=46.5pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
%\headheight=.2cm
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}

\usepackage{bm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{IEEEtrantools}
\usepackage{enumerate}



\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}

\newcommand{\ud}{\mathbf{d}}






\newtheorem{assumption}{Assumption}

\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{}
%\headrulewidth=0pt  %<-modified by Ivan

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{2}

\markright{ \hbox{\footnotesize\rm Statistica Sinica
%{\footnotesize\bf 24} (201?), 000-000
}\hfill\\[-13pt]
\hbox{\footnotesize\rm
%\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
}\hfill }

\markboth{\hfill{\footnotesize\rm Rui Wang AND Xingzhong Xu} \hfill}
{\hfill {\footnotesize\rm GENERALIZED LIKELIHOOD RATIO TEST} \hfill}

\renewcommand{\thefootnote}{}
$\ $\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont \vspace{0.8pc}
\centerline{\large\bf A GENERALIZED LIKELIHOOD RATIO TEST FOR }
\vspace{2pt} \centerline{\large\bf MULTIVARIATE ANALYSIS OF VARIANCE}
\vspace{2pt} \centerline{\large\bf IN HIGH DIMENSION}
\vspace{.4cm} \centerline{Author(s)} \vspace{.4cm} \centerline{\it
Affiliation(s)} \vspace{.55cm} \fontsize{9}{11.5pt plus.8pt minus
.6pt}\selectfont

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{quotation}
\noindent {\it Abstract:}
This paper considers in high dimensional setting a canonical testing problem, namely testing the equality of multiple mean vectors of normal distribution.
    Motivated by Roy's union-intersection principal, we propose a generalized likelihood ratio test.
    The critical value is determined by permutation method.
    We introduce an algorthm for permuting procedure, whose complexity does not depend on data dimension.
    The limiting distribution of the test statistic is derived in two different setting: non-spiked covariance and spiked covariance.
    Theoretical results and simulation studies show that the test is particularly powerful under spiked covariance.

\vspace{9pt}
\noindent {\it Key words and phrases:}
%Balanced incomplete block design, Hadamard matrix, nearly balanced incomplete block design, orthogonal array.
\par
\end{quotation}\par



\def\thefigure{\arabic{figure}}
\def\thetable{\arabic{table}}

\renewcommand{\theequation}{\thesection.\arabic{equation}}


\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont

\setcounter{section}{1} %***
\setcounter{equation}{0} %-1
\noindent {\bf 1. Introduction}
Suppose there are $k$ ($k\geq 2$) groups of $p$ dimensional data.
Within the $i$th group ($1\leq i\leq k$), we have observations $\{X_{ij}\}_{j=1}^{n_i}$ which are independent and identically distributed (i.i.d.) as $N_p(\xi_i,\Sigma)$, the $p$ dimensional normal distribution with mean vector $\xi_i$ and variance matrix $\Sigma$.
%Suppose there are $k$ normal populations with possibly different means $\xi_1,\ldots,\xi_k$, but all with the same variance $\Sigma$.
%Suppose we observe $k$ independent random samples, each from the distribution $N_p(\xi_i,\Sigma)$, where $1\leq i\leq k$, $k\geq 2$ is a fixed constant, $\xi_i$ and $\Sigma$ are unknown parameters.
%Denote by $X_{ij}\in \mathbb{R}^p$ the $j$th observation in group $i$, $j=1,\ldots,n_i$, $i=1,\ldots, k$,  where $n_i$ is the samples size of group $i$, $1\leq i \leq k$.
We would like to test the hypotheses
\begin{equation}\label{hypothesis}
    H_0: \xi_1=\xi_2=\cdots=\xi_k\quad \text{v.s.}\quadã€€H_1: \text{$\xi_i\neq \xi_j$ for some $i\neq j$}.
\end{equation}
This testing problem is known as one-way multivariate analysis of variance (MANOVA) and has been well studied when $p$ is small compared to $n$, where $n=\sum_{i=1}^k n_i$ is the total sample size.

Let $\bH=\sum_{i=1}^k n_i (\bar{\bX}_i-\bar{\bX})(\bar{\bX}_i-\bar{\bX})^T$ be the sum-of-squares between groups and $\bG=\sum_{i=1}^k \sum_{j=1}^{n_i}(X_{ij}-\bar{\bX}_i)(X_{ij}-\bar{\bX}_i)^T$ be the sum-of-squares within groups, where $\bar{\bX}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ is the sample mean of group $i$ and $\bar{\bX}=n^{-1}\sum_{i=1}^k\sum_{j=1}^{n_i}X_{ij}$ is the pooled sample mean.
   There are four classical test statistics for hypothesis~\eqref{hypothesis}, which are all based on the eigenvalues of $\bH\bG^{-1}$. 

   %\begin{IEEEeqnarray*}{CC}
       %\text{Wilks' Lambda:} \qquad& |G+H|/|G|\\
       %%\text{Pillai trace:} \qquad& \mytr[H(G+H)^{-1}]\\
       %\text{Hotelling-Lawley trace:}\qquad & \mytr[HG^{-1}]\\
       %\text{Roy's maximum root:} \qquad& \lambda_{\max}(HG^{-1})
   %\end{IEEEeqnarray*}

       \begin{center}
       \begin{tabular}{|cc|}
           \hline
       {Wilks' Lambda:} & $|\bG+\bH|/|\bG|$\\
       {Pillai trace:} & $\mytr[\bH(\bG+\bH)^{-1}]$\\
       {Hotelling-Lawley trace:} & $\mytr[\bH \bG^{-1}]$\\
       {Roy's maximum root:} & $\lambda_{\max}(\bH \bG^{-1})$\\
           \hline
           \end{tabular}
       \end{center}

%There are four classical tests for hypothesis~\eqref{hypothesis}: Wilks' Lambda (which is also the LRT), Hotelling-Lawley trace, Pillai Trace and Roy's maximum root.


In some modern scientific applications, people would like to test hypothesis~\eqref{hypothesis} in high dimensional setting, i.e., $p$ is greater than $n$.
See, for example,~\cite{Tsai2009}.
%\textcolor{red}{Some references}
%However, when $p>n-k$, the LRT for hypothesis~\eqref{hypothesis} is not well defined.
However, when $p\geq n$, the four classical test statistics can not be defined.
  Researchers have done extensive work to study the testing problem~\eqref{hypothesis} in high dimensional setting.
 So far, most tests in the literature are designed for two sample case, i.e. $k=2$.
  See, for example,~\citet{Bai1996Efiect,Chen2010A,Srivastava2009A,Tony2013,Feng2015Multivariate}.
  For multiple sample case,~\cite{Schott2007Some} modified Hotelling-Lawley trace and proposed the test statistic
  %$$
  %T_{SC}=\frac{1}{\sqrt{n-1}}\big(
  %\frac{1}{k-1}\mytr\big(\sum_{i=1}^k n_i\bar{X}_i\bar{X}_i^T-n\bar{X}\bar{X}^T\big)-\frac{1}{n-k}\mytr\big(\sum_{i=1}^k \sum_{j=1}^{n_i}X_{ij}X_{ij}^T-\sum_{i=1}^k n_i\bar{X}_i\bar{X}_i^T\big)
  %\big),
  %$$
  %where $\bar{\bX}_i=n_i^{-1}\sum_{j=1}^{n_i}X_{ij}$ and $\bar{\bX}=n^{-1}\sum_{i=1}^k\sum_{j=1}^{n_i}X_{ij}$.
  $$
  T_{SC}=\frac{1}{\sqrt{n-1}}\Big(
  \frac{1}{k-1}\mytr\big(\bH\big)-\frac{1}{n-k}\mytr\big(\bG\big)
  \Big).
  $$
In another work,~\cite{Cai2014High} proposed a test statistic
  $$
  T_{CX}=\max_{1\leq i\leq p} \sum_{1\leq j<l\leq k}\frac{n_j n_l}{n_j+n_l}\frac{(\Omega(\bar{\bX}_j-\bar{\bX}_l))_i^2}{\omega_{ii}},
  $$
  Where $\Omega=(\omega)_{ij}=\Sigma^{-1}$ is the precision matrix. When $\Omega$ is unknown, they substitute it by an estimator $\hat{\Omega}$.
  Stitistics $T_{SC}$ and $T_{CX}$ are the representatives of two popular methodologies for high dimensional tests.
  $T_{SC}$ is a so-called sum-of-squares type statistic as it is based on an estimation of squared Euclidean norm $\sum_{i=1}^k n_i\|\xi_i-\bar{\xi}\|^2$, where $\bar{\xi}=n^{-1}\sum_{i=1}^k n_i \xi_i$.
  $T_{CX}$ is an extreme value type statistic.
  
  %Suppose $\{X_{i1},\ldots, X_{in_i}\}$ are i.i.d.\ distributed as $N(\xi_i,\Sigma)$ for $1\leq i\leq K$.
%The $k$ samples are independent.
%%$\xi_i$, $i=1\ldots, k$ and $\Sigma>0$ are unknown. An interesting problem in multivariate analysis is to test the hypotheses
%\begin{equation}
    %H: \xi_1=\xi_2=\cdots=\xi_k\quad v.s.\quad K: \textrm{$\xi_i\neq \xi_j$ for some $i\neq j$}.
%\end{equation}
%The likelihood ratio test (LRT) has a dominated position in classical multivariate analysis.
   Note that both sum-of-squares type statistic and extreme value type statistic are not based on likelihood function.
    While the likelihood ration test (LRT), i.e., Wilks' Lambda, is not defined if $p>n-k$,
    It remains a problem how to construct likelihood-based tests in high dimensional setting.
    %Note that Roy's maximum root, one of the four classic test statistics, is derived by Roy's union intersection principle.
    In a recent work,~\cite{Zhao2016A} proposed a generalized likelihood ratio test in the context of one-sample mean vector test.
Inspired by Roy's union-intersection principle \citep{Roy1953},
    they wrote the null hypothesis as the intersection of a class of component hypotheses.
    For each component hypotheses, the likelihood ratio test is constructed.
    They use a least favorable argument to construct test statistic based on component tests.
    Their simulation results showed that their test has good power performance, especially when the variables are dependent.

    Following~\cite{Zhao2016A}'s methodology, we proposed a generalized likelihood ratio test for hypothesis~\eqref{hypothesis}.
    To understand the power behavior of the new test, we derive the asymptotic distribution of the new statistic under two different settings.
    In first setting, we assume the eigenvalues of $\Sigma$ are bounded.
    It's a common assumption in high dimensional statistics.
    In fact, most existing tests for hypothesis~\eqref{hypothesis} imposed conditions which prevent from large leading eigenvalues of $\Sigma$.
    However, when the correlations between variables are determined by a small number of factors, $\Sigma$ is spiked in the sense that a few leading eigenvalues are much larger than the others.
    See, for example~\cite{Cai2012Sparse} and~\cite{Shen2013Consistency}.
    %\textcolor{red}{References}
    We then derive the asymptotic distribution of the test statistic under spiked covariance.
    From the theoretical results we give, it can be seen that the new test is particularly powerful under spiked covariance.
    We also conduct a simulation study to examine the numerical performance of the test.

    The rest of the paper is organized  as follows.
    In Section 2, we propose a new test. Section 3 concerns the theoretical properties of the proposed test. In Section 4, the proposed test is compared with some existing tests. Section 5 complements our study with some numerical simulations. In Section 6, we give a short discussion. Finally, the proofs are gathered in the Appendix.
%\textcolor{red}{here}
    %Higher criticism CX are special case of UIT.



%\setcounter{section}{2} %***
%\setcounter{equation}{0} %-1
%\noindent {\bf 2. The Second Section}

 
\section{Methodology}\label{methodology}

 Let
 $$\bX=(X_{11},\ldots,X_{1n_1},\ldots,X_{k1},\ldots,X_{kn_k})$$
 be the pooled sample matrix.
 Define
 $$
 \bJ=\begin{pmatrix}
     \frac{1}{\sqrt{n_1}}\mathbf{1}_{n_1}&\mathbf{0} & \mathbf{0}\\
     \mathbf{0}&\frac{1}{\sqrt{n_2}} \mathbf{1}_{n_2}& \mathbf{0}\\
     \vdots &\vdots &\vdots \\
     \mathbf{0}&\mathbf{0}&\frac{1}{\sqrt{n_k}}\mathbf{1}_{n_k}
 \end{pmatrix}.
 $$
Then the matrices $\bI_n-\bJ\bJ^T$, $\bJ\bJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ and $\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ are three $n\times n$ projection matrices which are pairwise orthogonal with rank $n-k$, $k-1$ and $1$ respectively.
Let $\tilde{\bJ}$ be an $n\times (n-k)$ matrix satisfying $\tilde{\bJ}\tilde{\bJ}^T =\bI_n-\bJ\bJ^T$.
Let $\bY=\bX\tilde{\bJ}$.
Note that $\bI_k-\frac{1}{n}\bJ^T\mathbf{1}_n \mathbf{1}_n^T \bJ$ is a $k\times k$ projection matrix with rank $k-1$.
Let $\bC$ be a $k\times (k-1)$ matrix satisfying $\bC\bC^T=\bI_k-\frac{1}{n}\bJ^T\mathbf{1}_n \mathbf{1}_n^T \bJ$.
Then we have
$$
\bG=\bX(\bI_n-\bJ\bJ^T)\bX^T=
%Z\tilde{J}\tilde{J}^T Z^T
\bY \bY^T,
$$
and
\begin{equation*}
    \begin{aligned}
        \bH=\bX(\bJ\bJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)\bX^T
=\bX \bJ(\bI_k-\frac{1}{n}\bJ^T\mathbf{1}_n \mathbf{1}_n^T \bJ)\bJ^T \bX^T
=\bX \bJ\bC \bC^T \bJ^T \bX^T.
    \end{aligned}
\end{equation*}
 Define $\Xi=(\sqrt{n_1}\xi_1,\ldots,\sqrt{n_k}\xi_k)$
 and the null hypothesis $H_0$ is equivalent to $\Xi \bC=\bO_{p\times (k-1)}$, where $\bO_{p\times (k-1)}$ is a $p\times (k-1)$ matrix with all elements equal to $0$.
\subsection{Roy's maximum root}
%Roy's union intersection principle plays an important role in the methodology of~\cite{Zhao2016A}.
Roy's maximum root test statistic is derived in~\cite{Roy1953} as an application of his union intersection principle.
Roy's union intersection principle can be decomposed into $3$ main steps:
\begin{enumerate}
    \item
        Decompose the hypothesis $H_0$ and $H_1$ into component hypotheses
        $$
        H_0=\bigcap_{\gamma\in\Gamma} H_{0\gamma} \quad \text{v.s.} \quad 
        H_1=\bigcup_{\gamma\in \Gamma} H_{1\gamma},
        $$
        where $\Gamma$ is an index set.
    \item
        For each $\gamma$, construct a component test for $H_{0\gamma}$ against $H_{1\gamma}$.
    \item
        Accept $H_0$ if all component tests accept the null hypotheses.
        Or equivalently, reject $H_0$ if any component test reject the null hypothesis.
\end{enumerate}
Roy's union intersection principle is particularly useful when $H_0$ and $H_1$ themselves are complicated but can be decomposed into a class of simple hypotheses. %\textcolor{red}{Refereces}.

The decomposition in step 1 of union intersection principle is often induced by a data transformation.
The data matrix $\bX$ is not easy to deal with since it is multivariate. 
 Note that there is a one-to-one mapping between the data $\bX$ and the set $\{\bX_a: a\in\mathbb{R}^p, a^T a=1\}$, where $\bX_a=a^T \bX$ is the univariate data obtained by projecting $\bX$ on direction $a$.
 %For $a\in \mathbb{R}$ and $a^T a=1$, let $\bX_a=a^T \bX$ be the univariate data obtained by projecting $\bX$ on direction $a$.
 This naturally induces the decomposition  
 $$H_0=\bigcap_{a\in \mathbb{R}^p, a^T a=1}H_{0a} \quad\text{and}\quad K=\bigcup_{a\in \mathbb{R}^p,a^T a=1}H_{1a} ,$$
 where
%The decomposition in step $1$ is often induced by data transformation.
%For $a\in \mathbb{R}^p$ and $a^T a=1$, define the hypothesis $H_a$ and $K_a$ as
%$$
%H_a: a^T\xi_1=a^T\xi_2=\cdots=a^T\xi_k\quad \text{v.s.}\quad K_a: \text{$a^T\xi_i\neq a^T\xi_j$ for some $i\neq j$}.
%$$
 $$
 H_{0a}: a^T \Xi \bC = \bO_{1\times (k-1)}\quad \text{and}\quad H_{1a} : a^T \Xi \bC \neq \bO_{1\times (k-1)}.
 $$
%The union-intersection principle tells that $H$ is rejected if and only if any one of $H_a$ is rejected.
%Let $\bX_i=(X_{i1},\ldots,X_{in_i})$ be the $i$th sample, $i=1,\ldots,k$. Let $\bX=(\bX_1,\ldots,\bX_k)$ be the pooled sample.
Based on $\bX_a$, the likelihood ratio test statistic for $H_{0a}$ against $H_{1a}$ is
%The density function of $\bX_a$ is 
%$$
%f_a(\bX_a;\xi_1,\ldots,\xi_k,\Sigma)=
    %\frac{1}{(2\pi)^{n/2}|a^T \Sigma a|^{n/2}}
    %\exp\Big(-\frac{1}{2 a^T \Sigma a}\sum_{i=1}^k\sum_{j=1}^{n_i}(a^T X_{ij}-a^T\xi_i)^2\Big).
%$$
%Then the likelihood ratio test statistic for $H_a$ against $K_a$ is 
%Note that
%\begin{equation*}
    %\begin{aligned}
        %\max_{\xi_1,\ldots,\xi_k,\Sigma}f_a(a^T Z,\xi_1,\ldots,\xi_k,\Sigma)
        %=
        %(2\pi)^{-n/2}\big(\frac{1}{n} a^T G a\big)^{-n/2}e^{-{n}/{2}},
    %\end{aligned}
%\end{equation*}
%%Let $S_i=\sum_{j=1}^{n_i}(x_{ij}-\bar{\mathbf{X}}_i)(x_{ij}-\bar{\mathbf{X}}_i)^T$ and $S=\sum_{i=1}^k S_i$.
%\begin{equation*}
    %\begin{aligned}
        %\max_{\xi,\Sigma}f_a(a^T Z,\xi,\ldots,\xi,\Sigma)
        %=
        %(2\pi)^{-n/2}\big(\frac{1}{n}a^T (G+H)a\big)^{-n/2}e^{-{n}/{2}},
    %\end{aligned}
%\end{equation*}
\begin{equation*}
    \begin{aligned}
        \text{LR}_{a}%=\frac{\sup\limits_{\xi_1,\ldots,\xi_k,\Sigma}f_a(a^T \bX;\xi_1,\ldots,\xi_k,\Sigma)}{\sup\limits_{\xi,\Sigma}f_a(a^T \bX;\xi,\ldots,\xi,\Sigma)}
        =
        \Big(1+\frac{a^T \bH a}{a^T \bG a}\Big)^{n/2}.
    \end{aligned}
\end{equation*}
By Roy's union intersection principle, $H_0$ is rejected when $\max_{a^T a=1}\text{LR}_a$ is large.
If $p\leq n-k$, $\bG$ is invertible and
$\max_{a^T a=1}\text{LR}_a=(1+\lambda_{\max}(\bH\bG^{-1}))^{n/2}$, which is an increasing function of Roy's maximum root test statistic.


%%$$
%f(Z;\xi_1,\ldots,\xi_k,\Sigma)=\prod_{i=1}^k\Big[
    %(2\pi)^{-n_i p/2}|\Sigma|^{-n_i/2}\exp(-\frac{1}{2}\mathrm{tr}\Sigma^{-1}\sum_{j=1}^{n_i}(x_{ij}-\xi_i)(x_{ij}-\xi_i)^T)
    %\Big].
%$$


\subsection{A new test}
Despite the wide use of Roy's maximum root, it is not defined for $p> n-k$.
In fact, if $p>n-k$, $G$ is not invertible and $\max_{a^T a=1}\text{LR}_a=+\infty$. 

The derivation of Roy's maximum root implies that it is based on the likelihood ratio of projected data $\bX_a$.
From a likelihood point view, log likelihood ratio is an estimator of the KL divergence between the alternative distribution and the null distribution.
Thus, by maximizing $\text{LR}_a$, one obtains
the direction $a^*=\argmax_{a^T a=1}\text{LR}_a$ which hopefully distinct the null distribution and the alternative distribution of $\bX_a$. 


%Note that $\max_{a^T a=1}\text{LRT}_a=\text{LRT}_{a^*}$, where 
%$a^*=\argmax_{a^T a=1}\text{LRT}_a$
%is the direction achieving the maximum discrepancy between the null hypothesis and alternative hypothesis.
%We shall call $a^*$ the rejection direction.
%In the derivation of Roy's maximum root, we have
%$$
%\text{LRT}_a=\Big(1+\frac{a^T H a}{a^T G a}\Big)^{n/2}.
%$$
%In another viewpoint, union intersection principle finds an direction $a$ along which the evidence against null hypothesis is maximized.
%Such an $a$ is data dependent.
%In the classical setting, the evidence of direction $a$ is $\text{LRT}_a$.
%In the current context, there are a class of $a$ such that $\text{LRT}_a$ achieve the infinity, the largest evidence in classical sense.
%We need to further choose a single $a$ from $\{a\,|\,\text{LRT}_a=+\infty\text{ and }a^T a =1\}$.
 %From the expression of $\text{LRT}_a$, we would like to make the largest discrepancy between $a^T H a$ and $a^T G a$.
%Note that if $\text{LRT}_{\alpha}=+\infty$, then $a^T G a=0$.
%Hence it's natural to choose $a$ as
%$$
        %a^{*}=
        %\argmax_{a^T a=1, a^T G a=0} 
        %a^T H a.
%$$
%Since $a^{*T}G a^*=0$, we propose the following test statistic for $H$:


While it is hard to generalize Roy's maximum root to high dimensional setting, $a^*$ can be formally generalized to high dimensional setting.
%The essence of Roy's union intersection principle is to find a direction $a$ along which $\text{LRT}_a$ is maximized.
%Based on this idea, we have the following heuristic argument to propose a new test statistic.
Note that with probability $1$, we have $\{a:\text{LR}_a=+\infty\}=\{a:a^T \bG a=0\}$. When $p>n-k$, we have the following formal argument
$$
\begin{aligned}
    a^*&=\argmax_{a^T a=1} \text{LR}_{a}\\
    &=\argmax_{a^T a=1,\text{LR}_a=+\infty} \Big(1+\frac{a^T \bH a}{a^T \bG a}\Big)^{n/2}\\
    &=\argmax_{a^T a=1,a^T Ga=0} \Big(1+\frac{a^T \bH a}{0}\Big)^{n/2}\\
    &=\argmax_{a^T a=1,a^T Ga=0} {a^T \bH a}.
\end{aligned}
$$
This motivates us to propose the test statistic
\begin{equation*}
    \begin{aligned}
        T=a^{*T} \bH a^*
        =
        \max_{a^T a=1, a^T \bG a=0} 
        a^T \bH a.
    \end{aligned}
\end{equation*}
We reject the null hypothesis when $T$ is large enough.
%The above method is first proposed by~\cite{Zhao2016A} in the context of testing one sample mean vector.


Next we derive the explicit forms of the test statistic. 
%Let $J=\mathrm{diag}(n_1^{-1/2}\mathbf{1}_{n_1},\ldots,n_k^{-1/2}\mathbf{1}_{n_k})$.
%Then the matrices $I_n-JJ^T$, $JJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ and $\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$ are three $n\times n$ projection matrices which are pairwise orthogonal with rank $n-k$, $k-1$ and $1$.
%Let $\tilde{J}$ be a $n\times (n-k)$ matrix satisfying $\tilde{J}\tilde{J}^T =I-JJ^T$.
%Note that $I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J$ is a $k\times k$ projection matrix with rank $k-1$.
%Let $C$ be a $k\times (k-1)$ matrix satisfying $CC^T=I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J$.
%Then 
%$$
%G=Z(I_n-JJ^T)Z^T=
%Z\tilde{J}\tilde{J}^T Z^T.
%$$
%and
%\begin{equation*}
    %\begin{aligned}
        %F=\sum_{i=1}^k n_i (\bar{\mathbf{X}}_i-\bar{\mathbf{X}})(\bar{\mathbf{X}}_i-\bar{\mathbf{X}})^T 
        %=Z(JJ^T-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T)Z^T
%=ZJ(I_k-\frac{1}{n}J^T\mathbf{1}_n \mathbf{1}_n^T J)J^T Z^T
%=ZJC C^T J^T Z^T.
    %\end{aligned}
%\end{equation*}
Let $\bY=\bU_{\bY}\bD_{\bY}\bV_{\bY}^T$ be the singular value decomposition of $\bY$, where $\bU_{\bY}$ and $\bV_{\bY}$ are $p\times (n-k)$ and $(n-k)\times(n-k)$ both column orthogonal matrices, $\bD_{Y}$ is an $(n-k)\times (n-k)$ diagonal matrix.
Let $\bP_{\bY}=\bU_{\bY}\bU_{\bY}^T$ be the projection on the column space of $\bY$.
By Proposition~\ref{optProp}, we have
\begin{equation}\label{statisticForm1}
\begin{aligned}
    T(\bX)&=\lambda_{\max}\big(\bX \bJ\bC\bC^T\bJ^T\bX^T (\bI_p-
    \bP_{\bY})
    \big)
    =\lambda_{\max}\big(\bC^T\bJ^T\bX^T (\bI_p-
    \bP_{\bY}
    )\bX\bJ\bC\big).
\end{aligned}
\end{equation}

Next we introduce another form of $T$.
%where $\bP_A=Z\tilde{J}{(\tilde{J}^T Z^T Z\tilde{J})}^{-1}\tilde{J}^T Z^T$.
By the relationship
\begin{equation*}
    \begin{aligned}
        \begin{pmatrix}
            \bJ^T \bX^T \bX\bJ & \bJ^T \bX^T \bX\tilde{\bJ}\\
            \tilde{\bJ}^T \bX^T \bX \bJ & \tilde{\bJ}^T \bX^T \bX \tilde{\bJ}
        \end{pmatrix}^{-1}
        =
        \Big(
        \begin{pmatrix}
            \bJ^T\\
            \tilde{\bJ}^T
        \end{pmatrix}
        \bX^T \bX
        \begin{pmatrix}
            \bJ &\tilde{\bJ}
        \end{pmatrix}
        \Big)^{-1}
        =
        \begin{pmatrix}
            \bJ^T {(\bX^T \bX)}^{-1}\bJ & \bJ^T {(\bX^T \bX)}^{-1}\tilde{\bJ}\\
            \tilde{\bJ}^T {(\bX^T \bX)}^{-1}\bJ & \tilde{\bJ}^T {(\bX^T \bX)}^{-1} \tilde{\bJ}
        \end{pmatrix}
    \end{aligned}
\end{equation*}
and matrix inverse formula, we have that
\begin{equation*}
    \begin{aligned}
        &{\big( \bJ^T {(\bX^T \bX)}^{-1}\bJ \big)}^{-1}
        =\bJ^T \bX^T \bX \bJ - \bJ^T \bX^T \bX\tilde{\bJ}{(\tilde{\bJ}^T \bX^T \bX \tilde{\bJ})}^{-1}
            \tilde{\bJ}^T \bX^T \bX\bJ 
        = \bJ^T \bX^T( \bI_p- \bP_{\bY}) \bX \bJ.
    \end{aligned}
\end{equation*}
Thus, 
\begin{equation}\label{statisticForm2}
    \begin{aligned}
        T(\bX)=
        \lambda_{\max}\Big(\bC^T\big( \bJ^T (\bX^T \bX)^{-1}\bJ \big)^{-1}\bC\Big).
    \end{aligned}
\end{equation}

We will use~\eqref{statisticForm1} for theoretical analysis,~\eqref{statisticForm2} for computation.
\subsection{Permutation method}
Permutation method is a powerful tool to determine the critical value of a test statistic.
   The test procedure resulting from permutation method is exact as long as the null distribution of observations are exchangeable~\citep{Romano1990On}.
   The major down-side to permutation method is that it can be computationally intensive.
   Fortunately,  the permutation method can be computationally fast.
    By expression~\eqref{statisticForm2}, a permuted statistic can be written as
    \begin{equation}\label{permutedStatistic}
        T(\bX\Gamma)=\lambda_{\max}\Big(\bC^T{\big( \bJ^T \Gamma^T {(\bX^T \bX)}^{-1} \Gamma \bJ \big)}^{-1}  \bC\Big),
    \end{equation}
where $\Gamma$ is an $n\times n$ permutation matrix.
   Note that ${(\bX^T \bX)}^{-1}$, the most time-consuming component, can be calculated aforehand.
   The permutation procedure for our statistic can be summarized as:
   \begin{enumerate}
       \item
           Calculate $T(\bX)$ according to~\eqref{statisticForm2}, hold intermediate result ${(\bX^T \bX)}^{-1}$.
       \item For a large $M$, independently generate $M$ random permutation matrix $\Gamma_1,\ldots,\Gamma_M$ and calculate $T(\bX\Gamma_1),\ldots,T(\bX\Gamma_M)$ according to~\eqref{permutedStatistic}. 
       \item Calculate the $p$-value by
           $
           \tilde{p}={(M+1)}^{-1}\big[1+\sum_{i=1}^M I\{T(\bX\Gamma_i)\geq T(\bX)\}\big]
           $.
           Reject the null hypothesis if $\tilde{p}\leq \alpha$.
   \end{enumerate}

Here $M$ is the permutation times. 
   %It can be shown that for any integer $M>0$, the resulting test controls the Type I error. More precisely, we have $\Pr(\tilde{p}\leq u)\leq u$ for all $0\leq u\leq 1$.
   %Moreover, as $M$ tends to $\infty$, $\lim_{M\to \infty}\Pr(\tilde{p}\leq u)= u$.
   %See, for example,~\cite{Lehmann}, Chapter $15$.
   It can be seen that  step 1 and step 2 cost $O(n^2 p +n^3)$ and $O(n^2 M)$ operations respectively.
   In large sample or high dimensional setting, step 2 has negligible effect on total computational complexity.




   
   
   


%\section{Schott's method}

%$$
%E=ZZ^T-\sum_{i=1}^k n_i \bar{X}_i \bar{X}_i^T.
%$$

%$$
%H=\sum_{i=1}^{k} n_i \bar{X}_i \bar{X}_i^T - n\bar{X}\bar{X}^T.
%$$

%$$
%\mytr E = \mytr Z^T Z - \mytr J^T Z^T Z J.
%$$


%$$
%\mytr H = \mytr J^T Z^T Z J - \frac{1}{n} 1_n^T Z^T Z 1_n
%$$

%$$
%T_{SC}=\frac{1}{\sqrt{n-1}}(
%\frac{1}{k-1}\mytr H-\frac{1}{n-k} \mytr E
%)
%$$


\section{Theoretical results}

In this section, we investigate the asymptotic behavior of our test statistic when $p$ is much larger than $n$.
In high dimensional setting, it is a common phenomenon that the asymptotic distribution of a statistic relies on the covariance structure~\citep{Ma2015A}.
We shall derive the asymptotic distribution of our statistic under two different covariance structures: non-spiked covariance and spiked covariance.

Let $\bW_{k-1}$ be a $(k-1)\times(k-1)$ symmetric random matrix whose entries above the main diagonal are i.i.d.\ $N(0,1)$ and the entries on the diagonal are i.i.d.\ $N(0,2)$.
The random matrix $\bW_{k-1}$ will appear in the asymptotic distribution of $T(\bX)$.

The following theorem establishes the asymptotic distribution of $T(\bX)$ under non-spiked covariance.
\begin{theorem}\label{nonSpiked}
    Suppose $p/n\to \infty$, $c_1\geq \lambda_1(\Sigma)\geq \cdots\geq \lambda_p(\Sigma)\geq c_2$ and
    $$
    \mytr\Big(\Sigma-\frac{1}{p}(\mytr\Sigma)\bI_p\Big)^2=o\big(\frac{p}{n}\big).
    $$
    Under local alternative $p^{-1}\|\Xi \bC\|_F^2\to 0$,
    we have
    %$$
    %{\Big(2\mytr (\Sigma^2)\Big)}^{-1/2}{\Big(C^TJ^T Z^T(\bI_p-\bP_{Z\tilde J}) ZJC- \frac{p-n+k}{p}\mytr(\Sigma)\bI_{k-1}-C^T \Xi^T (\bI_p-\bP_{Z\tilde{J}})\Xi C}\Big)
    %\xrightarrow{\mathcal{L}}\bW_{k-1}.
    %$$
    $$
    \frac{T(\bX)-\frac{p-n+k}{p}\mytr(\Sigma)}{\sqrt{\mytr(\Sigma^2)}}
    \sim
    \lambda_{\max}\Big( \bW_{k-1} +\frac{1}{\sqrt{\mytr(\Sigma^2)}} \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC\Big)+o_P(1).
    $$
\end{theorem}



For some real world problems, variables are heavily correlated with common factors, then a few eigenvalues of $\Sigma$ are significantly larger than the others~\cite{Ma2015A}.
To characterize this correlation, we make the following assumption for the eigenvalues of $\Sigma$.

\begin{assumption}\label{assumpEigen}
    Let $r$ be a fixed integer.
    For small eigenvalues $\lambda_{r+1}(\Sigma),\ldots,\lambda_p(\Sigma)$, we assume
 $c_1 \geq \lambda_{r+1}(\Sigma) \geq \ldots \geq \lambda_{p}(\Sigma) \geq c_2$ for absolute constants $c_1$ and $c_2$.
    For large eigenvalues $\lambda_1(\Sigma),\ldots,\lambda_r(\Sigma)$,
    we assume 
    $$\frac{\lambda_r(\Sigma) n}{p}\to \infty,\quad
    \frac{\lambda_1(\Sigma)^2 p}{\lambda_r(\Sigma)^2 n^2}\to 0.$$
\end{assumption}


To state the asymptotic distribution of $T(\bX)$ under Assumption~\ref{assumpEigen}.
We need following notations.
Let $\Sigma= \bU\Lambda \bU^T$ be the eigenvalue decomposition of $\Sigma$, where $\Lambda =\mydiag (\lambda_1(\Sigma),\ldots,\lambda_p(\Sigma))$.
Let $\bU=(\bU_1,\bU_2)$ where $\bU_1$ is $p\times r$ and $\bU_2$ is $p\times (p-r)$.
Let $\Lambda_1=\mydiag(\lambda_1(\Sigma),\ldots,\lambda_r(\Sigma))$ and $\Lambda_2=\mydiag(\lambda_{r+1}(\Sigma),\ldots,\lambda_p(\Sigma))$.
Then $\Sigma=\bU_1\Lambda_1 \bU_1^T+\bU_2\Lambda_2 \bU_2^T$.

The following theorem establishes the asymptotic distribution of $T(\bX)$ under spiked covariance.
\begin{theorem}\label{thm1}
    Under Assumption~\eqref{assumpEigen}, suppose $p/n\to \infty$ and
    $$
    \mytr\Big(\Lambda_2-\frac{1}{p-r}(\mytr \Lambda_2)\bI_{p-r}\Big)^2=o\big(\frac{p}{n}\big).
    $$
    Then under local alternative
    \begin{equation*}
        \frac{1}{\sqrt{p}}\|\Xi \bC\|_F^2=O(1),
    \end{equation*}
    we have
    $$
    \begin{aligned}
        \frac{T(\bX)-\frac{p-r-n+k}{p-r}\mytr(\Lambda_2)}{\sqrt{\mytr (\Lambda_2^2)}}
        \sim
        \lambda_{\max}\Big(\bW_{k-1}+\tfrac{1}{\sqrt{\mytr(\Lambda_2^2)}} \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC\Big)
        +o_P(1).
    \end{aligned}
    $$
\end{theorem}


\subsection{Variance estimation}
%Under the assumptions of Theorem~\ref{nonSpiked}.
Let $w_{ij}$ be the $(i,j)$th element of $\bY^T \bU_{\bY,2} \bU_{\bY,2}^T \bY$.
We use
$$\frac{2}{(n-k)(n-k-1)}\sum_{1\leq i<j\leq n-k}w_{ij}^2$$
to estimate $\mytr(\Lambda_2^2)$.

\begin{proposition}

\end{proposition}
\begin{proof}
    $$
    \begin{aligned}
    \sum_{1\leq i < j\leq n-k} w_{ij}^2=
        \frac{1}{2}(\sum_{i=1}^{n-k}\sum_{j=1}^{n-k} w_{ij}^2-\sum_{i=1}^{n-k}w_{ii}^2)
    \end{aligned}
    $$
    Note that
    $$
    \begin{aligned}
\sum_{i=1}^{n-k}\sum_{j=1}^{n-k} w_{ij}^2
        &=\mytr(\bY^T \bU_{\bY,2} \bU_{\bY,2}^T \bY)^2\\
        &=\mytr(\bU_{\bY,2}^T \bY\bY^T \bU_{\bY,2} )^2\\
        &=\sum_{i=r+1}^{p}\lambda_i^2(\bY\bY^T)\\
        &=\sum_{i=r+1}^{n-k}\lambda_i^2(\bY^T\bY).
    \end{aligned}
    $$
    Note that $\bY^T\bY=\bG_{1A}^T \Lambda_1 \bG_{1A}+\bG_{1B}^T \Lambda_2 \bG_{1B}$. For $r+1\leq i\leq n-k$,
    $$
    \lambda_i (\bG_{1B}^T \Lambda_2 \bG_{1B})\leq \lambda_i(\bY^T \bY)\leq \lambda_{i-r}(\bG_{1B}^T \Lambda_2 \bG_{1B}).
    $$
    Then
    $$
    \Big|\sum_{i=r+1}^{n-k}\lambda_i^2(\bY^T \bY)-\sum_{i=1}^{n-k}\lambda_i^2 (\bG_{1B}^T \Lambda_2 \bG_{1B})\Big|\leq
    r\lambda_{1}^2(\bG_{1B}^T \Lambda_2 \bG_{1B})=O_P(r p^2).
    $$

    For $w_{ii}$, we have
    $$
    w_{ii} = Y_i^T \bU_{\bY,2}\bU_{\bY,2}Y_i
     = Y_i^T (\bU_{\bY,2}\bU_{\bY,2}^T-\bU_2 \bU_2^T)Y_i
     + Y_i^T \bU_2 \bU_2^T Y_i.
    $$
Since
$$
      Y_i^T (\bU_{\bY,2}\bU_{\bY,2}^T-\bU_2 \bU_2^T)Y_i\leq
     \|\bU_{\bY,2}\bU_{\bY,2}^T-\bU_2 \bU_2^T\| Y_i^T Y_i=O_P(\sqrt{\frac{p}{n\lambda_r}})O_P(\lambda_1 r+p)
    $$
    and $ Y_i^T \bU_2 \bU_2^T Y_i\asymp p$, we have 
    $$w_{ii}^2=(Y_i^T \bU_2 \bU_2^T Y_i)^2+O_P(\sqrt{\frac{p}{n\lambda_r}}(\lambda_1 r+p)p)+O_P(\frac{p}{n\lambda_r}(\lambda_1 r +p)^2)$$


    Hence
    $$
    \begin{aligned}
        &\sum_{1\leq i < j\leq n-k} w_{ij}^2=
        \frac{1}{2}(\sum_{i=1}^{n-k}\sum_{j=1}^{n-k} w_{ij}^2-\sum_{i=1}^{n-k}w_{ii}^2)\\
        =&
        \sum_{1\leq i < j\leq n-k} (\bY^T \bU_2 \bU_2^T \bY)_{ij}^2
        +O_P(rp^2)+n(O_P(\sqrt{\frac{p}{n\lambda_r}}(\lambda_1 r+p)p)+O_P(\frac{p}{n\lambda_r}(\lambda_1 r +p)^2))\\
        =&
        \sum_{1\leq i < j\leq n-k} (\bY^T \bU_2 \bU_2^T \bY)_{ij}^2
        +o_P(n^2 p)
    \end{aligned}
    $$
    provided
    $$
    \frac{rp}{n^2} \to 0.
    $$

\end{proof}



\section{Comparison with existing tests}
\label{sc:compare}
In this section, we revist some existing high dimensional tests in the point of view of union intersection principle.
This will help to compare the proposed test and other tests.

For high dimensional testing problem, the step 1 of Roy's union intersection principle is often induced by a data projection and component tests are univariate problem.
For univariate testing problem, likelihood ratio test statistic is often the best choice.
As we have obtained a set of component test statistic, we need to summarize them to obtain a global test statistic.
Union intersection principle suggest using the maximum of component test statistics.
But it is not the only choice.
In summary, a generalized union intersection principle consists the following 3 steps.
\begin{enumerate}
    \item
       Constructed a class of projected univariate $\{\bX_{\gamma}:\gamma\in\Gamma\}$ which contains all the information of data $\bX$.
        This induces a decomposition of the null hypothesis and the alternative hypohtesis:
        $$
        H_0=\bigcap_{\gamma\in\Gamma} H_{0\gamma} \quad \text{v.s.} \quad 
        H_1=\bigcup_{\gamma\in \Gamma} H_{1\gamma}.
        $$
    \item
        Construct a test statistic $T_\gamma$ for $H_{0\gamma}$ against $H_{1\gamma}$.
    \item
        Summarize the component test statistics $\{T_{\gamma}:\gamma\in\Gamma\}$ into a global test statistic.
\end{enumerate}
For step 1, we consider two different constructions of data projection.
\begin{enumerate}[i]
    \item
        Consider the set $\{\bX_{i}=e_i^T \bX:i=1,\ldots,p\}$, where $e_i$ is the $i$th standard basis.
\item
    Consider the set $\{\bX_{a}=a^T \bX:i=1,a\in\mathbb{R}^p, a^T a=1\}$.
\end{enumerate}
For step 3, we consider two different strategy of summarization.
\begin{enumerate}[I]
    \item
        Integrating $T_\gamma$ according some measure $\mu(\gamma)$ and use $\int_\gamma T_{\gamma}\,\mu(d\gamma)$ as global test statistic.
\item
    Use $\max_{\gamma\in\Gamma}T_\gamma$ as global test statistic.
\end{enumerate}

First, we consider using construction i in step 1.
If component statistics 
$${(k-1)^{-1}} e_i^T \bH e_i-(n-k)^{-1}e_i^T \bG e_i\quad i=1,\ldots, p$$
are used in step 2 and strategy I with $\mu$ being the uniform measure on $1,\ldots,p$ is used in step 3, one obtains $T_{SC}$.
If the likelihood ratio test statistic $e_i^T \bH e_i/e_i^T \bG e_i$ is used in step 2, one obtains a scalar invariant test statistic which is a direct generalization of~\citet{Srivastava2009A}.
By using data $\Omega^{-1}\bX$, the test statistic $T_{CX}$ can be obtained with strategy II.
The component test statistic corresponding $T_{CX}$ is similar to likelihood ratio tests.
Maybe $T_{CX}$ can be improved by replace their component tests by likelihood ratio tests.

We can see that the test statistics resulting from the construction i mostly requires that certain prior information about the covariance structure of data is known.
For example,~\citet{Schott2007Some} requires that $\mytr(\Sigma^{2j})/p\to \tau_j\in(0,\infty)$, $j=1,2$, and~\citet{Cai2014High} requires a consistent estimator of $\Omega$.
This may due to that the construction i chooses a orthogonal basis of $\mathbb{R}^p$.




Next, we consider using  construction ii in step 1.
Suppose the likelihood ratio test $T_a=a^T \bH a/a^T \bG a$ is used in step 2.
In step 3, if we choose strategy I with $\mu$ equals to the uniform distribution on the sphere, then the test statistic becomes
$$
\int_{a^T a=1} \frac{a^T \bH a}{a^T \bG a}\, \mu(da).
$$
Although it is hard to give the explicit form of the integration, it can be approximated by random projection. More specifically, one can randomly generate unit vectors $a_1,\ldots,a_M$ and the statistics can be approximated by $M^{-1}\sum_{i=1}^M a^T \bH a/a^T \bG a$.
A similar random projection method is proposed by~\cite{Lopes2015A} for $k=2$.
There analysis and simulations show that such random projection method has relative good performance especially when variables are correlated.

Our new test statistic comes from construction ii in step 1, the likelihood ratio test statistics in step 2 and strategy II in step 3.
Theorems~\ref{nonSpiked} and~\ref{thm1} allow us to analyze the properties of the proposed test.
Suppose $\sqrt{n_i}\mu_i$ is from prior distribution $N_p(0,\psi \bI_p)$, $i=1,\ldots, k$.
Then $\psi^{-1}\bC^T \Xi^T \Xi \bC$ is distributed as $\text{Wishart}_{k-1}(p,\bI_{k-1})$ (Wishart distribution with freedom $p$ and parameter $\bI_{k-1}$) and $\psi^{-1}\bC^T \Xi^T \bP_{\bY}\Xi \bC$ is distributed as $\text{Wishart}_{k-1}(n-k,\bI_{k-1})$.
In this case, we have
$$
\psi^{-1}\bC^T \Xi^T (\bI_P-\bP_{\bY})\Xi \bC=
(1+o_P(1))\psi^{-1}\bC^T \Xi^T \Xi \bC.
$$
If the conditions of Theorem~\ref{nonSpiked} hold and $k=2$, the asymptotic power of the proposed test is the same as that of~\cite{Bai1996Efiect}and~\cite{Chen2010A}'s method.
Since the method of~\cite{Schott2007Some} is a direct generalization of~\cite{Bai1996Efiect}'s method, it can be shown the asymptotic power of the proposed test is the same as that of~\cite{Schott2007Some} for general $k$.
Next, suppose the covariance matrix is spiked and the conditions of Theorem~\ref{thm1} hold.
Theorem~\ref{thm1} implies that the proposed test does not depend on large eigenvalues $\lambda_1,\ldots,\lambda_r$ while other existing test procedures are negatively affected by large eigenvalues $\lambda_1,\ldots,\lambda_r$.   
Thus, the new test has particular good power behavior when $\lambda_1,\ldots,\lambda_r$ are large.
 This property is not surprising since our statistic is from construction ii.
As a result, our statistic has a wider applicable range compared to the tests from construction i.


%%%%%%%%%%%%%%%%% Hotelling-Lawley trace %%%%%%%%%%%%%%%%%%%%%%%%%
%Another classical test statistic, Hotelling-Lawley trace,  can also be derived by Roy's union intersection principle. This is shown by~\cite{Mudholkar1974}.
 %In that paper, they consider the transformed data $\{M^T \bX: M \textrm{ is } \text{$(k-1)\times p$ matrix}\}$ and the decomposition of hypotheses:
 %$$H_{0}=\bigcap_{M}H_{0M} \quad\text{and}\quad H_1=\bigcup_{M}H_{1M} ,$$
 %where
 %$$
 %H_{0M}: \mytr( M \Xi C) = 0\quad \text{and}\quad H_{1M} : \mytr( M \Xi C )> 0.
 %$$
%
%Note that $\myE Z=\Xi J^T$, hence
%the uniformly minimum variance unbiased estimator of $\mytr(M\Xi C)$ is $\mytr(MZJ C)$.
%It can be seen that
%$
%\mytr \big(MZJ C\big)
%\sim
%N\big(\mytr(M\Xi C),\mytr(M\Sigma M^T )\big)
%$.

%=====================
%$$
%\mytr \big(MZJC\big)
%=
%\mytr \big(CMZJ\big)
%$$
%$ZJ=(\sqrt{n_1}\bar{\bX}_1,\ldots,\sqrt{n_k}\bar{\bX}_k)$.
%Note that $CM\sqrt{n_i}\bar{\bX}_i\sim N_{k-1}(\sqrt{n_i}CM\xi_i,CM\Sigma M^T C^T)$.
%Hence we have that
%$$
%\mytr \big(CMZJ\big)
%\sim
%N(\mytr(CM\Xi),\mytr(CM\Sigma M^T C^T))
%\sim
%N(\mytr(M\Xi C),\mytr(M\Sigma M^T )).
%$$
%==================

%Hence it's natural to use one side $t$ type statistic
%$$
%T_M = \frac{
%\mytr \big(MZJC\big)
%}{
    %\sqrt{\mytr(M G M^T)}
%}
%$$
%to test $H_M$ against $K_M$.



%By Cauchy inequality $\max_B \mytr(AB^T)/\mytr^{1/2}(BB^T)=\mytr^{1/2}(AA^T)$, we have
%$$
%\begin{aligned}
    %\max_M T_M &=\max_M \frac{\mytr \big(MG^{1/2}G^{-1/2}ZJC\big)
%}{\sqrt{\mytr(M G^{1/2} (M G^{1/2})^T)}
%}
    %=\mytr^{1/2}((ZJC)^T G^{-1}ZJC)\\
    %&=\mytr^{1/2}( ZJC(ZJC)^T G^{-1})
    %=\mytr^{1/2}(H G^{-1}).
%\end{aligned}
%$$

%====================
\section{Simulation Results}

In this section, we evaluate the numerical performance of the new test. For comparison, we also carried out simulation for the test of~\citet{Cai2014High} and the test of~\citet{Schott2007Some}.
These tests are denoted respectively by NEW, CX and SC.
Since the critical value of $CX$ and $SC$ may not be valid under spiked covariance model, we use permutation method to determine the critical value for all three test.
The empirical power is computed based on $1000$ simulations.

In the simulations, we set $k=3$.
Note that the new test is invariant under orthogonal transformation.
Without loss of generality, we only consider diagonal $\Sigma$.
We consider two different structure of $\Sigma$.
\begin{itemize}
    \item
        Covariance structure I: $\Sigma=\mydiag(p,1,\ldots,1)$.
    \item
        Covariance structure II: $\Sigma=\mydiag(\rho_1,\ldots,\rho_p)$, where $\rho_1\geq \cdots\geq \rho_p$ are order statistics of $p$ i.i.d.\ random variables which have uniform distribution between $0$ and $1$.
\end{itemize}

Define signal-to-noise ratio (SNR) as
$$
\textrm{SNR}=\frac{\|\xi_f\|_F^2}{\sqrt{\sum_{i=2}^{p}\lambda_i(\Sigma)^2}}.
$$
We use SNR to characterize the signal strength.
We consider two structure of alternative hypotheses: the non-sparse alternative and the sparse alternative.
In the non-sparse case, we set $\xi_1=\kappa 1_p$, $\xi_2=-\kappa 1_p$ and $\xi_3=\mathbf{0}_p$, where $\kappa$ is selected to make the SNR equal to the given value.
In the sparse case, we set $\xi_1=\kappa (1_{p/5}^T,\mathbf{0}_{4p/5}^T)^T$, $\xi_2=\kappa (\mathbf{0}_{p/5}^T, 1_{p/5}^T,\mathbf{0}_{3p/5}^T)^T$ and $\xi_3=\mathbf{0}_p$. Again, $\kappa$ is selected to make the SNR equal to the given value.

The simulation results are summarized in Tables~\ref{table1}-\ref{table6}. It can be seen from the results that under spiked covariance, the proposed test outperforms the other two tests for both non-sparse and sparse alternatives.
Under non-spiked covariance, the power of the new test is a little lower than that of SC.\ 
As $p$ increase, the power of the new test approaches to that of SC.

%$$
%SNR=\frac{\|\xi_f\|_F^2}{\sqrt{\mytr (\Sigma^2)}}
%$$


\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and non-sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=10$. }
    \label{table1}
    \centering
    \begin{tabular}{*{10}{c}}
    \toprule
    \multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=50$}&\multicolumn{3}{c}{$p=75$}&\multicolumn{3}{c}{$p=100$} \\
        \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
    \midrule
0 & 0.035 & 0.048 & 0.052 & 0.057 & 0.052 & 0.057 & 0.053 & 0.048 & 0.045 \\ 
1 & 0.060 & 0.049 & 0.096 & 0.081 & 0.050 & 0.092 & 0.063 & 0.062 & 0.085 \\ 
2 & 0.100 & 0.058 & 0.140 & 0.073 & 0.045 & 0.169 & 0.086 & 0.055 & 0.171 \\ 
3 & 0.145 & 0.066 & 0.234 & 0.119 & 0.070 & 0.266 & 0.117 & 0.056 & 0.307 \\ 
4 & 0.126 & 0.064 & 0.317 & 0.121 & 0.059 & 0.380 & 0.122 & 0.061 & 0.402 \\ 
5 & 0.179 & 0.072 & 0.392 & 0.178 & 0.068 & 0.541 & 0.141 & 0.071 & 0.579 \\ 
6 & 0.198 & 0.070 & 0.513 & 0.189 & 0.071 & 0.639 & 0.143 & 0.066 & 0.717 \\ 
7 & 0.249 & 0.085 & 0.629 & 0.227 & 0.084 & 0.777 & 0.206 & 0.073 & 0.822 \\ 
8 & 0.268 & 0.092 & 0.685 & 0.252 & 0.084 & 0.822 & 0.217 & 0.078 & 0.894 \\ 
9 & 0.324 & 0.100 & 0.786 & 0.256 & 0.090 & 0.911 & 0.246 & 0.074 & 0.949 \\ 
10 & 0.342 & 0.115 & 0.828 & 0.303 & 0.097 & 0.937 & 0.270 & 0.075 & 0.973 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and non-sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table2}
\centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.050 & 0.043 & 0.050 & 0.056 & 0.066 & 0.048 & 0.062 & 0.045 & 0.054 \\ 
1 & 0.069 & 0.048 & 0.063 & 0.046 & 0.052 & 0.091 & 0.068 & 0.048 & 0.095 \\ 
2 & 0.097 & 0.046 & 0.131 & 0.086 & 0.053 & 0.164 & 0.068 & 0.057 & 0.173 \\ 
3 & 0.113 & 0.061 & 0.200 & 0.117 & 0.057 & 0.270 & 0.101 & 0.045 & 0.313 \\ 
4 & 0.135 & 0.053 & 0.247 & 0.130 & 0.054 & 0.402 & 0.118 & 0.066 & 0.485 \\ 
5 & 0.158 & 0.065 & 0.357 & 0.134 & 0.066 & 0.526 & 0.134 & 0.073 & 0.616 \\ 
6 & 0.198 & 0.081 & 0.433 & 0.161 & 0.052 & 0.668 & 0.138 & 0.067 & 0.765 \\ 
7 & 0.217 & 0.068 & 0.514 & 0.191 & 0.067 & 0.759 & 0.174 & 0.068 & 0.862 \\ 
8 & 0.229 & 0.063 & 0.582 & 0.223 & 0.075 & 0.853 & 0.187 & 0.060 & 0.927 \\ 
9 & 0.264 & 0.094 & 0.680 & 0.218 & 0.080 & 0.918 & 0.227 & 0.067 & 0.966 \\ 
10 & 0.298 & 0.091 & 0.758 & 0.245 & 0.076 & 0.934 & 0.228 & 0.052 & 0.982 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=10$. }
    \label{table3}
\centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=50$}&\multicolumn{3}{c}{$p=75$}&\multicolumn{3}{c}{$p=100$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.063 & 0.056 & 0.052 & 0.048 & 0.049 & 0.048 & 0.057 & 0.047 & 0.042 \\ 
1 & 0.087 & 0.058 & 0.071 & 0.069 & 0.044 & 0.096 & 0.076 & 0.051 & 0.080 \\ 
2 & 0.091 & 0.066 & 0.116 & 0.113 & 0.037 & 0.133 & 0.080 & 0.058 & 0.139 \\ 
3 & 0.155 & 0.065 & 0.177 & 0.131 & 0.062 & 0.228 & 0.113 & 0.058 & 0.218 \\ 
4 & 0.184 & 0.065 & 0.246 & 0.174 & 0.076 & 0.308 & 0.144 & 0.061 & 0.310 \\ 
5 & 0.225 & 0.081 & 0.337 & 0.214 & 0.075 & 0.386 & 0.176 & 0.083 & 0.417 \\ 
6 & 0.270 & 0.088 & 0.425 & 0.266 & 0.085 & 0.507 & 0.228 & 0.071 & 0.508 \\ 
7 & 0.364 & 0.080 & 0.501 & 0.307 & 0.078 & 0.571 & 0.302 & 0.087 & 0.629 \\ 
8 & 0.405 & 0.105 & 0.549 & 0.381 & 0.080 & 0.698 & 0.362 & 0.089 & 0.721 \\ 
9 & 0.470 & 0.121 & 0.634 & 0.408 & 0.078 & 0.774 & 0.391 & 0.070 & 0.797 \\ 
10 & 0.547 & 0.128 & 0.702 & 0.484 & 0.109 & 0.819 & 0.415 & 0.088 & 0.877 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure I and sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table4}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.048 & 0.045 & 0.046 & 0.053 & 0.046 & 0.043 & 0.051 & 0.034 & 0.046 \\ 
1 & 0.079 & 0.055 & 0.082 & 0.066 & 0.063 & 0.079 & 0.063 & 0.059 & 0.100 \\ 
2 & 0.097 & 0.054 & 0.119 & 0.088 & 0.055 & 0.138 & 0.085 & 0.055 & 0.160 \\ 
3 & 0.133 & 0.069 & 0.167 & 0.113 & 0.066 & 0.223 & 0.114 & 0.054 & 0.235 \\ 
4 & 0.149 & 0.062 & 0.212 & 0.126 & 0.084 & 0.298 & 0.132 & 0.057 & 0.344 \\ 
5 & 0.204 & 0.060 & 0.281 & 0.169 & 0.066 & 0.427 & 0.154 & 0.057 & 0.469 \\ 
6 & 0.252 & 0.060 & 0.352 & 0.227 & 0.070 & 0.548 & 0.195 & 0.072 & 0.641 \\ 
7 & 0.310 & 0.072 & 0.429 & 0.252 & 0.059 & 0.614 & 0.220 & 0.061 & 0.711 \\ 
8 & 0.372 & 0.088 & 0.529 & 0.314 & 0.085 & 0.719 & 0.297 & 0.060 & 0.800 \\ 
9 & 0.427 & 0.083 & 0.547 & 0.362 & 0.085 & 0.794 & 0.300 & 0.057 & 0.881 \\ 
10 & 0.449 & 0.093 & 0.619 & 0.396 & 0.072 & 0.853 & 0.340 & 0.076 & 0.911 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure II and non-sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table5}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.063 & 0.054 & 0.058 & 0.052 & 0.040 & 0.042 & 0.045 & 0.049 & 0.070 \\ 
1 & 0.141 & 0.120 & 0.115 & 0.126 & 0.120 & 0.112 & 0.103 & 0.110 & 0.102 \\ 
2 & 0.181 & 0.209 & 0.169 & 0.330 & 0.260 & 0.210 & 0.200 & 0.227 & 0.201 \\ 
3 & 0.692 & 0.367 & 0.244 & 0.759 & 0.385 & 0.341 & 0.468 & 0.413 & 0.394 \\ 
4 & 0.753 & 0.539 & 0.420 & 0.744 & 0.573 & 0.515 & 0.516 & 0.554 & 0.561 \\ 
5 & 0.828 & 0.690 & 0.509 & 0.871 & 0.697 & 0.693 & 0.556 & 0.724 & 0.727 \\ 
6 & 0.809 & 0.812 & 0.622 & 0.822 & 0.824 & 0.766 & 0.959 & 0.838 & 0.859 \\ 
7 & 1.000 & 0.882 & 0.780 & 0.979 & 0.916 & 0.903 & 0.990 & 0.923 & 0.947 \\ 
8 & 0.993 & 0.955 & 0.789 & 1.000 & 0.965 & 0.954 & 0.999 & 0.972 & 0.971 \\ 
9 & 1.000 & 0.979 & 0.911 & 0.999 & 0.981 & 0.979 & 0.964 & 0.986 & 0.987 \\ 
10 & 1.000 & 0.991 & 0.877 & 0.989 & 0.996 & 0.988 & 0.996 & 0.996 & 0.997 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbp]
    \caption{Empirical powers of tests under covariance structure II and sparse alternative. $\alpha=0.05$, $k=3$, $n_1=n_2=n_3=25$. }
    \label{table6}
    \centering
\begin{tabular}{*{10}{c}}
\toprule
\multirow{2}{*}{SNR} &\multicolumn{3}{c}{$p=100$}&\multicolumn{3}{c}{$p=150$}&\multicolumn{3}{c}{$p=200$} \\
    \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}
        & CX & SC & NEW & CX &SC &NEW &CX & SC & NEW\\
\midrule
0 & 0.052 & 0.055 & 0.047 & 0.055 & 0.057 & 0.053 & 0.044 & 0.055 & 0.057 \\ 
1 & 0.068 & 0.124 & 0.065 & 0.070 & 0.130 & 0.085 & 0.049 & 0.116 & 0.087 \\ 
2 & 0.085 & 0.233 & 0.112 & 0.076 & 0.239 & 0.149 & 0.067 & 0.241 & 0.161 \\ 
3 & 0.110 & 0.388 & 0.161 & 0.090 & 0.408 & 0.215 & 0.097 & 0.417 & 0.227 \\ 
4 & 0.120 & 0.530 & 0.184 & 0.112 & 0.552 & 0.282 & 0.103 & 0.556 & 0.309 \\ 
5 & 0.167 & 0.708 & 0.238 & 0.142 & 0.699 & 0.387 & 0.140 & 0.687 & 0.394 \\ 
6 & 0.196 & 0.807 & 0.261 & 0.168 & 0.820 & 0.472 & 0.162 & 0.823 & 0.547 \\ 
7 & 0.217 & 0.875 & 0.318 & 0.177 & 0.892 & 0.505 & 0.173 & 0.896 & 0.646 \\ 
8 & 0.234 & 0.935 & 0.378 & 0.220 & 0.951 & 0.625 & 0.195 & 0.948 & 0.749 \\ 
9 & 0.312 & 0.965 & 0.407 & 0.222 & 0.970 & 0.672 & 0.224 & 0.979 & 0.809 \\ 
10 & 0.334 & 0.976 & 0.505 & 0.292 & 0.987 & 0.773 & 0.254 & 0.989 & 0.881 \\ 
\bottomrule
\end{tabular}
\end{table}

\section{Concluding remarks}
In this paper, motivated by Roy's union intersection principle, we proposed a generalized likelihood ratio statistic for MANOVA in high dimensional setting.
We proved that the proposed test has similar asymptotic power with $T_{SC}$ under non-spiked covariance.
On the other hand, if covariance matrix is spiked, the asymptotic power of the proposed test is not affected by the large eigenvalues.
We give a discussion of existing MANOVA tests from union intersection principle point of view, this explains why the proposed test has good power behavior.

%In Section~\label{sc:compare}, we consider a random projection test.
%The theoretical properties of random projection test may be a.





\section*{Appendix}

\begin{proposition}\label{optProp}
    Suppose $\bA$ is a $p\times r$ matrix with rank $r$ and $\bB$ is a $p\times p$  non-zero semi-definite matrix.
    Denote by $\bA=\bU_\bA \bD_\bA \bV_\bA^T$ the singular value decomposition of $\bA$, where $\bU_\bA$ and $\bV_\bA$ are $p\times r$ and $r\times r$ column orthogonal matrix, $\bD_\bA$ is a $r\times r$ diagonal matrix.
    Let $\bP_\bA=\bU_\bA \bU_\bA^T$ be the projection on the column space of $\bA$.
    Then
    \begin{equation}
        \max_{a^T a=1, a^T \bA \bA^T a=0}a^T \bB a=
        \lambda_{\max}\big(\bB(\bI_p-\bP_\bA)\big).
    \end{equation}
\end{proposition}
\begin{proof}
    Note that $a^T \bA \bA^T a=0$ is equivalent to $\bP_\bA a=0$ which in turn is equivalent to $a= (\bI_p-\bP_\bA)a$.
    Then
    \begin{equation}\label{eq:prop1eq1}
        \begin{aligned}
        \max_{a^T a=1, a^T \bA \bA^T a=0}a^T \bB a
            &=
        \max_{a^T a=1, \bP_\bA a=0}a^T(\bI_p-\bP_\bA) \bB (\bI_p-\bP_\bA)a,
        \end{aligned}
    \end{equation}
    which is obviously no greater than $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)$.
    To prove that they are equal,  without loss of generality, we can assume $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)>0$.
    Let $\alpha_1$ be one eigenvector corresponding to the largest eigenvalue of $(\bI-\bP_\bA)\bB(\bI-\bP_\bA)$.
    Since $(\bI-\bP_\bA)\bB(\bI-\bP_\bA)\bP_\bA=(\bI-\bP_\bA)\bB(\bP_\bA-\bP_\bA)=\bO_{p\times p}$ and $\bP_\bA$ is symmetric, the rows of $\bP_\bA$ are eigenvetors of $(\bI-\bP_\bA)\bB(\bI-\bP_\bA)$ corresponding to eigenvalue $0$.
    It follows that $\bP_\bA\alpha_1=0$.
    Therefore, $\alpha_1$ satisfies the constraint of~\eqref{eq:prop1eq1} and~\eqref{eq:prop1eq1} is no less than $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)$.
    The conclusion now follows by noting that $\lambda_{\max}\big((\bI-\bP_\bA)\bB(\bI-\bP_\bA)\big)=\lambda_{\max}\big( \bB(\bI-\bP_\bA)\big)$.
    
\end{proof}





\paragraph{Proof of the main results}
\begin{lemma}[\citet{DAVIDSON2001317} Theorem II.7]\label{DSbound}
    Let $\bA$ be $m\times n$ with iid $N(0,1)$ entries.
    If $m>n$, then for any $t>0$,
    \begin{align*}
        \Pr(\sqrt{\lambda_1(\bA \bA^T)}>\sqrt{m}+\sqrt{n}+t)\leq \exp(-t^2/2),\\
        \Pr(\sqrt{\lambda_n(\bA \bA^T)}<\sqrt{m}-\sqrt{n}-t)\leq \exp(-t^2/2).
    \end{align*}
\end{lemma}

It can be seen that $\bX\bJ\bC$ is independent of $\bY$.
Since
$
\myE \bY = \bO_{p\times (n-k)}
$,
we can write
$
\bY = \bU\Lambda^{1/2} \bG_1
$,
where $\bG_1$ is a $p\times (n-k)$ matrix with i.i.d.\ $N(0,1)$ entries.
We write
$
\bX\bJ\bC = \xi_f + \bU\Lambda^{1/2} \bG_2
$, 
where $\bG_2$ is a $p\times (k-1)$ matrix with i.i.d. $N(0,1)$ entries.

Then 
\begin{equation}\label{eq:maindec}
\begin{aligned}
\bC^T\bJ^T \bX^T(\bI_p-\bP_{\bY}) \bX\bJ\bC
=&
\bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2+
\xi_f^T (\bI_p -\bP_{\bY})\xi_f+\\
&\xi_f^T (\bI_p -\bP_{\bY})\bU\Lambda^{1/2}\bG_2+
\bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\xi_f.
\end{aligned}
\end{equation}
    The first term of~\eqref{eq:maindec} can be represented as
\begin{equation}\label{eq:firstTerm}
\bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2=
\sum_{i=1}^p \lambda_i (\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\xi_i \xi_i^T,
\end{equation}
where $\xi_i\overset{i.i.d.}{\sim} N(0,\bI_{k-1})$.
\begin{proof}[\textrm{Proof of Theorem~\ref{nonSpiked}}]
    First we deal with the first term of~\eqref{eq:maindec}.
    Note that for $i=1,\ldots, p$, we have 
    \begin{equation}\label{eq:EigU}
    \lambda_i (\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\leq
    \lambda_i (\Lambda).
    \end{equation}
    Note that $\bP_{\bY}$ has rank $n-k$. For $i=1,\ldots, p-n+k$, by Weyl's inequality, we have
    \begin{equation}\label{eq:EigL}
    \lambda_i (\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\geq
    \lambda_{i+n-k} (\Lambda).
    \end{equation}
Then we have
    $$
    \begin{aligned}
        \frac{\lambda_1^2(\Lambda^{1/2}\bU^T(\bI_p-\bP_{\bY})\bU\Lambda^{1/2})}{\sum_{i=1}^p \lambda_i^2(\Lambda^{1/2}\bU^T(\bI_p-\bP_{\bY})\bU\Lambda^{1/2})}
    \leq
        \frac{c_1}{c_2(p-n+k)}\to 0.
    \end{aligned}
    $$
    Apply Lyapunov central limit theorem conditioning on $\bY$, we have
    $$
    \begin{aligned}
        &\Big( \sum_{i=1}^p \lambda_i^2(\Lambda^{1/2}\bU^T(\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\Big)^{-1/2}\\
        &{\Big( \bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2-\sum_{i=1}^p \lambda_i(\Lambda^{1/2}\bU^T(\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\bI_{k-1}\Big)}
    \xrightarrow{\mathcal{L}} \bW_{k-1}.
    \end{aligned}
    $$

    Also by~\eqref{eq:EigU} and~\eqref{eq:EigL}, we have
    $$
    \sum_{i=n-k+1}^p \lambda_i^2
    \leq
    \mytr\Big((\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})^2\Big)\leq
    \mytr(\Lambda^2).
    $$ 
    Hence we have
    $$
    \mytr\Big((\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})^2\Big)
    =
    \mytr(\Lambda^2)+O_P(n)
    =
    \big(1+O_P(\frac{n}{p})\big)\mytr(\Lambda^2).
    $$ 

Note that
    $$
    \mytr(\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})
    =
    \mytr(\Lambda)-\mytr(\bP_{\bY}\bU\Lambda \bU^T).
    $$ 
and
    $$
    \begin{aligned}
        &
        \big|
    \mytr(\bP_{\bY}\bU\Lambda \bU^T)
    -\frac{n-k}{p}\mytr(\Lambda)
    \big|
    =
    \big|
    \mytr\Big(\bP_{\bY} \bU \big(\Lambda-\frac{1}{p} (\mytr \Lambda) \bI_p \big) \bU^T\Big)
    \big|
        \\
        \leq &
        \sqrt{\mytr \big(\bP_{\bY}^2\big)}
        \sqrt{\mytr \Big(\Lambda-\frac{1}{p}\big(\mytr \Lambda\big) \bI_p\Big)^2}
        =\sqrt{(n-k)\mytr \Big(\Lambda-\frac{1}{p}\big(\mytr \Lambda\big) \bI_p\Big)^2}
        =o(\sqrt{p}).
    \end{aligned}
    $$
    Hence 
    $$
    \mytr(\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})
    =
    \frac{p-n+k}{p}\mytr(\Lambda)+o(\sqrt{p}).
    $$
    It follows that
    $$
    \begin{aligned}
        &\Big(\sum_{i=1}^p \lambda_i^2(\Lambda^{1/2}\bU^T(\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\Big)^{-1/2}\\
        &{\Big( \bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2-\sum_{i=1}^p \lambda_i(\Lambda^{1/2}\bU^T(\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\bI_{k-1}\Big)}\\
        =&
        {\Big( (1+O_P(\frac{n}{p}))\mytr (\Lambda^2)\Big) }^{-1/2} {\Big( \bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2-
        \big(\frac{p-n+k}{p}\mytr(\Lambda)+O_P(\sqrt{p})\big)\bI_{k-1}
        \Big)}
    \end{aligned}
    $$
    By Slutsky's theorem, we have that
    $$
    \frac{1}{\sqrt{\mytr(\Lambda_2^2)}}
    {\Big( \bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2-
        \frac{p-n+k}{p}\mytr(\Lambda)\bI_{k-1}\Big)}
    \xrightarrow{\mathcal{L}}\bW_{k-1}
    $$

    Note that
    $$
    \begin{aligned}
        &\myE\Big(\| \bC^T \Xi^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2\|_F^2\Big)\\
        = &
        (k-1)\myE \Big(\mytr\big(
        \bC^T \Xi^T (\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\Xi \bC
        \big)\Big)\\
        \leq &
        (k-1)\myE \Big(\lambda_1\big(
         (\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})
        \big)\Big)
        \|\Xi \bC\|_F^2\\
        \leq &
        (k-1)\lambda_1(\Lambda)\|\Xi \bC\|_F^2
        \leq 
        (k-1)\bC \|\Xi \bC\|_F^2=o(p),
    \end{aligned}
    $$
    we have
    $$
    \frac{1}{\sqrt{\mytr(\Lambda_2^2)}}
    {\Big(\bC^T\bJ^T \bX^T(\bI_p-\bP_{\bY}) \bX \bJ\bC- \frac{p-n+k}{p}\mytr(\Sigma)\bI_{k-1}-\bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC}\Big)
    \xrightarrow{\mathcal{L}}\bW_{k-1}.
    $$
    Equivalently, we have
    $$
    \begin{aligned}
        &
    \frac{1}{\sqrt{\mytr(\Lambda_2^2)}}
        {\Big(\bC^T \bJ^T \bX^T(\bI_p-\bP_{\bY}) \bX \bJ\bC- \frac{p-n+k}{p}\mytr(\Sigma)\bI_{k-1}}\Big)\\
        \sim&
    \frac{1}{\sqrt{\mytr(\Lambda_2^2)}}
         \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC
        +\bW_{k-1}+o_P(1).
    \end{aligned}
    $$
    Then the conclusion follows by taking the maximum eigenvalue.
\end{proof}




Let $\bG_1=(\bG_{1A}^T,\bG_{1B}^T)^T$, where $\bG_{1A}$ is the first $r$ rows of $\bG_1$ and $\bG_{1B}$ is the last $p-r$ rows of $\bG_1$.
The following lemma gives the asympototic property of $\lambda_{i}(\bY^T \bY)$, $i=1,\ldots, r$.
\begin{lemma}\label{PCAlemma1}
    Under the Assumptions of Theorem~\ref{thm1}, $r=o(n)$,
    $$\sup_{1\leq i\leq r}\Big|\frac{\lambda_{i}(\bY^T \bY)}{n\lambda_i}-1\Big|\to 0$$
    almost surely.
\end{lemma}
\begin{proof}
    %For a matrix $\bA$, we denote by  $\bA_{[a:b,c:d]}$ the $a$-to-$b$-th row, $c$-to-$d$-th column of matrix $\bA$, by $\bA_{[a:b,:]}$ and $\bA_{[:,c:d]}$ the $a$-to-$b$-th full rows and $c$-to-$d$-th full columns of $\bA$ respectively.
%$
%\bY^T \bY=
%\bG_1^T \Lambda \bG_1   = \bV_{\bY} \bD_{\bY}^2 \bV_{\bY}^T
%$, and 
%$
%\bG_1^T \Lambda \bG_1=
%\bG_{1A}^T \Lambda_1 \bG_{1A}+
%\bG_{1B}^T \Lambda_2 \bG_{1B}
%$. We have
    Note that
$$
\bY^T \bY =\bG_1^T \Lambda \bG_1
=
\bG_{1A}^T \Lambda_1 \bG_{1A}+
\bG_{1B}^T \Lambda_2 \bG_{1B}.
$$
For $1\leq i \leq r$, we have
    \begin{equation}\label{eq:DLU}
\lambda_i(\bG_{1A}^T \Lambda_1 \bG_{1A})
    \leq \lambda_i(\bY^T \bY) \leq \lambda_i(\bG_{1A}^T \Lambda_1 \bG_{1A})+
    c_1 \lambda_1(\bG_{1B}^T  \bG_{1B}).
    \end{equation}
 Using Weyl's inequality, we can derive a lower bound for $\lambda_i(\bG_{1A}^T \Lambda_1 \bG_{1A})$, $ i=1,\ldots, r$.
\begin{equation}\label{eq:DLower}
\begin{aligned}
&\lambda_i(\bG_{1A}^T \Lambda_1 \bG_{1A})
\geq
\lambda_i(\bG_{1A}^T \mydiag(\lambda_i \bI_{i},\bO_{(r-i)\times(r-i)}) \bG_{1A})
\\
    =&
    \lambda_i\Big( \lambda_i \bG_{1A}^T \bG_{1A}-\lambda_i\bG_{1A}^T \mydiag(\bO_{i\times i}, \bI_{r-i}) \bG_{1A}\Big)\\
    \geq&
    \lambda_r\Big( \lambda_i \bG_{1A}^T \bG_{1A}\Big)+\lambda_{p+i-r}\Big(-\lambda_i\bG_{1A}^T \mydiag(\bO_{i\times i}, \bI_{r-i}) \bG_{1A}\Big)\\
= &
\lambda_i \lambda_r(\bG_{1A}\bG_{1A}^T).
\end{aligned}
\end{equation}
Similarly, we can obtain the upper bound.
\begin{equation}\label{eq:DUpper}
\begin{aligned}
&\lambda_i(\bG_{1A}^T \Lambda_1 \bG_{1A})
\\
=&\lambda_i\Big(
\bG_{1A}^T \big(
\mydiag(\lambda_1,\ldots,\lambda_{i-1},\bO_{(r-i+1)\times(r-i+1)})+
\mydiag(\bO_{(i-1)\times(i-1)},\lambda_i,\ldots,\lambda_r)
\big)
\bG_{1A}
\Big)\\
%\leq&
%\lambda_1(\bG_{1[1:r,:]}^T \mydiag(\bO_{(i-1)\times(i-1)},\lambda_i,\ldots,\lambda_r) \bG_{1[1:r,:]})
    \leq&
\lambda_1(\bG_{1A}^T \mydiag(\bO_{(i-1)\times(i-1)},\lambda_i \bI_{r-i+1}) \bG_{1A})
\leq  \lambda_i \lambda_1(\bG_{1A}\bG_{1A}^T).
\end{aligned}
\end{equation}
    The inequality~\eqref{eq:DLU},~\eqref{eq:DLower} and~\eqref{eq:DUpper} implies that
    $$
   \sup_{1\leq i \leq r} \Big|\frac{\lambda_i(\bY^T \bY)}{n\lambda_i}-1\Big|\leq
    \max\Big(\Big|\frac{\lambda_1(\bG_{1A}\bG_{1A}^T)}{n}-1\Big|,\Big|\frac{\lambda_r(\bG_{1A}\bG_{1A}^T)}{n}-1\Big|\Big)+\frac{c_1}{n\lambda_r}\lambda_1(\bG_{1B}^T \bG_{1B}).
    $$
    We only need to prove the right hand side converges to $0$ almost surely.

    By Lemma~\ref{DSbound}, for every $t>0$, we have
    $$
    \begin{aligned}
        &\Pr\Big(\sqrt{1-\frac{k}{n}}-\sqrt{\frac{r}{n}}-\frac{t}{\sqrt{n}}\leq \sqrt{\frac{\lambda_r(\bG_{1A}\bG_{1A}^T)}{n}}\leq \sqrt{\frac{\lambda_1(\bG_{1A}\bG_{1A}^T)}{n}}\leq \sqrt{1-\frac{k}{n}}+\sqrt{\frac{r}{n}}+\frac{t}{\sqrt{n}} \Big) \\
        &\geq 1-2\exp(-\frac{t^2}{2}).
    \end{aligned}
    $$
    Let $t=n^{1/4}$, then Borel-Cantelli lemma implies 
    $$
    \frac{\lambda_r(\bG_{1A}\bG_{1A}^T)}{n}\to 1\,\quad
    \frac{\lambda_1(\bG_{1A}\bG_{1A}^T)}{n}\to 1,
    $$
    almost surely.
    For $\lambda_1(\bG_{1B}^T \bG_{1B})$, by Lemma~\ref{DSbound}, we have
    $$
   \Pr\Big( \frac{c_1}{n\lambda_r}{\lambda_1(\bG_{1B}\bG_{1B}^T)}\leq \frac{c_1}{n\lambda_r}(\sqrt{n-k}+\sqrt{p-r}+t)^2 \Big) 
   \geq 1-\exp(-\frac{t^2}{2}).
    $$
Let $t=n^{1/2}$, since we have assumed $\lambda_r n/p\to \infty$, $$\frac{c_1}{n\lambda_r}{\lambda_1(\bG_{1B}\bG_{1B}^T)}\to 0$$
almost surely.
This completes the proof.
\end{proof}


Let $\bU_\bY=(\bU_{\bY,1},\bU_{\bY,2})$, where $\bU_{\bY,1}$ and $\bU_{\bY,2}$ are the first $r$ and last $p-r$ columns of $\bU_\bY$ respectively.
\begin{lemma}\label{PCAlemma2}
    Under the Assumptions of Theorem~\ref{thm1}, we have
$$
\lambda_{\max}(\bI_r-\bU_1^T \bU_{\bY,1}\bU_{\bY,1}^T \bU_1)
=O_P(\frac{ p}{\lambda_r n}).
$$
\end{lemma}
\begin{proof}
    From
$
\bU\Lambda^{1/2} \bG_1 \bG_1^T \Lambda^{1/2} \bU^T 
=\bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T
$,
we have
%$$
 %\Lambda^{1/2}\bG_1 \bG_1^T  \Lambda^{1/2}
%= \bU^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU.
%$$ 
$$
    \begin{pmatrix}        
        \Lambda_{1}^{\frac{1}{2}}\bG_{1A} \bG_{1A}^T \Lambda_1^{\frac{1}{2}}&
        \Lambda_{1}^{\frac{1}{2}} \bG_{1A}\bG_{1B}^T\Lambda_2^{\frac{1}{2}}\\
        \Lambda_{2}^{\frac{1}{2}} \bG_{1B} \bG_{1A}^T\Lambda_1^{\frac{1}{2}} &
        \Lambda_{2}^{\frac{1}{2}}\bG_{1B}\bG_{1B}^T\Lambda_2^{\frac{1}{2}}\\
    \end{pmatrix}
    =
    \begin{pmatrix}        
         \bU_1^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_1&
         \bU_1^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_2\\
        \bU_2^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_1&
         \bU_2^T \bU_{\bY}\bD_{\bY}^2 \bU_{\bY}^T \bU_2\\
    \end{pmatrix}
$$
It follows that
$$
\begin{aligned}
 \Lambda_{2}^{\frac{1}{2}}\bG_{1B} \bG_{1B}^T  \Lambda_{2}^{\frac{1}{2}}
\geq&
    \lambda_r(\bY^T \bY) \bU_{2}^T \bU_{\bY,1}\bU_{\bY,1}^T \bU_{2}.
\end{aligned}
$$
Hence
    \begin{equation}\label{ineq:eigenvector}
\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})\leq
\frac{c_1}{\lambda_r(\bY^T \bY)} \lambda_{1}
(\bG_{1B} \bG_{1B}^T).
%=O_P(\frac{p}{\lambda_r n}),
    \end{equation}
    %where the last equality follows by Lemma~\ref{PCAlemma1} and Weyl's inequality.
    By Lemma~\ref{DSbound}, for every $t>0$, we have
    $$
    \Pr\Big(\frac{1}{p}(\sqrt{p-r}-\sqrt{n-k}-t)^2 \leq \frac{1}{p}{\lambda_1(\bG_{1B}\bG_{1B}^T)}\leq \frac{1}{p}(\sqrt{p-r}+\sqrt{n-k}+t)^2 \Big) 
   \geq 1-2\exp(-\frac{t^2}{2}).
    $$
    Let $t=n^{1/2}$, then Borel-Cantelli lemma implies that 
    \begin{equation}\label{eq:qu1}
    \frac{1}{p}\lambda_1(\bG_{1B}\bG_{1B}^T)\to 1
    \end{equation}
    almost surely.
    Then~\eqref{eq:qu1},~\eqref{ineq:eigenvector} and Lemma~\ref{PCAlemma1} implies that
    $$\lambda_{1}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})=O_P(\frac{p}{\lambda_r n}).
    $$
 The conclusion then follows by the following simple relationship
$$
\begin{aligned}
&\lambda_{\max}(\bU_{2}^T \bU_{\bY,1} \bU_{\bY,1}^T \bU_{2})
=
\lambda_{\max}(\bU_{\bY,1}^T \bU_{2} \bU_{2}^T \bU_{\bY,1})\\
=&
    \lambda_{\max}(\bU_{\bY,1}^T (\bI_p-\bU_{1} \bU_{1}^T) \bU_{\bY,1})
    =
\lambda_{\max}(\bI_r-\bU_{\bY,1}^T\bU_{1} \bU_{1}^T \bU_{\bY,1})\\
=&
1-\lambda_{\min}(\bU_{\bY,1}^T\bU_{1} \bU_{1}^T \bU_{\bY,1})
=
1-\lambda_{\min}( \bU_{1}^T \bU_{\bY,1}\bU_{\bY,1}^T\bU_{1})\\
=&
\lambda_{\max}(\bI_r-\bU_{1}^T \bU_{\bY,1}\bU_{\bY,1}^T\bU_{1}).
\end{aligned}
$$
\end{proof}

\begin{proof}[\textrm{Proof of Theorem~\ref{thm1}}]
    As in the proof of Theorem~\ref{nonSpiked}, for $i=r+1,\ldots, p$, we have
    \begin{equation}\label{eq:mybound1}
    \lambda_i (\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\leq
    \lambda_i (\Lambda).
    \end{equation}
     And for $i=1,\ldots, p-n+k$, we have
    \begin{equation}\label{eq:mybound2}
    \lambda_i (\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})\geq
    \lambda_{i+n-k} (\Lambda).
    \end{equation}

    Next, we need to give an upper bound for $\lambda_i(\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2})$, $i=1,\ldots,r$.
    Note that the positive eigenvalues of $\Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}$  equal to the positive eigenvalues of $(\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})$.
Write $(\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})$ as the sum of two terms
$$
\begin{aligned}
&(\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})
\\
=&
(\bI_p-\bP_{\bY})\bU_1\Lambda_1 \bU_1^T(\bI_p-\bP_{\bY})+(\bI_p-\bP_{\bY})\bU_2\Lambda_2 \bU_2^T (\bI_p-\bP_{\bY})
\overset{def}{=}\bR_1+\bR_2.
\end{aligned}
$$

Note that
$$
\begin{aligned}
&\lambda_{\max}\big( \bR_1 \big)
=
\lambda_{\max}\big(\Lambda_1^{1/2} \bU_1^T(\bI_p-\bP_{\bY}) \bU_1 \Lambda_1^{1/2}\big)
\leq 
\lambda_{\max}\big(\Lambda_1^{1/2} \bU_1^T(\bI_p-\bU_{\bY,1}\bU_{\bY,1}^T) \bU_1 \Lambda_1^{1/2}\big)\\
\leq &
\lambda_1
\lambda_{\max}\big(\bU_1^T(\bI_p-\bU_{\bY,1}\bU_{\bY,1}^T) \bU_1 \big)
= 
\lambda_1
\lambda_{\max}\big(\bI_r - \bU_1^T\bU_{\bY,1}\bU_{\bY,1}^T \bU_1 \big)=O_P(\frac{\lambda_1 p}{\lambda_r n}).
\end{aligned}
$$
    The last equality follows by Lemma~\ref{PCAlemma2}.
Thus, for $i=1,\ldots, r$, we have
    \begin{equation}\label{eq:mybound3}
\lambda_i\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)\leq
\lambda_1(\bR_1)+\lambda_1(\bR_2)= O_P(\frac{\lambda_1 p}{\lambda_r n}) + c_1.
\end{equation}

    As a consequence of the bound~\eqref{eq:mybound1},~\eqref{eq:mybound2} and~\eqref{eq:mybound3}, we have
$$
    \sum_{i=n-k+1}^p \lambda_i^2\leq \mytr\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)^2 \leq  r(O_P(\frac{\lambda_1 p}{\lambda_r n})+c_1)^2+\sum_{i=r+1}^p \lambda_i^2.
$$
    Hence
    \begin{equation}\label{eq:spiketrace1}
    \big|\mytr\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)^2 - \sum_{i=r+1}^p \lambda_i^2 \big|\leq 
    r(O_P(\frac{\lambda_1 p}{\lambda_r n})+c_1)^2+O(n).
    \end{equation}

Note that
    $$
    \mytr(\bR_2)
    =
    \mytr(\Lambda_2)-\mytr(\bP_{\bY}\bU_2\Lambda_2 \bU_2^T).
    $$ 
and
    $$
    \begin{aligned}
        &
        \big|
    \mytr(\bP_{\bY}\bU_2\Lambda_2 \bU_2^T)
    -\frac{n-k}{p-r}\mytr(\Lambda_2)
    \big|
    =
    \Big|
        \mytr\Big(\bP_{\bY} \bU \big(\Lambda_2-\frac{1}{p-r} (\mytr \Lambda_2) \bI_{p-r} \big) \bU^T\Big)
    \Big|
        \\
        \leq &
        \sqrt{\mytr \big(\bP_{\bY}^2\big)}
        \sqrt{\mytr \Big(\Lambda_2-\frac{1}{p-r}\big(\mytr \Lambda_2\big) \bI_{p-r}\Big)^2}
        =\sqrt{(n-k)\mytr \Big(\Lambda_2-\frac{1}{p-r}\big(\mytr \Lambda_2\big) \bI_{p-r}\Big)^2}
        =o(\sqrt{p}).
    \end{aligned}
    $$
    Hence 
    $$
    \mytr(\bR_2)
    =
    \frac{p-r-n+k}{p-r}\mytr(\Lambda_2)+o(\sqrt{p}).
    $$
    Then
\begin{equation}\label{eq:spiketrace2}
\big| \mytr [(\bR_1+\bR_2)]-\frac{p-r-n+k}{p-r}\mytr(\Lambda_2)\big|\leq 
rO_P\big(\frac{\lambda_1 p}{\lambda_r n}\big)+o(\sqrt{p}).
\end{equation}
Equation~\eqref{eq:spiketrace1} and~\eqref{eq:spiketrace2}, conbined with the assumptions, yield
$$
    \mytr\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)^2 
 =(1+o_P(1))\mytr(\Lambda_2),
$$
and
$$
\mytr\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)
= \frac{p-r-n+k}{p-r}\mytr(\Lambda_2)+o_P(\sqrt{p}).
$$

Now we have the Lyapunov condition
$$
\frac{\lambda_1\Big(\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)^2\Big)}{\mytr \Big( \big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)^2\Big)}
%\leq
%\frac{
%\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
%}{\sum_{i=1}^{p-r-n+k}\lambda_{i+n-k}^2}
%\leq
%\frac{
%\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+C\big)^2
%}{c(p-r-n+k)}
=
\frac{
\big( O_P(\frac{\lambda_1 p}{\lambda_r n})+c_1\big)^2
}{
    (1+o_P(1))\mytr(\Lambda_2)
}
\xrightarrow{P} 0.
$$
Apply Lyapunov central limit theorem conditioning on $\bP_{\bY}$, we have
$$
\begin{aligned}
    &\Big(\mytr \Big( \big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)^2\Big) \Big)^{-1/2}\\
    &\big( \bG_2^T \Lambda^{1/2}\bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2
    -\mytr\big((\bI_p-\bP_{\bY})\bU\Lambda \bU^T (\bI_p-\bP_{\bY})\big)
     \bI_{k-1} \big)
\xrightarrow{\mathcal{L}} \bW_{k-1},
\end{aligned}
$$
where $\bW_{k-1}$ is a $(k-1)\times(k-1)$ symmetric random matrix whose entries above the main diagonal are i.i.d.\ $N(0,1)$ and the entries on the diagonal are i.i.d.\ $N(0,2)$.
By Slutsky's theorem, we have
$$
\begin{aligned}
    \frac{1}{\sqrt{\mytr(\Lambda_2^2)}}
    \big( \bG_2^T \Lambda^{1/2} \bU^T (\bI_p-\bP_{\bY})\bU\Lambda^{1/2}\bG_2
    -\tfrac{p-r-n+k}{p-r}\mytr(\Lambda_2)\bI_{k-1} \big)
\xrightarrow{\mathcal{L}} \bW_{k-1}.
\end{aligned}
$$



As for the cross term of~\eqref{eq:maindec}, we have
$$
\begin{aligned}
    &\myE [\|\bC^T \Xi^T (\bI_p -\bP_{Z\tilde{J}})\bU\Lambda^{1/2}\bG_2\|_F^2|\bY]\\
    = &
    (k-1)\mytr(\bC^T \Xi^T (\bI_p -\bP_{\bY})\bU\Lambda \bU^T (\bI_p -\bP_{\bY})\Xi \bC)\\
    \leq &
    (k-1)\lambda_1\big((\bI_p -\bP_{\bY})\bU\Lambda \bU^T (\bI_p -\bP_{\bY})\big)\|\Xi \bC\|^2_F\\
    = &
    (k-1) O_P(\frac{\lambda_1 p}{\lambda_r n})  \|\Xi \bC\|^2_F\\
    = &
    (k-1) O_P(\frac{\lambda_1 \sqrt{p}}{\lambda_r n}) \sqrt{p}  \|\Xi \bC\|^2_F=o_P(p)
\end{aligned}
$$
The last equality holds when we assume $\frac{1}{\sqrt{p}}\|\Xi \bC\|_F^2=O(1)$.
Hence $\|\bC^T \Xi^T (\bI_p -\bP_{\bY})\bU\Lambda^{1/2}\bG_2\|_F^2=o_P(p)$, and we have
$$
\begin{aligned}
\frac{1}{\sqrt{\mytr(\Lambda_2^2)}}
    \big( \bC^T\bY^T(\bI_p-\bP_{\bY}) \bY \bC
    -\tfrac{p-r-n+k}{p-r}\mytr(\Lambda_2)\bI_{k-1} -\bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC\big)
\xrightarrow{\mathcal{L}} \bW_{k-1}.
\end{aligned}
$$
    Equivalently, we have
    $$
    \begin{aligned}
        &\frac{1}{\sqrt{\mytr(\Lambda_2^2)}} {\Big(\bC^T\bY^T(\bI_p-\bP_{\bY}) \bY \bC- \frac{p-r-n+k}{p-r}\mytr(\Lambda_2)\bI_{k-1}}\Big)\\
        \sim&
\frac{1}{\sqrt{\mytr(\Lambda_2^2)}} \bC^T \Xi^T (\bI_p-\bP_{\bY})\Xi \bC
        +\bW_{k-1}+o_P(1).
    \end{aligned}
    $$

    Then the conclusion follows by taking the maximum eigenvalue.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Supplementary Materials}

Contain
the brief description of the online supplementary materials.
\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Acknowledgements}

Write the acknowledgements here.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\markboth{\hfill{\footnotesize\rm FIRSTNAME1 LASTNAME1 AND FIRSTNAME2 LASTNAME2} \hfill}
{\hfill {\footnotesize\rm GENERALIZED LIKELIHOOD RATIO TEST} \hfill}

%\iffalse
\bibhang=1.7pc
\bibsep=2pt
\fontsize{9}{14pt plus.8pt minus .6pt}\selectfont
\renewcommand\bibname{\large \bf References}
%\begin{thebibliography}{11}
\expandafter\ifx\csname
natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL}\fi
%\fi

\bibliographystyle{chicago}      % Chicago style, author-year citations
\bibliography{mybibfile}   % name your BibTeX data base

%-------------------------------------------
\vskip .65cm
\noindent
first author affiliation
\vskip 2pt
\noindent
E-mail: (first author email)
\vskip 2pt

\noindent
second author affiliation
\vskip 2pt
\noindent
E-mail: (second author email)

\end{document}
